2021-11-11 09:03:58.590634 (MainThread): Running with dbt=0.21.0
2021-11-11 09:03:58.746244 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-11-11 09:03:58.746884 (MainThread): Tracking: tracking
2021-11-11 09:03:58.752801 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ee5cf0910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ee3f831f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ee3f83640>]}
2021-11-11 09:03:58.759335 (MainThread): Partial parsing not enabled
2021-11-11 09:03:58.763522 (MainThread): Parsing macros/catalog.sql
2021-11-11 09:03:58.767972 (MainThread): Parsing macros/etc.sql
2021-11-11 09:03:58.769169 (MainThread): Parsing macros/adapters.sql
2021-11-11 09:03:58.782486 (MainThread): Parsing macros/materializations/seed.sql
2021-11-11 09:03:58.784185 (MainThread): Parsing macros/materializations/copy.sql
2021-11-11 09:03:58.785872 (MainThread): Parsing macros/materializations/view.sql
2021-11-11 09:03:58.787538 (MainThread): Parsing macros/materializations/snapshot.sql
2021-11-11 09:03:58.788605 (MainThread): Parsing macros/materializations/table.sql
2021-11-11 09:03:58.794080 (MainThread): Parsing macros/materializations/incremental.sql
2021-11-11 09:03:58.802500 (MainThread): Parsing macros/core.sql
2021-11-11 09:03:58.804799 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-11-11 09:03:58.805904 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-11-11 09:03:58.807365 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-11-11 09:03:58.808131 (MainThread): Parsing macros/schema_tests/unique.sql
2021-11-11 09:03:58.809033 (MainThread): Parsing macros/materializations/test.sql
2021-11-11 09:03:58.813544 (MainThread): Parsing macros/materializations/helpers.sql
2021-11-11 09:03:58.819139 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-11-11 09:03:58.832589 (MainThread): Parsing macros/materializations/view/view.sql
2021-11-11 09:03:58.836796 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-11-11 09:03:58.839156 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-11-11 09:03:58.850720 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-11-11 09:03:58.851787 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-11-11 09:03:58.857976 (MainThread): Parsing macros/materializations/common/merge.sql
2021-11-11 09:03:58.865866 (MainThread): Parsing macros/materializations/table/table.sql
2021-11-11 09:03:58.870106 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-11-11 09:03:58.879973 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-11-11 09:03:58.881055 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-11-11 09:03:58.899199 (MainThread): Parsing macros/etc/is_incremental.sql
2021-11-11 09:03:58.900215 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-11-11 09:03:58.901245 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-11-11 09:03:58.902132 (MainThread): Parsing macros/etc/query.sql
2021-11-11 09:03:58.902723 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-11-11 09:03:58.904228 (MainThread): Parsing macros/etc/where_subquery.sql
2021-11-11 09:03:58.905315 (MainThread): Parsing macros/etc/datetime.sql
2021-11-11 09:03:58.910266 (MainThread): Parsing macros/adapters/common.sql
2021-11-11 09:03:59.056003 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-11-11 09:03:59.063690 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-11-11 09:03:59.079080 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-11-11 09:03:59.080188 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-11-11 09:03:59.081240 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-11-11 09:03:59.082360 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-11-11 09:03:59.090820 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0cbdf6c6-86a5-4c65-bd06-df38adfa2fe7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ee3df21c0>]}
2021-11-11 09:03:59.093716 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-11-11 09:03:59.093977 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0cbdf6c6-86a5-4c65-bd06-df38adfa2fe7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ee3df2040>]}
2021-11-11 09:03:59.094156 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-11-11 09:03:59.095093 (MainThread): 
2021-11-11 09:03:59.095336 (MainThread): Acquiring new bigquery connection "master".
2021-11-11 09:03:59.096041 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-11-11 09:03:59.096288 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-11-11 09:04:49.920959 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_testingBigQuery".
2021-11-11 09:04:49.921593 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-11-11 09:04:49.928019 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-11-11 09:04:50.320050 (MainThread): 16:04:50 | Concurrency: 1 threads (target='prod')
2021-11-11 09:04:50.320622 (MainThread): 16:04:50 | 
2021-11-11 09:04:50.324807 (Thread-1): Began running node model.my_new_project.my_first_dbt_model
2021-11-11 09:04:50.325589 (Thread-1): 16:04:50 | 1 of 2 START table model testingBigQuery.my_first_dbt_model.......... [RUN]
2021-11-11 09:04:50.326471 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-11-11 09:04:50.326946 (Thread-1): Compiling model.my_new_project.my_first_dbt_model
2021-11-11 09:04:50.331145 (Thread-1): Writing injected SQL for node "model.my_new_project.my_first_dbt_model"
2021-11-11 09:04:50.331626 (Thread-1): finished collecting timing info
2021-11-11 09:04:50.341098 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.18', 38178), raddr=('172.217.194.95', 443)>
2021-11-11 09:04:50.341374 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.18', 57210), raddr=('34.101.5.42', 443)>
2021-11-11 09:04:50.341504 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.18', 38182), raddr=('172.217.194.95', 443)>
2021-11-11 09:04:50.341621 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.18', 57214), raddr=('34.101.5.42', 443)>
2021-11-11 09:04:50.347230 (Thread-1): Opening a new connection, currently in state closed
2021-11-11 09:04:50.351401 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-11-11 09:04:51.009928 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_first_dbt_model"
2021-11-11 09:04:51.010233 (Thread-1): On model.my_new_project.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_first_dbt_model"} */


  create or replace table `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-11-11 09:04:54.180224 (Thread-1): finished collecting timing info
2021-11-11 09:04:54.181247 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0cbdf6c6-86a5-4c65-bd06-df38adfa2fe7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ee3d9cee0>]}
2021-11-11 09:04:54.182076 (Thread-1): 16:04:54 | 1 of 2 OK created table model testingBigQuery.my_first_dbt_model..... [CREATE TABLE (2.0 rows, 0.0 Bytes processed) in 3.86s]
2021-11-11 09:04:54.182533 (Thread-1): Finished running node model.my_new_project.my_first_dbt_model
2021-11-11 09:04:54.183843 (Thread-1): Began running node model.my_new_project.my_second_dbt_model
2021-11-11 09:04:54.184444 (Thread-1): 16:04:54 | 2 of 2 START view model testingBigQuery.my_second_dbt_model.......... [RUN]
2021-11-11 09:04:54.185125 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-11-11 09:04:54.185509 (Thread-1): Compiling model.my_new_project.my_second_dbt_model
2021-11-11 09:04:54.188652 (Thread-1): Writing injected SQL for node "model.my_new_project.my_second_dbt_model"
2021-11-11 09:04:54.189082 (Thread-1): finished collecting timing info
2021-11-11 09:04:54.203151 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_second_dbt_model"
2021-11-11 09:04:54.203536 (Thread-1): Opening a new connection, currently in state closed
2021-11-11 09:04:54.207604 (Thread-1): On model.my_new_project.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_second_dbt_model"} */


  create or replace view `playground-325606`.`testingBigQuery`.`my_second_dbt_model`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
where id = 1;


2021-11-11 09:04:56.491486 (Thread-1): finished collecting timing info
2021-11-11 09:04:56.492806 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0cbdf6c6-86a5-4c65-bd06-df38adfa2fe7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ee1ceb040>]}
2021-11-11 09:04:56.494052 (Thread-1): 16:04:56 | 2 of 2 OK created view model testingBigQuery.my_second_dbt_model..... [OK in 2.31s]
2021-11-11 09:04:56.494764 (Thread-1): Finished running node model.my_new_project.my_second_dbt_model
2021-11-11 09:04:56.496975 (MainThread): Acquiring new bigquery connection "master".
2021-11-11 09:04:56.497905 (MainThread): 16:04:56 | 
2021-11-11 09:04:56.498565 (MainThread): 16:04:56 | Finished running 1 table model, 1 view model in 57.40s.
2021-11-11 09:04:56.499216 (MainThread): Connection 'master' was properly closed.
2021-11-11 09:04:56.499799 (MainThread): Connection 'model.my_new_project.my_second_dbt_model' was properly closed.
2021-11-11 09:04:56.502463 (MainThread): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.18', 38190), raddr=('172.217.194.95', 443)>
2021-11-11 09:04:56.502991 (MainThread): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.18', 57222), raddr=('34.101.5.42', 443)>
2021-11-11 09:04:56.506781 (MainThread): 
2021-11-11 09:04:56.506977 (MainThread): Completed successfully
2021-11-11 09:04:56.507149 (MainThread): 
Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
2021-11-11 09:04:56.507409 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ee3f83a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ee1ca2190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ee3df21f0>]}
2021-11-11 09:04:56.507718 (MainThread): Flushing usage events
2021-11-11 09:05:26.536833 (MainThread): The 'warn' method is deprecated, use 'warning' instead
2021-11-11 09:05:26.537070 (MainThread): Error sending message, disabling tracking
2021-12-10 07:47:42.914331 (MainThread): Running with dbt=0.21.0
2021-12-10 07:47:43.085374 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-10 07:47:43.090604 (MainThread): Tracking: tracking
2021-12-10 07:47:43.098091 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb86d8910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb696d310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb696d760>]}
2021-12-10 07:47:43.105297 (MainThread): Partial parsing not enabled
2021-12-10 07:47:43.113403 (MainThread): Parsing macros/catalog.sql
2021-12-10 07:47:43.118334 (MainThread): Parsing macros/etc.sql
2021-12-10 07:47:43.119700 (MainThread): Parsing macros/adapters.sql
2021-12-10 07:47:43.132916 (MainThread): Parsing macros/materializations/seed.sql
2021-12-10 07:47:43.134713 (MainThread): Parsing macros/materializations/copy.sql
2021-12-10 07:47:43.136370 (MainThread): Parsing macros/materializations/view.sql
2021-12-10 07:47:43.138148 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-10 07:47:43.139338 (MainThread): Parsing macros/materializations/table.sql
2021-12-10 07:47:43.145269 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-10 07:47:43.154401 (MainThread): Parsing macros/core.sql
2021-12-10 07:47:43.156908 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-10 07:47:43.158174 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-10 07:47:43.159653 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-10 07:47:43.160556 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-10 07:47:43.161548 (MainThread): Parsing macros/materializations/test.sql
2021-12-10 07:47:43.166341 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-10 07:47:43.173407 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-10 07:47:43.189034 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-10 07:47:43.193503 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-10 07:47:43.195817 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-10 07:47:43.208469 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-10 07:47:43.209660 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-10 07:47:43.215451 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-10 07:47:43.223552 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-10 07:47:43.227753 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-10 07:47:43.238395 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-10 07:47:43.239534 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-10 07:47:43.258643 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-10 07:47:43.260457 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-10 07:47:43.261568 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-10 07:47:43.262550 (MainThread): Parsing macros/etc/query.sql
2021-12-10 07:47:43.263183 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-10 07:47:43.264735 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-10 07:47:43.265905 (MainThread): Parsing macros/etc/datetime.sql
2021-12-10 07:47:43.271266 (MainThread): Parsing macros/adapters/common.sql
2021-12-10 07:47:43.419078 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-10 07:47:43.426729 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-12-10 07:47:43.442178 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-10 07:47:43.443424 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-10 07:47:43.444487 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-10 07:47:43.445697 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-10 07:47:43.454894 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ad7198dc-c46b-427d-b42e-67b0aff32cfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb6866a30>]}
2021-12-10 07:47:43.457852 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-10 07:47:43.458129 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ad7198dc-c46b-427d-b42e-67b0aff32cfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb67db220>]}
2021-12-10 07:47:43.458351 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-10 07:47:43.459293 (MainThread): 
2021-12-10 07:47:43.459536 (MainThread): Acquiring new bigquery connection "master".
2021-12-10 07:47:43.460256 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-10 07:47:43.460529 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-10 07:47:49.102668 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_testingBigQuery".
2021-12-10 07:47:49.103262 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-10 07:47:49.112117 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-10 07:47:49.886137 (MainThread): 14:47:49 | Concurrency: 1 threads (target='prod')
2021-12-10 07:47:49.886703 (MainThread): 14:47:49 | 
2021-12-10 07:47:49.896081 (Thread-1): Began running node model.my_new_project.my_first_dbt_model
2021-12-10 07:47:49.896881 (Thread-1): 14:47:49 | 1 of 2 START table model testingBigQuery.my_first_dbt_model.......... [RUN]
2021-12-10 07:47:49.898399 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-10 07:47:49.899198 (Thread-1): Compiling model.my_new_project.my_first_dbt_model
2021-12-10 07:47:49.906016 (Thread-1): Writing injected SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-10 07:47:49.906647 (Thread-1): finished collecting timing info
2021-12-10 07:47:49.925285 (Thread-1): Opening a new connection, currently in state closed
2021-12-10 07:47:49.928709 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-10 07:47:50.638792 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.17', 60112), raddr=('74.125.24.95', 443)>
2021-12-10 07:47:50.639210 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.17', 46160), raddr=('34.101.5.42', 443)>
2021-12-10 07:47:50.639483 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.17', 60116), raddr=('74.125.24.95', 443)>
2021-12-10 07:47:50.639738 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.17', 46164), raddr=('34.101.5.42', 443)>
2021-12-10 07:47:50.650144 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-10 07:47:50.650482 (Thread-1): On model.my_new_project.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_first_dbt_model"} */


  create or replace table `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-10 07:47:54.267561 (Thread-1): finished collecting timing info
2021-12-10 07:47:54.268588 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad7198dc-c46b-427d-b42e-67b0aff32cfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb6877b20>]}
2021-12-10 07:47:54.269290 (Thread-1): 14:47:54 | 1 of 2 OK created table model testingBigQuery.my_first_dbt_model..... [CREATE TABLE (2.0 rows, 0.0 Bytes processed) in 4.37s]
2021-12-10 07:47:54.269653 (Thread-1): Finished running node model.my_new_project.my_first_dbt_model
2021-12-10 07:47:54.270693 (Thread-1): Began running node model.my_new_project.my_second_dbt_model
2021-12-10 07:47:54.271343 (Thread-1): 14:47:54 | 2 of 2 START view model testingBigQuery.my_second_dbt_model.......... [RUN]
2021-12-10 07:47:54.272014 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-12-10 07:47:54.272368 (Thread-1): Compiling model.my_new_project.my_second_dbt_model
2021-12-10 07:47:54.275336 (Thread-1): Writing injected SQL for node "model.my_new_project.my_second_dbt_model"
2021-12-10 07:47:54.275747 (Thread-1): finished collecting timing info
2021-12-10 07:47:54.290782 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_second_dbt_model"
2021-12-10 07:47:54.291111 (Thread-1): Opening a new connection, currently in state closed
2021-12-10 07:47:54.294756 (Thread-1): On model.my_new_project.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_second_dbt_model"} */


  create or replace view `playground-325606`.`testingBigQuery`.`my_second_dbt_model`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
where id = 1;


2021-12-10 07:47:56.365088 (Thread-1): finished collecting timing info
2021-12-10 07:47:56.366031 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad7198dc-c46b-427d-b42e-67b0aff32cfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb68463a0>]}
2021-12-10 07:47:56.366915 (Thread-1): 14:47:56 | 2 of 2 OK created view model testingBigQuery.my_second_dbt_model..... [OK in 2.09s]
2021-12-10 07:47:56.367374 (Thread-1): Finished running node model.my_new_project.my_second_dbt_model
2021-12-10 07:47:56.369488 (MainThread): Acquiring new bigquery connection "master".
2021-12-10 07:47:56.370276 (MainThread): 14:47:56 | 
2021-12-10 07:47:56.370722 (MainThread): 14:47:56 | Finished running 1 table model, 1 view model in 12.91s.
2021-12-10 07:47:56.371093 (MainThread): Connection 'master' was properly closed.
2021-12-10 07:47:56.371383 (MainThread): Connection 'model.my_new_project.my_second_dbt_model' was properly closed.
2021-12-10 07:47:56.380279 (MainThread): 
2021-12-10 07:47:56.380705 (MainThread): Completed successfully
2021-12-10 07:47:56.381081 (MainThread): 
Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
2021-12-10 07:47:56.381513 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb696d310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb6850ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb6788940>]}
2021-12-10 07:47:56.381987 (MainThread): Flushing usage events
2021-12-21 09:42:35.039678 (MainThread): Running with dbt=0.21.0
2021-12-21 09:42:35.190764 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 09:42:35.196011 (MainThread): Tracking: tracking
2021-12-21 09:42:35.201349 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3289f46af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32881e1640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32881e16a0>]}
2021-12-21 09:42:35.207939 (MainThread): Partial parsing not enabled
2021-12-21 09:42:35.216329 (MainThread): Parsing macros/catalog.sql
2021-12-21 09:42:35.220335 (MainThread): Parsing macros/etc.sql
2021-12-21 09:42:35.221617 (MainThread): Parsing macros/adapters.sql
2021-12-21 09:42:35.233704 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 09:42:35.235177 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 09:42:35.236862 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 09:42:35.238366 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 09:42:35.239331 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 09:42:35.244634 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 09:42:35.252966 (MainThread): Parsing macros/core.sql
2021-12-21 09:42:35.255322 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 09:42:35.256358 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 09:42:35.257763 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 09:42:35.258563 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 09:42:35.259429 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 09:42:35.263833 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 09:42:35.269266 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 09:42:35.282869 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 09:42:35.287074 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 09:42:35.289584 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 09:42:35.302145 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 09:42:35.303199 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 09:42:35.311545 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 09:42:35.319833 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 09:42:35.324321 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 09:42:35.334373 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 09:42:35.335400 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 09:42:35.357154 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 09:42:35.358350 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 09:42:35.359754 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 09:42:35.361022 (MainThread): Parsing macros/etc/query.sql
2021-12-21 09:42:35.361996 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 09:42:35.364110 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 09:42:35.365633 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 09:42:35.371907 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 09:42:35.561695 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-21 09:42:35.571053 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-12-21 09:42:35.591945 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:42:35.593406 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:42:35.594684 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:42:35.596105 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:42:35.606647 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '88ce5247-2505-4dfa-894f-19dff141ceaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3288049100>]}
2021-12-21 09:42:35.609395 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 09:42:35.609627 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '88ce5247-2505-4dfa-894f-19dff141ceaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32880490d0>]}
2021-12-21 09:42:35.609804 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 09:42:35.610698 (MainThread): 
2021-12-21 09:42:35.610957 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 09:42:35.611544 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 09:42:35.611702 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 09:42:36.100454 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_testingBigQuery".
2021-12-21 09:42:36.101147 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 09:42:36.107796 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 09:42:36.501360 (MainThread): 16:42:36 | Concurrency: 1 threads (target='prod')
2021-12-21 09:42:36.501906 (MainThread): 16:42:36 | 
2021-12-21 09:42:36.506568 (Thread-1): Began running node model.my_new_project.my_first_dbt_model
2021-12-21 09:42:36.507362 (Thread-1): 16:42:36 | 1 of 2 START table model testingBigQuery.my_first_dbt_model.......... [RUN]
2021-12-21 09:42:36.508114 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-21 09:42:36.508528 (Thread-1): Compiling model.my_new_project.my_first_dbt_model
2021-12-21 09:42:36.514063 (Thread-1): Writing injected SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-21 09:42:36.515158 (Thread-1): finished collecting timing info
2021-12-21 09:42:36.535017 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 09:42:36.538380 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 09:42:36.961965 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49274), raddr=('172.217.194.95', 443)>
2021-12-21 09:42:36.962443 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 52078), raddr=('34.101.5.106', 443)>
2021-12-21 09:42:36.962749 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49278), raddr=('172.217.194.95', 443)>
2021-12-21 09:42:36.962948 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 52082), raddr=('34.101.5.106', 443)>
2021-12-21 09:42:36.972849 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-21 09:42:36.973109 (Thread-1): On model.my_new_project.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_first_dbt_model"} */


  create or replace table `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 09:42:40.164831 (Thread-1): finished collecting timing info
2021-12-21 09:42:40.165874 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88ce5247-2505-4dfa-894f-19dff141ceaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3287ffd4f0>]}
2021-12-21 09:42:40.166661 (Thread-1): 16:42:40 | 1 of 2 OK created table model testingBigQuery.my_first_dbt_model..... [CREATE TABLE (2.0 rows, 0.0 Bytes processed) in 3.66s]
2021-12-21 09:42:40.167062 (Thread-1): Finished running node model.my_new_project.my_first_dbt_model
2021-12-21 09:42:40.168527 (Thread-1): Began running node model.my_new_project.my_second_dbt_model
2021-12-21 09:42:40.169159 (Thread-1): 16:42:40 | 2 of 2 START view model testingBigQuery.my_second_dbt_model.......... [RUN]
2021-12-21 09:42:40.169803 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-12-21 09:42:40.170154 (Thread-1): Compiling model.my_new_project.my_second_dbt_model
2021-12-21 09:42:40.173599 (Thread-1): Writing injected SQL for node "model.my_new_project.my_second_dbt_model"
2021-12-21 09:42:40.173988 (Thread-1): finished collecting timing info
2021-12-21 09:42:40.186248 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_second_dbt_model"
2021-12-21 09:42:40.186636 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 09:42:40.190204 (Thread-1): On model.my_new_project.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_second_dbt_model"} */


  create or replace view `playground-325606`.`testingBigQuery`.`my_second_dbt_model`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
where id = 1;


2021-12-21 09:42:42.464971 (Thread-1): finished collecting timing info
2021-12-21 09:42:42.465960 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88ce5247-2505-4dfa-894f-19dff141ceaf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3287ffd3a0>]}
2021-12-21 09:42:42.466812 (Thread-1): 16:42:42 | 2 of 2 OK created view model testingBigQuery.my_second_dbt_model..... [OK in 2.30s]
2021-12-21 09:42:42.467267 (Thread-1): Finished running node model.my_new_project.my_second_dbt_model
2021-12-21 09:42:42.469500 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 09:42:42.470469 (MainThread): 16:42:42 | 
2021-12-21 09:42:42.471188 (MainThread): 16:42:42 | Finished running 1 table model, 1 view model in 6.86s.
2021-12-21 09:42:42.471907 (MainThread): Connection 'master' was properly closed.
2021-12-21 09:42:42.472481 (MainThread): Connection 'model.my_new_project.my_second_dbt_model' was properly closed.
2021-12-21 09:42:42.482909 (MainThread): 
2021-12-21 09:42:42.483424 (MainThread): Completed successfully
2021-12-21 09:42:42.483816 (MainThread): 
Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
2021-12-21 09:42:42.484318 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32880db820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3288094d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f328b8ddac0>]}
2021-12-21 09:42:42.484827 (MainThread): Flushing usage events
2021-12-21 09:50:48.965815 (MainThread): Running with dbt=0.21.0
2021-12-21 09:50:49.117904 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 09:50:49.118450 (MainThread): Tracking: tracking
2021-12-21 09:50:49.124002 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb98c22100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb96eba6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb96eba700>]}
2021-12-21 09:50:49.130299 (MainThread): Partial parsing not enabled
2021-12-21 09:50:49.134303 (MainThread): Parsing macros/catalog.sql
2021-12-21 09:50:49.138402 (MainThread): Parsing macros/etc.sql
2021-12-21 09:50:49.139710 (MainThread): Parsing macros/adapters.sql
2021-12-21 09:50:49.153059 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 09:50:49.154682 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 09:50:49.156320 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 09:50:49.157896 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 09:50:49.158991 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 09:50:49.164690 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 09:50:49.173403 (MainThread): Parsing macros/core.sql
2021-12-21 09:50:49.175664 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 09:50:49.176694 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 09:50:49.178034 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 09:50:49.178767 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 09:50:49.179579 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 09:50:49.183936 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 09:50:49.189607 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 09:50:49.203032 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 09:50:49.207312 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 09:50:49.209622 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 09:50:49.221497 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 09:50:49.222586 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 09:50:49.228622 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 09:50:49.236872 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 09:50:49.241313 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 09:50:49.251362 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 09:50:49.252365 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 09:50:49.270740 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 09:50:49.271692 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 09:50:49.272671 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 09:50:49.273651 (MainThread): Parsing macros/etc/query.sql
2021-12-21 09:50:49.274253 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 09:50:49.275668 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 09:50:49.276743 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 09:50:49.281746 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 09:50:49.430018 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-21 09:50:49.437784 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-12-21 09:50:49.453573 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:50:49.454749 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:50:49.455883 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:50:49.457046 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:50:49.465473 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5719370f-6edd-4a23-b545-20741d9f055e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb96daf640>]}
2021-12-21 09:50:49.468310 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 09:50:49.468559 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5719370f-6edd-4a23-b545-20741d9f055e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb96d240d0>]}
2021-12-21 09:50:49.468755 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 09:50:49.469670 (MainThread): 
2021-12-21 09:50:49.469921 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 09:50:49.470605 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 09:50:49.470829 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 09:50:50.130150 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_testingBigQuery".
2021-12-21 09:50:50.130943 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 09:50:50.142722 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 09:50:50.923063 (MainThread): 16:50:50 | Concurrency: 1 threads (target='prod')
2021-12-21 09:50:50.923251 (MainThread): 16:50:50 | 
2021-12-21 09:50:50.925185 (Thread-1): Began running node model.my_new_project.my_first_dbt_model
2021-12-21 09:50:50.926110 (Thread-1): 16:50:50 | 1 of 2 START table model testingBigQuery.my_first_dbt_model.......... [RUN]
2021-12-21 09:50:50.926895 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-21 09:50:50.927340 (Thread-1): Compiling model.my_new_project.my_first_dbt_model
2021-12-21 09:50:50.933891 (Thread-1): Writing injected SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-21 09:50:50.934520 (Thread-1): finished collecting timing info
2021-12-21 09:50:50.951051 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 09:50:50.958591 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 09:50:51.336410 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37798), raddr=('142.251.10.95', 443)>
2021-12-21 09:50:51.336917 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37280), raddr=('34.101.5.74', 443)>
2021-12-21 09:50:51.337157 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37802), raddr=('142.251.10.95', 443)>
2021-12-21 09:50:51.337411 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37284), raddr=('34.101.5.74', 443)>
2021-12-21 09:50:51.347980 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-21 09:50:51.348260 (Thread-1): On model.my_new_project.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_first_dbt_model"} */


  create or replace table `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



create or replace table `playground-325606.dataset_dbt.dqa_uniqueness_belajar_analytics_266501688` as with x as ( 
select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result

select *
from dqa_uniqueness_belajar_analytics_266501688

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 09:50:51.800571 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected "(" or keyword SELECT or keyword WITH but got keyword CREATE at [19:1]')
2021-12-21 09:50:52.275524 (Thread-1): finished collecting timing info
2021-12-21 09:50:52.276583 (Thread-1): Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  Syntax error: Expected "(" or keyword SELECT or keyword WITH but got keyword CREATE at [19:1]
  compiled SQL at target/run/my_new_project/models/example/my_first_dbt_model.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Expected "(" or keyword SELECT or keyword WITH but got keyword CREATE at [19:1]

(job ID: 52a3bb54-7200-4975-ad23-68b3363c613c)

                                                                                                                                                                                                                                                            -----Query Job SQL Follows-----                                                                                                                                                                                                                                                            

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_first_dbt_model"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    /*
  10:    Welcome to your first dbt model!
  11:    Did you know that you can also configure models directly within SQL files?
  12:    This will override configurations stated in dbt_project.yml
  13:
  14:    Try changing "table" to "view" below
  15:*/
  16:
  17:
  18:
  19:create or replace table `playground-325606.dataset_dbt.dqa_uniqueness_belajar_analytics_266501688` as with x as ( 
  20:select distinct 
  21:date_gmt7,
  22:time_gmt7,
  23:hour_gmt7, 
  24:timestamp_gmt7,
  25:event_name,
  26:ga_session_id,
  27:user_id,
  28:event_params_page_title,
  29:event_params_product,
  30:action_type_cleaned, count(1) total
  31:FROM `playground-325606.dataset_dbt.event_tracker_belajar`
  32:group by date_gmt7,
  33:time_gmt7,
  34:hour_gmt7, 
  35:timestamp_gmt7,
  36:event_name,
  37:ga_session_id,
  38:user_id,
  39:event_params_page_title,
  40:event_params_product,
  41:action_type_cleaned)
  42:
  43:select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
  44:from (
  45:select 'Uniqueness' criteria,
  46:'duplicate in rows' metrics,
  47:count(x.total ) as total_data,
  48:count(
  49:    case 
  50:    when (x.total = 1) then 'pass' end ) as good_data,
  51:count(
  52:    case
  53:    when (x.total > 1) then 'fail' end ) as bad_data        
  54:from x
  55:)result
  56:
  57:select *
  58:from dqa_uniqueness_belajar_analytics_266501688
  59:
  60:/*
  61:    Uncomment the line below to remove records with null `id` values
  62:*/
  63:
  64:-- where id is not null
  65:  );
  66:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  Syntax error: Expected "(" or keyword SELECT or keyword WITH but got keyword CREATE at [19:1]
  compiled SQL at target/run/my_new_project/models/example/my_first_dbt_model.sql
2021-12-21 09:50:52.285627 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5719370f-6edd-4a23-b545-20741d9f055e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb96d83370>]}
2021-12-21 09:50:52.286398 (Thread-1): 16:50:52 | 1 of 2 ERROR creating table model testingBigQuery.my_first_dbt_model. [ERROR in 1.36s]
2021-12-21 09:50:52.286816 (Thread-1): Finished running node model.my_new_project.my_first_dbt_model
2021-12-21 09:50:52.288433 (Thread-1): Began running node model.my_new_project.my_second_dbt_model
2021-12-21 09:50:52.289075 (Thread-1): 16:50:52 | 2 of 2 SKIP relation testingBigQuery.my_second_dbt_model............. [SKIP]
2021-12-21 09:50:52.289607 (Thread-1): Finished running node model.my_new_project.my_second_dbt_model
2021-12-21 09:50:52.291738 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 09:50:52.292555 (MainThread): 16:50:52 | 
2021-12-21 09:50:52.292989 (MainThread): 16:50:52 | Finished running 1 table model, 1 view model in 2.82s.
2021-12-21 09:50:52.293562 (MainThread): Connection 'master' was properly closed.
2021-12-21 09:50:52.293999 (MainThread): Connection 'model.my_new_project.my_first_dbt_model' was properly closed.
2021-12-21 09:50:52.300551 (MainThread): 
2021-12-21 09:50:52.300808 (MainThread): Completed with 1 error and 0 warnings:
2021-12-21 09:50:52.300984 (MainThread): 
2021-12-21 09:50:52.301155 (MainThread): Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
2021-12-21 09:50:52.301334 (MainThread):   Syntax error: Expected "(" or keyword SELECT or keyword WITH but got keyword CREATE at [19:1]
2021-12-21 09:50:52.301474 (MainThread):   compiled SQL at target/run/my_new_project/models/example/my_first_dbt_model.sql
2021-12-21 09:50:52.301614 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
2021-12-21 09:50:52.301835 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb96edca00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb96d999d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb96dcf7f0>]}
2021-12-21 09:50:52.302072 (MainThread): Flushing usage events
2021-12-21 09:51:32.806888 (MainThread): Running with dbt=0.21.0
2021-12-21 09:51:32.963024 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 09:51:32.963590 (MainThread): Tracking: tracking
2021-12-21 09:51:32.969184 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee02a28c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee00c7b790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee00c7b7c0>]}
2021-12-21 09:51:32.975676 (MainThread): Partial parsing not enabled
2021-12-21 09:51:32.979862 (MainThread): Parsing macros/catalog.sql
2021-12-21 09:51:32.983896 (MainThread): Parsing macros/etc.sql
2021-12-21 09:51:32.985471 (MainThread): Parsing macros/adapters.sql
2021-12-21 09:51:32.998725 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 09:51:33.000376 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 09:51:33.002096 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 09:51:33.003704 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 09:51:33.004779 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 09:51:33.010379 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 09:51:33.018916 (MainThread): Parsing macros/core.sql
2021-12-21 09:51:33.021059 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 09:51:33.022131 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 09:51:33.023550 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 09:51:33.024294 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 09:51:33.025123 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 09:51:33.029448 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 09:51:33.034932 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 09:51:33.048198 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 09:51:33.052198 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 09:51:33.054473 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 09:51:33.065577 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 09:51:33.066621 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 09:51:33.072421 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 09:51:33.080316 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 09:51:33.084386 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 09:51:33.093769 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 09:51:33.094778 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 09:51:33.111740 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 09:51:33.112700 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 09:51:33.113696 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 09:51:33.114558 (MainThread): Parsing macros/etc/query.sql
2021-12-21 09:51:33.115158 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 09:51:33.116665 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 09:51:33.117715 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 09:51:33.122443 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 09:51:33.268674 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-21 09:51:33.276124 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-12-21 09:51:33.291038 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:51:33.292124 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:51:33.293111 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:51:33.294230 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:51:33.302769 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '39e2b6c9-7bb3-4c40-bb68-847f621718dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee00bb7700>]}
2021-12-21 09:51:33.305537 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 09:51:33.305791 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '39e2b6c9-7bb3-4c40-bb68-847f621718dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee00c7ba00>]}
2021-12-21 09:51:33.305971 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 09:51:33.306954 (MainThread): 
2021-12-21 09:51:33.307191 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 09:51:33.307903 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 09:51:33.308157 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 09:51:33.816768 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_testingBigQuery".
2021-12-21 09:51:33.817438 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 09:51:33.823649 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 09:51:34.169725 (MainThread): 16:51:34 | Concurrency: 1 threads (target='prod')
2021-12-21 09:51:34.170218 (MainThread): 16:51:34 | 
2021-12-21 09:51:34.174293 (Thread-1): Began running node model.my_new_project.my_first_dbt_model
2021-12-21 09:51:34.175066 (Thread-1): 16:51:34 | 1 of 2 START table model testingBigQuery.my_first_dbt_model.......... [RUN]
2021-12-21 09:51:34.175916 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-21 09:51:34.176330 (Thread-1): Compiling model.my_new_project.my_first_dbt_model
2021-12-21 09:51:34.181392 (Thread-1): Writing injected SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-21 09:51:34.182205 (Thread-1): finished collecting timing info
2021-12-21 09:51:34.199406 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 09:51:34.203299 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 09:51:34.553884 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37814), raddr=('142.251.10.95', 443)>
2021-12-21 09:51:34.554281 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37296), raddr=('34.101.5.74', 443)>
2021-12-21 09:51:34.554543 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37818), raddr=('142.251.10.95', 443)>
2021-12-21 09:51:34.554777 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37300), raddr=('34.101.5.74', 443)>
2021-12-21 09:51:34.564827 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-21 09:51:34.565094 (Thread-1): On model.my_new_project.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_first_dbt_model"} */


  create or replace table `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



create or replace table `playground-325606.dataset_dbt.dqa_uniqueness_belajar_analytics_266501688` as with x as ( 
select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 09:51:35.040122 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected "(" or keyword SELECT or keyword WITH but got keyword CREATE at [19:1]')
2021-12-21 09:51:36.023730 (Thread-1): finished collecting timing info
2021-12-21 09:51:36.024651 (Thread-1): Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  Syntax error: Expected "(" or keyword SELECT or keyword WITH but got keyword CREATE at [19:1]
  compiled SQL at target/run/my_new_project/models/example/my_first_dbt_model.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Expected "(" or keyword SELECT or keyword WITH but got keyword CREATE at [19:1]

(job ID: 9436dc8c-ee3e-4ee2-8fb5-95630fd3d3a0)

                                                                                                                                                                                                                                                            -----Query Job SQL Follows-----                                                                                                                                                                                                                                                            

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_first_dbt_model"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    /*
  10:    Welcome to your first dbt model!
  11:    Did you know that you can also configure models directly within SQL files?
  12:    This will override configurations stated in dbt_project.yml
  13:
  14:    Try changing "table" to "view" below
  15:*/
  16:
  17:
  18:
  19:create or replace table `playground-325606.dataset_dbt.dqa_uniqueness_belajar_analytics_266501688` as with x as ( 
  20:select distinct 
  21:date_gmt7,
  22:time_gmt7,
  23:hour_gmt7, 
  24:timestamp_gmt7,
  25:event_name,
  26:ga_session_id,
  27:user_id,
  28:event_params_page_title,
  29:event_params_product,
  30:action_type_cleaned, count(1) total
  31:FROM `playground-325606.dataset_dbt.event_tracker_belajar`
  32:group by date_gmt7,
  33:time_gmt7,
  34:hour_gmt7, 
  35:timestamp_gmt7,
  36:event_name,
  37:ga_session_id,
  38:user_id,
  39:event_params_page_title,
  40:event_params_product,
  41:action_type_cleaned)
  42:
  43:select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
  44:from (
  45:select 'Uniqueness' criteria,
  46:'duplicate in rows' metrics,
  47:count(x.total ) as total_data,
  48:count(
  49:    case 
  50:    when (x.total = 1) then 'pass' end ) as good_data,
  51:count(
  52:    case
  53:    when (x.total > 1) then 'fail' end ) as bad_data        
  54:from x
  55:)result
  56:
  57:select *
  58:from x
  59:
  60:/*
  61:    Uncomment the line below to remove records with null `id` values
  62:*/
  63:
  64:-- where id is not null
  65:  );
  66:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  Syntax error: Expected "(" or keyword SELECT or keyword WITH but got keyword CREATE at [19:1]
  compiled SQL at target/run/my_new_project/models/example/my_first_dbt_model.sql
2021-12-21 09:51:36.029071 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '39e2b6c9-7bb3-4c40-bb68-847f621718dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee00c6d1c0>]}
2021-12-21 09:51:36.029947 (Thread-1): 16:51:36 | 1 of 2 ERROR creating table model testingBigQuery.my_first_dbt_model. [ERROR in 1.85s]
2021-12-21 09:51:36.030407 (Thread-1): Finished running node model.my_new_project.my_first_dbt_model
2021-12-21 09:51:36.031846 (Thread-1): Began running node model.my_new_project.my_second_dbt_model
2021-12-21 09:51:36.032318 (Thread-1): 16:51:36 | 2 of 2 SKIP relation testingBigQuery.my_second_dbt_model............. [SKIP]
2021-12-21 09:51:36.032738 (Thread-1): Finished running node model.my_new_project.my_second_dbt_model
2021-12-21 09:51:36.034535 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 09:51:36.035249 (MainThread): 16:51:36 | 
2021-12-21 09:51:36.035626 (MainThread): 16:51:36 | Finished running 1 table model, 1 view model in 2.73s.
2021-12-21 09:51:36.036008 (MainThread): Connection 'master' was properly closed.
2021-12-21 09:51:36.036455 (MainThread): Connection 'model.my_new_project.my_first_dbt_model' was properly closed.
2021-12-21 09:51:36.045675 (MainThread): 
2021-12-21 09:51:36.046040 (MainThread): Completed with 1 error and 0 warnings:
2021-12-21 09:51:36.046233 (MainThread): 
2021-12-21 09:51:36.046417 (MainThread): Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
2021-12-21 09:51:36.046586 (MainThread):   Syntax error: Expected "(" or keyword SELECT or keyword WITH but got keyword CREATE at [19:1]
2021-12-21 09:51:36.046759 (MainThread):   compiled SQL at target/run/my_new_project/models/example/my_first_dbt_model.sql
2021-12-21 09:51:36.046944 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
2021-12-21 09:51:36.047218 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee043bebe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee00b76df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee00c9eb80>]}
2021-12-21 09:51:36.047534 (MainThread): Flushing usage events
2021-12-21 09:54:36.927383 (MainThread): Running with dbt=0.21.0
2021-12-21 09:54:37.106367 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 09:54:37.107110 (MainThread): Tracking: tracking
2021-12-21 09:54:37.117100 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a96ad850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a79035e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a7903640>]}
2021-12-21 09:54:37.129568 (MainThread): Partial parsing not enabled
2021-12-21 09:54:37.135921 (MainThread): Parsing macros/catalog.sql
2021-12-21 09:54:37.140966 (MainThread): Parsing macros/etc.sql
2021-12-21 09:54:37.142702 (MainThread): Parsing macros/adapters.sql
2021-12-21 09:54:37.167834 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 09:54:37.171074 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 09:54:37.174609 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 09:54:37.177493 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 09:54:37.179148 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 09:54:37.188845 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 09:54:37.203238 (MainThread): Parsing macros/core.sql
2021-12-21 09:54:37.206179 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 09:54:37.207435 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 09:54:37.209180 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 09:54:37.210121 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 09:54:37.211143 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 09:54:37.216631 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 09:54:37.223885 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 09:54:37.241190 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 09:54:37.246570 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 09:54:37.249597 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 09:54:37.263987 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 09:54:37.265373 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 09:54:37.273048 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 09:54:37.283247 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 09:54:37.288650 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 09:54:37.301342 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 09:54:37.302616 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 09:54:37.324848 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 09:54:37.326072 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 09:54:37.327285 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 09:54:37.328385 (MainThread): Parsing macros/etc/query.sql
2021-12-21 09:54:37.329156 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 09:54:37.330964 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 09:54:37.332317 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 09:54:37.338614 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 09:54:37.528871 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-21 09:54:37.538512 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-12-21 09:54:37.559770 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:54:37.561216 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:54:37.562541 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:54:37.563968 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:54:37.575296 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc7e3252-a001-4d3d-889f-f2068a03093f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a77af1c0>]}
2021-12-21 09:54:37.578811 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 09:54:37.579088 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc7e3252-a001-4d3d-889f-f2068a03093f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a77af040>]}
2021-12-21 09:54:37.579305 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 09:54:37.580281 (MainThread): 
2021-12-21 09:54:37.580554 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 09:54:37.581282 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 09:54:37.581504 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 09:54:38.155246 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_testingBigQuery".
2021-12-21 09:54:38.155974 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 09:54:38.168029 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 09:54:38.527476 (MainThread): 16:54:38 | Concurrency: 1 threads (target='prod')
2021-12-21 09:54:38.528015 (MainThread): 16:54:38 | 
2021-12-21 09:54:38.532431 (Thread-1): Began running node model.my_new_project.my_first_dbt_model
2021-12-21 09:54:38.533563 (Thread-1): 16:54:38 | 1 of 2 START table model testingBigQuery.my_first_dbt_model.......... [RUN]
2021-12-21 09:54:38.534735 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-21 09:54:38.535395 (Thread-1): Compiling model.my_new_project.my_first_dbt_model
2021-12-21 09:54:38.537967 (Thread-1): Writing injected SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-21 09:54:38.538271 (Thread-1): finished collecting timing info
2021-12-21 09:54:38.549880 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 09:54:38.553190 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 09:54:38.910949 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 40656), raddr=('74.125.68.95', 443)>
2021-12-21 09:54:38.911361 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37328), raddr=('34.101.5.74', 443)>
2021-12-21 09:54:38.911630 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 40660), raddr=('74.125.68.95', 443)>
2021-12-21 09:54:38.911870 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37332), raddr=('34.101.5.74', 443)>
2021-12-21 09:54:38.922878 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-21 09:54:38.923198 (Thread-1): On model.my_new_project.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_first_dbt_model"} */


  create or replace table `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 09:54:43.222853 (Thread-1): finished collecting timing info
2021-12-21 09:54:43.223933 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc7e3252-a001-4d3d-889f-f2068a03093f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a780f100>]}
2021-12-21 09:54:43.224722 (Thread-1): 16:54:43 | 1 of 2 OK created table model testingBigQuery.my_first_dbt_model..... [CREATE TABLE (7.8k rows, 1018.8 KB processed) in 4.69s]
2021-12-21 09:54:43.225117 (Thread-1): Finished running node model.my_new_project.my_first_dbt_model
2021-12-21 09:54:43.226225 (Thread-1): Began running node model.my_new_project.my_second_dbt_model
2021-12-21 09:54:43.226919 (Thread-1): 16:54:43 | 2 of 2 START view model testingBigQuery.my_second_dbt_model.......... [RUN]
2021-12-21 09:54:43.227547 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-12-21 09:54:43.227851 (Thread-1): Compiling model.my_new_project.my_second_dbt_model
2021-12-21 09:54:43.231019 (Thread-1): Writing injected SQL for node "model.my_new_project.my_second_dbt_model"
2021-12-21 09:54:43.231408 (Thread-1): finished collecting timing info
2021-12-21 09:54:43.243528 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_second_dbt_model"
2021-12-21 09:54:43.243860 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 09:54:43.247225 (Thread-1): On model.my_new_project.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_second_dbt_model"} */


  create or replace view `playground-325606`.`testingBigQuery`.`my_second_dbt_model`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`testingBigQuery`.`my_first_dbt_model`;


2021-12-21 09:54:45.344469 (Thread-1): finished collecting timing info
2021-12-21 09:54:45.345546 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc7e3252-a001-4d3d-889f-f2068a03093f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a7761d90>]}
2021-12-21 09:54:45.346471 (Thread-1): 16:54:45 | 2 of 2 OK created view model testingBigQuery.my_second_dbt_model..... [OK in 2.12s]
2021-12-21 09:54:45.346965 (Thread-1): Finished running node model.my_new_project.my_second_dbt_model
2021-12-21 09:54:45.349273 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 09:54:45.350140 (MainThread): 16:54:45 | 
2021-12-21 09:54:45.350574 (MainThread): 16:54:45 | Finished running 1 table model, 1 view model in 7.77s.
2021-12-21 09:54:45.351145 (MainThread): Connection 'master' was properly closed.
2021-12-21 09:54:45.351648 (MainThread): Connection 'model.my_new_project.my_second_dbt_model' was properly closed.
2021-12-21 09:54:45.360772 (MainThread): 
2021-12-21 09:54:45.361023 (MainThread): Completed successfully
2021-12-21 09:54:45.361242 (MainThread): 
Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
2021-12-21 09:54:45.361532 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79ab04ba90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a77af070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a78d9a00>]}
2021-12-21 09:54:45.361818 (MainThread): Flushing usage events
2021-12-21 09:57:09.035792 (MainThread): Running with dbt=0.21.0
2021-12-21 09:57:09.193237 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 09:57:09.193843 (MainThread): Tracking: tracking
2021-12-21 09:57:09.200352 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f605d622640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f605b8758b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f605b8758e0>]}
2021-12-21 09:57:09.207303 (MainThread): Partial parsing not enabled
2021-12-21 09:57:09.211510 (MainThread): Parsing macros/catalog.sql
2021-12-21 09:57:09.215497 (MainThread): Parsing macros/etc.sql
2021-12-21 09:57:09.216691 (MainThread): Parsing macros/adapters.sql
2021-12-21 09:57:09.229671 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 09:57:09.231215 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 09:57:09.232914 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 09:57:09.234562 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 09:57:09.235571 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 09:57:09.241496 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 09:57:09.250152 (MainThread): Parsing macros/core.sql
2021-12-21 09:57:09.252427 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 09:57:09.253521 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 09:57:09.255001 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 09:57:09.255784 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 09:57:09.256606 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 09:57:09.260860 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 09:57:09.266520 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 09:57:09.279912 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 09:57:09.284138 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 09:57:09.286504 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 09:57:09.298593 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 09:57:09.299780 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 09:57:09.306048 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 09:57:09.314163 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 09:57:09.318548 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 09:57:09.328566 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 09:57:09.329647 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 09:57:09.348220 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 09:57:09.349228 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 09:57:09.350221 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 09:57:09.351175 (MainThread): Parsing macros/etc/query.sql
2021-12-21 09:57:09.351817 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 09:57:09.353355 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 09:57:09.354495 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 09:57:09.359591 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 09:57:09.509103 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-21 09:57:09.516841 (MainThread): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-12-21 09:57:09.533942 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:57:09.535148 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:57:09.536231 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:57:09.537394 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 09:57:09.546042 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b1725753-54da-4832-9ff0-0d0d17101423', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f605b7260d0>]}
2021-12-21 09:57:09.548804 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 09:57:09.549045 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b1725753-54da-4832-9ff0-0d0d17101423', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f605b7b2ac0>]}
2021-12-21 09:57:09.549229 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 09:57:09.550108 (MainThread): 
2021-12-21 09:57:09.550335 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 09:57:09.550940 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 09:57:09.551180 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 09:57:10.176847 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_testingBigQuery".
2021-12-21 09:57:10.177597 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 09:57:10.189392 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 09:57:10.538449 (MainThread): 16:57:10 | Concurrency: 1 threads (target='prod')
2021-12-21 09:57:10.538991 (MainThread): 16:57:10 | 
2021-12-21 09:57:10.543063 (Thread-1): Began running node model.my_new_project.my_first_dbt_model
2021-12-21 09:57:10.544207 (Thread-1): 16:57:10 | 1 of 2 START table model testingBigQuery.my_first_dbt_model.......... [RUN]
2021-12-21 09:57:10.545441 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_first_dbt_model".
2021-12-21 09:57:10.546123 (Thread-1): Compiling model.my_new_project.my_first_dbt_model
2021-12-21 09:57:10.552120 (Thread-1): Writing injected SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-21 09:57:10.552859 (Thread-1): finished collecting timing info
2021-12-21 09:57:10.569079 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 09:57:10.572505 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 09:57:10.921574 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 33296), raddr=('74.125.200.95', 443)>
2021-12-21 09:57:10.922095 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49946), raddr=('34.101.5.42', 443)>
2021-12-21 09:57:10.922468 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 33300), raddr=('74.125.200.95', 443)>
2021-12-21 09:57:10.922767 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49950), raddr=('34.101.5.42', 443)>
2021-12-21 09:57:10.933620 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_first_dbt_model"
2021-12-21 09:57:10.933958 (Thread-1): On model.my_new_project.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_first_dbt_model"} */


  create or replace table `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 09:57:14.399022 (Thread-1): finished collecting timing info
2021-12-21 09:57:14.399860 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b1725753-54da-4832-9ff0-0d0d17101423', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f605b7915e0>]}
2021-12-21 09:57:14.400582 (Thread-1): 16:57:14 | 1 of 2 OK created table model testingBigQuery.my_first_dbt_model..... [CREATE TABLE (7.8k rows, 1018.8 KB processed) in 3.85s]
2021-12-21 09:57:14.400999 (Thread-1): Finished running node model.my_new_project.my_first_dbt_model
2021-12-21 09:57:14.402269 (Thread-1): Began running node model.my_new_project.my_second_dbt_model
2021-12-21 09:57:14.402890 (Thread-1): 16:57:14 | 2 of 2 START view model testingBigQuery.my_second_dbt_model.......... [RUN]
2021-12-21 09:57:14.403573 (Thread-1): Acquiring new bigquery connection "model.my_new_project.my_second_dbt_model".
2021-12-21 09:57:14.403917 (Thread-1): Compiling model.my_new_project.my_second_dbt_model
2021-12-21 09:57:14.407588 (Thread-1): Writing injected SQL for node "model.my_new_project.my_second_dbt_model"
2021-12-21 09:57:14.408009 (Thread-1): finished collecting timing info
2021-12-21 09:57:14.422091 (Thread-1): Writing runtime SQL for node "model.my_new_project.my_second_dbt_model"
2021-12-21 09:57:14.422470 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 09:57:14.425821 (Thread-1): On model.my_new_project.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_second_dbt_model"} */


  create or replace view `playground-325606`.`testingBigQuery`.`my_second_dbt_model`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select ate_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
from `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
group by group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned
having count(1) > 1;


2021-12-21 09:57:14.990045 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Syntax error: Unexpected keyword GROUP at [19:10]')
2021-12-21 09:57:16.393259 (Thread-1): finished collecting timing info
2021-12-21 09:57:16.394218 (Thread-1): Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/my_second_dbt_model.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Unexpected keyword GROUP at [19:10]

(job ID: 36b586a5-0326-426b-9de6-a7f49dd2a343)

                                                              -----Query Job SQL Follows-----                                                              

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.my_second_dbt_model"} */
   2:
   3:
   4:  create or replace view `playground-325606`.`testingBigQuery`.`my_second_dbt_model`
   5:  OPTIONS()
   6:  as -- Use the `ref` function to select from other models
   7:
   8:select ate_gmt7,
   9:time_gmt7,
  10:hour_gmt7, 
  11:timestamp_gmt7,
  12:event_name,
  13:ga_session_id,
  14:user_id,
  15:event_params_page_title,
  16:event_params_product,
  17:action_type_cleaned, count(1) total
  18:from `playground-325606`.`testingBigQuery`.`my_first_dbt_model`
  19:group by group by date_gmt7,
  20:time_gmt7,
  21:hour_gmt7, 
  22:timestamp_gmt7,
  23:event_name,
  24:ga_session_id,
  25:user_id,
  26:event_params_page_title,
  27:event_params_product,
  28:action_type_cleaned
  29:having count(1) > 1;
  30:
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 22, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/my_second_dbt_model.sql
2021-12-21 09:57:16.397437 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b1725753-54da-4832-9ff0-0d0d17101423', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f605b6e0250>]}
2021-12-21 09:57:16.397808 (Thread-1): 16:57:16 | 2 of 2 ERROR creating view model testingBigQuery.my_second_dbt_model. [ERROR in 1.99s]
2021-12-21 09:57:16.398010 (Thread-1): Finished running node model.my_new_project.my_second_dbt_model
2021-12-21 09:57:16.398801 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 09:57:16.399028 (MainThread): 16:57:16 | 
2021-12-21 09:57:16.399145 (MainThread): 16:57:16 | Finished running 1 table model, 1 view model in 6.85s.
2021-12-21 09:57:16.399295 (MainThread): Connection 'master' was properly closed.
2021-12-21 09:57:16.399429 (MainThread): Connection 'model.my_new_project.my_second_dbt_model' was properly closed.
2021-12-21 09:57:16.407033 (MainThread): 
2021-12-21 09:57:16.407434 (MainThread): Completed with 1 error and 0 warnings:
2021-12-21 09:57:16.407721 (MainThread): 
2021-12-21 09:57:16.407818 (MainThread): Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
2021-12-21 09:57:16.407909 (MainThread):   Syntax error: Unexpected keyword GROUP at [19:10]
2021-12-21 09:57:16.407993 (MainThread):   compiled SQL at target/run/my_new_project/models/example/my_second_dbt_model.sql
2021-12-21 09:57:16.408087 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
2021-12-21 09:57:16.408240 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f605b7baa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f605b7ff9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f605efbac40>]}
2021-12-21 09:57:16.408403 (MainThread): Flushing usage events
2021-12-21 10:01:28.214211 (MainThread): Running with dbt=0.21.0
2021-12-21 10:01:28.386054 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 10:01:28.386705 (MainThread): Tracking: tracking
2021-12-21 10:01:28.393441 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7be9e57c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7bcc7f610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7bcc7f670>]}
2021-12-21 10:01:28.400885 (MainThread): Partial parsing not enabled
2021-12-21 10:01:28.405754 (MainThread): Parsing macros/catalog.sql
2021-12-21 10:01:28.410347 (MainThread): Parsing macros/etc.sql
2021-12-21 10:01:28.411790 (MainThread): Parsing macros/adapters.sql
2021-12-21 10:01:28.427310 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 10:01:28.429378 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 10:01:28.431437 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 10:01:28.433369 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 10:01:28.434702 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 10:01:28.441644 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 10:01:28.451884 (MainThread): Parsing macros/core.sql
2021-12-21 10:01:28.454784 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 10:01:28.456042 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 10:01:28.457680 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 10:01:28.458628 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 10:01:28.459648 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 10:01:28.465147 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 10:01:28.471750 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 10:01:28.488225 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 10:01:28.493575 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 10:01:28.496615 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 10:01:28.511370 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 10:01:28.512876 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 10:01:28.520253 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 10:01:28.529674 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 10:01:28.534913 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 10:01:28.547073 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 10:01:28.548468 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 10:01:28.569135 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 10:01:28.570462 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 10:01:28.571659 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 10:01:28.572789 (MainThread): Parsing macros/etc/query.sql
2021-12-21 10:01:28.573579 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 10:01:28.575327 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 10:01:28.576667 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 10:01:28.582502 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 10:01:28.757081 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:01:28.766176 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:01:28.785874 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:01:28.787276 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:01:28.788588 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:01:28.790059 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:01:28.790394 (MainThread): [WARNING]: Test 'test.my_new_project.unique_my_first_dbt_model_id.16e066b321' (models/example/schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2021-12-21 10:01:28.790580 (MainThread): [WARNING]: Test 'test.my_new_project.not_null_my_first_dbt_model_id.5fb22c2710' (models/example/schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2021-12-21 10:01:28.790811 (MainThread): [WARNING]: Test 'test.my_new_project.unique_my_second_dbt_model_id.57a0f8c493' (models/example/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2021-12-21 10:01:28.790991 (MainThread): [WARNING]: Test 'test.my_new_project.not_null_my_second_dbt_model_id.151b76d778' (models/example/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2021-12-21 10:01:28.803726 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0a4bdd77-59d2-4e39-a6a7-9668e2baa1b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7bcae8f70>]}
2021-12-21 10:01:28.807693 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 10:01:28.808104 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0a4bdd77-59d2-4e39-a6a7-9668e2baa1b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7bcb73910>]}
2021-12-21 10:01:28.808434 (MainThread): Found 2 models, 0 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 10:01:28.809571 (MainThread): 
2021-12-21 10:01:28.809939 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:01:28.810818 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 10:01:28.811045 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 10:01:29.661549 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_testingBigQuery".
2021-12-21 10:01:29.662221 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 10:01:29.671702 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:01:30.043947 (MainThread): 17:01:30 | Concurrency: 1 threads (target='prod')
2021-12-21 10:01:30.044474 (MainThread): 17:01:30 | 
2021-12-21 10:01:30.048726 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:01:30.049516 (Thread-1): 17:01:30 | 1 of 2 START table model testingBigQuery.dqa_uniqueness_1............ [RUN]
2021-12-21 10:01:30.050262 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:01:30.050697 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-21 10:01:30.056651 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:01:30.057433 (Thread-1): finished collecting timing info
2021-12-21 10:01:30.084628 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49456), raddr=('172.217.194.95', 443)>
2021-12-21 10:01:30.084844 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49998), raddr=('34.101.5.42', 443)>
2021-12-21 10:01:30.084968 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49460), raddr=('172.217.194.95', 443)>
2021-12-21 10:01:30.085073 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 50002), raddr=('34.101.5.42', 443)>
2021-12-21 10:01:30.088108 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:01:30.088412 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:01:30.091852 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 10:01:32.669081 (Thread-1): finished collecting timing info
2021-12-21 10:01:32.669966 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a4bdd77-59d2-4e39-a6a7-9668e2baa1b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7bca87f70>]}
2021-12-21 10:01:32.670666 (Thread-1): 17:01:32 | 1 of 2 OK created table model testingBigQuery.dqa_uniqueness_1....... [CREATE TABLE (7.8k rows, 1018.8 KB processed) in 2.62s]
2021-12-21 10:01:32.671088 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:01:32.672430 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:01:32.672975 (Thread-1): 17:01:32 | 2 of 2 START view model testingBigQuery.dqa_uniqueness_2............. [RUN]
2021-12-21 10:01:32.673618 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:01:32.673931 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-21 10:01:32.677556 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:01:32.678227 (Thread-1): finished collecting timing info
2021-12-21 10:01:32.695140 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:01:32.695532 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:01:32.698923 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`testingBigQuery`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select ate_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
from `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
group by group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned
having count(1) > 1;


2021-12-21 10:01:33.235073 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Syntax error: Unexpected keyword GROUP at [19:10]')
2021-12-21 10:01:34.626589 (Thread-1): finished collecting timing info
2021-12-21 10:01:34.627520 (Thread-1): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Unexpected keyword GROUP at [19:10]

(job ID: bde4309a-8fc8-48b1-b105-da91eb5501f7)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */
   2:
   3:
   4:  create or replace view `playground-325606`.`testingBigQuery`.`dqa_uniqueness_2`
   5:  OPTIONS()
   6:  as -- Use the `ref` function to select from other models
   7:
   8:select ate_gmt7,
   9:time_gmt7,
  10:hour_gmt7, 
  11:timestamp_gmt7,
  12:event_name,
  13:ga_session_id,
  14:user_id,
  15:event_params_page_title,
  16:event_params_product,
  17:action_type_cleaned, count(1) total
  18:from `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
  19:group by group by date_gmt7,
  20:time_gmt7,
  21:hour_gmt7, 
  22:timestamp_gmt7,
  23:event_name,
  24:ga_session_id,
  25:user_id,
  26:event_params_page_title,
  27:event_params_product,
  28:action_type_cleaned
  29:having count(1) > 1;
  30:
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 22, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:01:34.631754 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a4bdd77-59d2-4e39-a6a7-9668e2baa1b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7bc20c880>]}
2021-12-21 10:01:34.632566 (Thread-1): 17:01:34 | 2 of 2 ERROR creating view model testingBigQuery.dqa_uniqueness_2.... [ERROR in 1.96s]
2021-12-21 10:01:34.633102 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:01:34.635490 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:01:34.636299 (MainThread): 17:01:34 | 
2021-12-21 10:01:34.636696 (MainThread): 17:01:34 | Finished running 1 table model, 1 view model in 5.83s.
2021-12-21 10:01:34.637019 (MainThread): Connection 'master' was properly closed.
2021-12-21 10:01:34.637346 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-21 10:01:34.646910 (MainThread): 
2021-12-21 10:01:34.647394 (MainThread): Completed with 1 error and 0 warnings:
2021-12-21 10:01:34.647773 (MainThread): 
2021-12-21 10:01:34.648150 (MainThread): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
2021-12-21 10:01:34.648549 (MainThread):   Syntax error: Unexpected keyword GROUP at [19:10]
2021-12-21 10:01:34.648900 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:01:34.649350 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
2021-12-21 10:01:34.649835 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7bcae8ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7bcb32a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7bcc196d0>]}
2021-12-21 10:01:34.650348 (MainThread): Flushing usage events
2021-12-21 10:02:03.698943 (MainThread): Running with dbt=0.21.0
2021-12-21 10:02:03.878075 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 10:02:03.878711 (MainThread): Tracking: tracking
2021-12-21 10:02:03.885274 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3d0885640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3ceb1f640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3ceb1f6a0>]}
2021-12-21 10:02:03.892664 (MainThread): Partial parsing not enabled
2021-12-21 10:02:03.897401 (MainThread): Parsing macros/catalog.sql
2021-12-21 10:02:03.902226 (MainThread): Parsing macros/etc.sql
2021-12-21 10:02:03.903784 (MainThread): Parsing macros/adapters.sql
2021-12-21 10:02:03.920232 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 10:02:03.922284 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 10:02:03.924181 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 10:02:03.926143 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 10:02:03.927368 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 10:02:03.934430 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 10:02:03.944711 (MainThread): Parsing macros/core.sql
2021-12-21 10:02:03.947322 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 10:02:03.948479 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 10:02:03.950170 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 10:02:03.951002 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 10:02:03.952073 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 10:02:03.957190 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 10:02:03.963724 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 10:02:03.979341 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 10:02:03.984160 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 10:02:03.986995 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 10:02:04.000535 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 10:02:04.001917 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 10:02:04.009192 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 10:02:04.018439 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 10:02:04.023428 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 10:02:04.035021 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 10:02:04.036486 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 10:02:04.057139 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 10:02:04.058386 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 10:02:04.059517 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 10:02:04.060590 (MainThread): Parsing macros/etc/query.sql
2021-12-21 10:02:04.061349 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 10:02:04.063035 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 10:02:04.064282 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 10:02:04.070053 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 10:02:04.245748 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:02:04.254929 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:02:04.274550 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:02:04.275835 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:02:04.277179 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:02:04.278542 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:02:04.289226 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3c400b53-d995-44e2-ab89-5fc5306a6206', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3ce99f4c0>]}
2021-12-21 10:02:04.292593 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 10:02:04.292876 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3c400b53-d995-44e2-ab89-5fc5306a6206', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3cea13940>]}
2021-12-21 10:02:04.293091 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 10:02:04.294248 (MainThread): 
2021-12-21 10:02:04.294533 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:02:04.295209 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 10:02:04.295442 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 10:02:04.802325 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_testingBigQuery".
2021-12-21 10:02:04.802933 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 10:02:04.813909 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:02:05.167983 (MainThread): 17:02:05 | Concurrency: 1 threads (target='prod')
2021-12-21 10:02:05.168558 (MainThread): 17:02:05 | 
2021-12-21 10:02:05.172585 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:02:05.172838 (Thread-1): 17:02:05 | 1 of 2 START table model testingBigQuery.dqa_uniqueness_1............ [RUN]
2021-12-21 10:02:05.173071 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:02:05.173199 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-21 10:02:05.175104 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:02:05.175402 (Thread-1): finished collecting timing info
2021-12-21 10:02:05.195349 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:02:05.198632 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:02:05.551313 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49480), raddr=('172.217.194.95', 443)>
2021-12-21 10:02:05.551693 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 50022), raddr=('34.101.5.42', 443)>
2021-12-21 10:02:05.551969 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49484), raddr=('172.217.194.95', 443)>
2021-12-21 10:02:05.552207 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 50026), raddr=('34.101.5.42', 443)>
2021-12-21 10:02:05.561704 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:02:05.561993 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 10:02:08.804242 (Thread-1): finished collecting timing info
2021-12-21 10:02:08.805327 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c400b53-d995-44e2-ab89-5fc5306a6206', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3ce988880>]}
2021-12-21 10:02:08.806228 (Thread-1): 17:02:08 | 1 of 2 OK created table model testingBigQuery.dqa_uniqueness_1....... [CREATE TABLE (7.8k rows, 1018.8 KB processed) in 3.63s]
2021-12-21 10:02:08.806684 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:02:08.808209 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:02:08.808793 (Thread-1): 17:02:08 | 2 of 2 START view model testingBigQuery.dqa_uniqueness_2............. [RUN]
2021-12-21 10:02:08.809404 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:02:08.809823 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-21 10:02:08.815217 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:02:08.815620 (Thread-1): finished collecting timing info
2021-12-21 10:02:08.826692 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:02:08.827052 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:02:08.831564 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`testingBigQuery`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select ate_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
from `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
group by group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned
having count(1) > 1;


2021-12-21 10:02:09.371956 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Syntax error: Unexpected keyword GROUP at [19:10]')
2021-12-21 10:02:10.771487 (Thread-1): finished collecting timing info
2021-12-21 10:02:10.772414 (Thread-1): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Unexpected keyword GROUP at [19:10]

(job ID: 6fe664b0-b96c-43f2-afb1-6604e58a3820)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */
   2:
   3:
   4:  create or replace view `playground-325606`.`testingBigQuery`.`dqa_uniqueness_2`
   5:  OPTIONS()
   6:  as -- Use the `ref` function to select from other models
   7:
   8:select ate_gmt7,
   9:time_gmt7,
  10:hour_gmt7, 
  11:timestamp_gmt7,
  12:event_name,
  13:ga_session_id,
  14:user_id,
  15:event_params_page_title,
  16:event_params_product,
  17:action_type_cleaned, count(1) total
  18:from `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
  19:group by group by date_gmt7,
  20:time_gmt7,
  21:hour_gmt7, 
  22:timestamp_gmt7,
  23:event_name,
  24:ga_session_id,
  25:user_id,
  26:event_params_page_title,
  27:event_params_product,
  28:action_type_cleaned
  29:having count(1) > 1;
  30:
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 22, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:02:10.776810 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c400b53-d995-44e2-ab89-5fc5306a6206', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3ce937580>]}
2021-12-21 10:02:10.777668 (Thread-1): 17:02:10 | 2 of 2 ERROR creating view model testingBigQuery.dqa_uniqueness_2.... [ERROR in 1.97s]
2021-12-21 10:02:10.778119 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:02:10.780441 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:02:10.781217 (MainThread): 17:02:10 | 
2021-12-21 10:02:10.781724 (MainThread): 17:02:10 | Finished running 1 table model, 1 view model in 6.49s.
2021-12-21 10:02:10.782091 (MainThread): Connection 'master' was properly closed.
2021-12-21 10:02:10.782405 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-21 10:02:10.791681 (MainThread): 
2021-12-21 10:02:10.792130 (MainThread): Completed with 1 error and 0 warnings:
2021-12-21 10:02:10.792463 (MainThread): 
2021-12-21 10:02:10.792775 (MainThread): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
2021-12-21 10:02:10.793122 (MainThread):   Syntax error: Unexpected keyword GROUP at [19:10]
2021-12-21 10:02:10.793362 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:02:10.793611 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
2021-12-21 10:02:10.793792 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3ceb41970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3ce9e6f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3cea25850>]}
2021-12-21 10:02:10.793971 (MainThread): Flushing usage events
2021-12-21 10:03:53.277926 (MainThread): Running with dbt=0.21.0
2021-12-21 10:03:53.444042 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 10:03:53.444603 (MainThread): Tracking: tracking
2021-12-21 10:03:53.450302 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e5bcd1af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e59f6b6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e59f6b700>]}
2021-12-21 10:03:53.456388 (MainThread): Partial parsing not enabled
2021-12-21 10:03:53.460523 (MainThread): Parsing macros/catalog.sql
2021-12-21 10:03:53.464730 (MainThread): Parsing macros/etc.sql
2021-12-21 10:03:53.466054 (MainThread): Parsing macros/adapters.sql
2021-12-21 10:03:53.478636 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 10:03:53.480144 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 10:03:53.481980 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 10:03:53.483490 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 10:03:53.484690 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 10:03:53.490013 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 10:03:53.498481 (MainThread): Parsing macros/core.sql
2021-12-21 10:03:53.500943 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 10:03:53.502048 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 10:03:53.503375 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 10:03:53.504155 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 10:03:53.504988 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 10:03:53.509108 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 10:03:53.514632 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 10:03:53.527301 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 10:03:53.531567 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 10:03:53.533878 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 10:03:53.544617 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 10:03:53.545766 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 10:03:53.551565 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 10:03:53.560023 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 10:03:53.564157 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 10:03:53.573410 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 10:03:53.574448 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 10:03:53.595508 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 10:03:53.596484 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 10:03:53.597476 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 10:03:53.598378 (MainThread): Parsing macros/etc/query.sql
2021-12-21 10:03:53.598997 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 10:03:53.600432 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 10:03:53.601585 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 10:03:53.606846 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 10:03:53.746688 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:03:53.756774 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:03:53.772687 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:03:53.773957 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:03:53.775076 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:03:53.776262 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:03:53.784716 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '13208060-001f-4f6b-9c5a-f9d58f08a65b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e59dea4f0>]}
2021-12-21 10:03:53.787680 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 10:03:53.788026 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '13208060-001f-4f6b-9c5a-f9d58f08a65b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e59e5f940>]}
2021-12-21 10:03:53.788307 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 10:03:53.789326 (MainThread): 
2021-12-21 10:03:53.789619 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:03:53.790274 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 10:03:53.790474 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 10:03:54.641167 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_testingBigQuery".
2021-12-21 10:03:54.641904 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 10:03:54.648780 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:03:55.285074 (MainThread): 17:03:55 | Concurrency: 1 threads (target='prod')
2021-12-21 10:03:55.285665 (MainThread): 17:03:55 | 
2021-12-21 10:03:55.290323 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:03:55.291088 (Thread-1): 17:03:55 | 1 of 2 START table model testingBigQuery.dqa_uniqueness_1............ [RUN]
2021-12-21 10:03:55.291919 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:03:55.292452 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-21 10:03:55.297854 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:03:55.298663 (Thread-1): finished collecting timing info
2021-12-21 10:03:55.315791 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:03:55.319199 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:03:55.933077 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49512), raddr=('172.217.194.95', 443)>
2021-12-21 10:03:55.933479 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 50054), raddr=('34.101.5.42', 443)>
2021-12-21 10:03:55.933720 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49516), raddr=('172.217.194.95', 443)>
2021-12-21 10:03:55.933927 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 50058), raddr=('34.101.5.42', 443)>
2021-12-21 10:03:55.943930 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:03:55.944221 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 10:04:00.689457 (Thread-1): finished collecting timing info
2021-12-21 10:04:00.690385 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '13208060-001f-4f6b-9c5a-f9d58f08a65b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e59dd4850>]}
2021-12-21 10:04:00.691183 (Thread-1): 17:04:00 | 1 of 2 OK created table model testingBigQuery.dqa_uniqueness_1....... [CREATE TABLE (7.8k rows, 1018.8 KB processed) in 5.40s]
2021-12-21 10:04:00.691618 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:04:00.693121 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:04:00.693653 (Thread-1): 17:04:00 | 2 of 2 START view model testingBigQuery.dqa_uniqueness_2............. [RUN]
2021-12-21 10:04:00.694299 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:04:00.694614 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-21 10:04:00.697872 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:04:00.698313 (Thread-1): finished collecting timing info
2021-12-21 10:04:00.710449 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:04:00.710749 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:04:00.714021 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`testingBigQuery`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select ate_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
from `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
group by group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned
having count(1) > 1;


2021-12-21 10:04:01.516203 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Syntax error: Unexpected keyword GROUP at [19:10]')
2021-12-21 10:04:02.904897 (Thread-1): finished collecting timing info
2021-12-21 10:04:02.905972 (Thread-1): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Unexpected keyword GROUP at [19:10]

(job ID: 22233b27-08ab-49b0-a6ab-d60ee4981b74)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */
   2:
   3:
   4:  create or replace view `playground-325606`.`testingBigQuery`.`dqa_uniqueness_2`
   5:  OPTIONS()
   6:  as -- Use the `ref` function to select from other models
   7:
   8:select ate_gmt7,
   9:time_gmt7,
  10:hour_gmt7, 
  11:timestamp_gmt7,
  12:event_name,
  13:ga_session_id,
  14:user_id,
  15:event_params_page_title,
  16:event_params_product,
  17:action_type_cleaned, count(1) total
  18:from `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
  19:group by group by date_gmt7,
  20:time_gmt7,
  21:hour_gmt7, 
  22:timestamp_gmt7,
  23:event_name,
  24:ga_session_id,
  25:user_id,
  26:event_params_page_title,
  27:event_params_product,
  28:action_type_cleaned
  29:having count(1) > 1;
  30:
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 22, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:04:02.908296 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '13208060-001f-4f6b-9c5a-f9d58f08a65b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e59d83520>]}
2021-12-21 10:04:02.908552 (Thread-1): 17:04:02 | 2 of 2 ERROR creating view model testingBigQuery.dqa_uniqueness_2.... [ERROR in 2.21s]
2021-12-21 10:04:02.908690 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:04:02.909465 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:04:02.909676 (MainThread): 17:04:02 | 
2021-12-21 10:04:02.909793 (MainThread): 17:04:02 | Finished running 1 table model, 1 view model in 9.12s.
2021-12-21 10:04:02.909959 (MainThread): Connection 'master' was properly closed.
2021-12-21 10:04:02.910074 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-21 10:04:02.919068 (MainThread): 
2021-12-21 10:04:02.919351 (MainThread): Completed with 1 error and 0 warnings:
2021-12-21 10:04:02.919735 (MainThread): 
2021-12-21 10:04:02.920018 (MainThread): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
2021-12-21 10:04:02.920278 (MainThread):   Syntax error: Unexpected keyword GROUP at [19:10]
2021-12-21 10:04:02.920542 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:04:02.920707 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
2021-12-21 10:04:02.920928 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e59f89a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e59e716d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e59f6b8b0>]}
2021-12-21 10:04:02.921165 (MainThread): Flushing usage events
2021-12-21 10:05:10.864713 (MainThread): Running with dbt=0.21.0
2021-12-21 10:05:11.044907 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 10:05:11.045623 (MainThread): Tracking: tracking
2021-12-21 10:05:11.052517 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f52462f5910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f524458d700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f524458d760>]}
2021-12-21 10:05:11.060379 (MainThread): Partial parsing not enabled
2021-12-21 10:05:11.065609 (MainThread): Parsing macros/catalog.sql
2021-12-21 10:05:11.070526 (MainThread): Parsing macros/etc.sql
2021-12-21 10:05:11.072129 (MainThread): Parsing macros/adapters.sql
2021-12-21 10:05:11.088432 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 10:05:11.090517 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 10:05:11.092654 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 10:05:11.094802 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 10:05:11.096158 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 10:05:11.103488 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 10:05:11.114612 (MainThread): Parsing macros/core.sql
2021-12-21 10:05:11.117487 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 10:05:11.118815 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 10:05:11.120569 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 10:05:11.121600 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 10:05:11.122697 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 10:05:11.128084 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 10:05:11.135351 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 10:05:11.152439 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 10:05:11.158020 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 10:05:11.161012 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 10:05:11.175437 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 10:05:11.176879 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 10:05:11.184481 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 10:05:11.194631 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 10:05:11.200215 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 10:05:11.213205 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 10:05:11.214613 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 10:05:11.236641 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 10:05:11.237926 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 10:05:11.239177 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 10:05:11.240390 (MainThread): Parsing macros/etc/query.sql
2021-12-21 10:05:11.241232 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 10:05:11.243149 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 10:05:11.244599 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 10:05:11.250936 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 10:05:11.440833 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:05:11.450362 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:05:11.471994 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:05:11.473490 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:05:11.474866 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:05:11.476364 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:05:11.514124 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fc8447f5-639a-4bdb-95d7-80060a4aac9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f52443f88b0>]}
2021-12-21 10:05:11.518230 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 10:05:11.518613 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fc8447f5-639a-4bdb-95d7-80060a4aac9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5244482670>]}
2021-12-21 10:05:11.518927 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 10:05:11.519965 (MainThread): 
2021-12-21 10:05:11.520248 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:05:11.521003 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 10:05:11.521210 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 10:05:12.098638 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_testingBigQuery".
2021-12-21 10:05:12.099401 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 10:05:12.110576 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:05:12.721685 (MainThread): 17:05:12 | Concurrency: 1 threads (target='prod')
2021-12-21 10:05:12.722247 (MainThread): 17:05:12 | 
2021-12-21 10:05:12.726464 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:05:12.727273 (Thread-1): 17:05:12 | 1 of 2 START table model testingBigQuery.dqa_uniqueness_1............ [RUN]
2021-12-21 10:05:12.728131 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:05:12.728583 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-21 10:05:12.733931 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:05:12.734736 (Thread-1): finished collecting timing info
2021-12-21 10:05:12.753113 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:05:12.756617 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:05:13.121219 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49540), raddr=('172.217.194.95', 443)>
2021-12-21 10:05:13.121631 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 50082), raddr=('34.101.5.42', 443)>
2021-12-21 10:05:13.121894 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49544), raddr=('172.217.194.95', 443)>
2021-12-21 10:05:13.122129 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 50086), raddr=('34.101.5.42', 443)>
2021-12-21 10:05:13.132603 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:05:13.132856 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 10:05:16.485448 (Thread-1): finished collecting timing info
2021-12-21 10:05:16.486426 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc8447f5-639a-4bdb-95d7-80060a4aac9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5244574970>]}
2021-12-21 10:05:16.487210 (Thread-1): 17:05:16 | 1 of 2 OK created table model testingBigQuery.dqa_uniqueness_1....... [CREATE TABLE (7.8k rows, 1018.8 KB processed) in 3.76s]
2021-12-21 10:05:16.487614 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:05:16.488836 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:05:16.489475 (Thread-1): 17:05:16 | 2 of 2 START view model testingBigQuery.dqa_uniqueness_2............. [RUN]
2021-12-21 10:05:16.490147 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:05:16.490548 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-21 10:05:16.495112 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:05:16.495993 (Thread-1): finished collecting timing info
2021-12-21 10:05:16.513978 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:05:16.514306 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:05:16.517637 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`testingBigQuery`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select ate_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
from `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
group by group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned
having count(1) > 1;


2021-12-21 10:05:17.067133 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Syntax error: Unexpected keyword GROUP at [19:10]')
2021-12-21 10:05:17.813445 (Thread-1): finished collecting timing info
2021-12-21 10:05:17.814451 (Thread-1): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Unexpected keyword GROUP at [19:10]

(job ID: 4449817d-b39b-4c09-b6b4-7e6b31dda6dc)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */
   2:
   3:
   4:  create or replace view `playground-325606`.`testingBigQuery`.`dqa_uniqueness_2`
   5:  OPTIONS()
   6:  as -- Use the `ref` function to select from other models
   7:
   8:select ate_gmt7,
   9:time_gmt7,
  10:hour_gmt7, 
  11:timestamp_gmt7,
  12:event_name,
  13:ga_session_id,
  14:user_id,
  15:event_params_page_title,
  16:event_params_product,
  17:action_type_cleaned, count(1) total
  18:from `playground-325606`.`testingBigQuery`.`dqa_uniqueness_1`
  19:group by group by date_gmt7,
  20:time_gmt7,
  21:hour_gmt7, 
  22:timestamp_gmt7,
  23:event_name,
  24:ga_session_id,
  25:user_id,
  26:event_params_page_title,
  27:event_params_product,
  28:action_type_cleaned
  29:having count(1) > 1;
  30:
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 22, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:05:17.817087 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc8447f5-639a-4bdb-95d7-80060a4aac9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f52443b05b0>]}
2021-12-21 10:05:17.817371 (Thread-1): 17:05:17 | 2 of 2 ERROR creating view model testingBigQuery.dqa_uniqueness_2.... [ERROR in 1.33s]
2021-12-21 10:05:17.817503 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:05:17.818287 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:05:17.818514 (MainThread): 17:05:17 | 
2021-12-21 10:05:17.818639 (MainThread): 17:05:17 | Finished running 1 table model, 1 view model in 6.30s.
2021-12-21 10:05:17.818740 (MainThread): Connection 'master' was properly closed.
2021-12-21 10:05:17.818862 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-21 10:05:17.827547 (MainThread): 
2021-12-21 10:05:17.827971 (MainThread): Completed with 1 error and 0 warnings:
2021-12-21 10:05:17.828233 (MainThread): 
2021-12-21 10:05:17.828588 (MainThread): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
2021-12-21 10:05:17.828944 (MainThread):   Syntax error: Unexpected keyword GROUP at [19:10]
2021-12-21 10:05:17.829233 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:05:17.829567 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
2021-12-21 10:05:17.829995 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5244505f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5244459130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5247c8bb50>]}
2021-12-21 10:05:17.830426 (MainThread): Flushing usage events
2021-12-21 10:07:06.526377 (MainThread): Running with dbt=0.21.0
2021-12-21 10:07:06.678992 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 10:07:06.679553 (MainThread): Tracking: tracking
2021-12-21 10:07:06.685347 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8befe02a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bee09a6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bee09a730>]}
2021-12-21 10:07:06.691765 (MainThread): Partial parsing not enabled
2021-12-21 10:07:06.695729 (MainThread): Parsing macros/catalog.sql
2021-12-21 10:07:06.699837 (MainThread): Parsing macros/etc.sql
2021-12-21 10:07:06.701052 (MainThread): Parsing macros/adapters.sql
2021-12-21 10:07:06.714109 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 10:07:06.715723 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 10:07:06.717408 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 10:07:06.718953 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 10:07:06.719982 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 10:07:06.725548 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 10:07:06.734936 (MainThread): Parsing macros/core.sql
2021-12-21 10:07:06.737150 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 10:07:06.738161 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 10:07:06.739567 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 10:07:06.740322 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 10:07:06.741162 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 10:07:06.745485 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 10:07:06.751106 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 10:07:06.764365 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 10:07:06.768678 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 10:07:06.771077 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 10:07:06.782736 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 10:07:06.783857 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 10:07:06.790234 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 10:07:06.798090 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 10:07:06.802387 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 10:07:06.812192 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 10:07:06.813256 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 10:07:06.831328 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 10:07:06.832315 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 10:07:06.833311 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 10:07:06.834220 (MainThread): Parsing macros/etc/query.sql
2021-12-21 10:07:06.834811 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 10:07:06.836204 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 10:07:06.837435 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 10:07:06.842704 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 10:07:06.990446 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:07:06.998061 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:07:07.014208 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:07:07.015451 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:07:07.016588 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:07:07.017747 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:07:07.026847 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '846c5124-a806-46d4-b052-248cdc038d7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bedf050d0>]}
2021-12-21 10:07:07.029669 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 10:07:07.029945 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '846c5124-a806-46d4-b052-248cdc038d7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bedf05460>]}
2021-12-21 10:07:07.030130 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 10:07:07.031079 (MainThread): 
2021-12-21 10:07:07.031311 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:07:07.031993 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 10:07:07.032211 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 10:07:07.969865 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-21 10:07:07.970555 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 10:07:07.976652 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:07:08.894533 (MainThread): 17:07:08 | Concurrency: 1 threads (target='prod')
2021-12-21 10:07:08.895119 (MainThread): 17:07:08 | 
2021-12-21 10:07:08.898636 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:07:08.898910 (Thread-1): 17:07:08 | 1 of 2 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-21 10:07:08.899201 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:07:08.899380 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-21 10:07:08.901189 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:07:08.901769 (Thread-1): finished collecting timing info
2021-12-21 10:07:08.928740 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49566), raddr=('172.217.194.95', 443)>
2021-12-21 10:07:08.928962 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 50108), raddr=('34.101.5.42', 443)>
2021-12-21 10:07:08.929143 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49570), raddr=('172.217.194.95', 443)>
2021-12-21 10:07:08.929331 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 50112), raddr=('34.101.5.42', 443)>
2021-12-21 10:07:08.932560 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:07:08.932913 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:07:08.936429 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 10:07:12.259833 (Thread-1): finished collecting timing info
2021-12-21 10:07:12.260167 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '846c5124-a806-46d4-b052-248cdc038d7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bedea3e20>]}
2021-12-21 10:07:12.260490 (Thread-1): 17:07:12 | 1 of 2 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (7.8k rows, 1018.8 KB processed) in 3.36s]
2021-12-21 10:07:12.260642 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:07:12.261177 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:07:12.261395 (Thread-1): 17:07:12 | 2 of 2 START view model dataset_dbt.dqa_uniqueness_2................. [RUN]
2021-12-21 10:07:12.261705 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:07:12.261847 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-21 10:07:12.263707 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:07:12.264025 (Thread-1): finished collecting timing info
2021-12-21 10:07:12.275067 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:07:12.275404 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:07:12.278768 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select ate_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
group by group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned
having count(1) > 1;


2021-12-21 10:07:12.789128 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Syntax error: Unexpected keyword GROUP at [19:10]')
2021-12-21 10:07:13.414732 (Thread-1): finished collecting timing info
2021-12-21 10:07:13.415652 (Thread-1): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Unexpected keyword GROUP at [19:10]

(job ID: 76486e1e-8cdd-4108-b1c2-a353844e84b6)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */
   2:
   3:
   4:  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
   5:  OPTIONS()
   6:  as -- Use the `ref` function to select from other models
   7:
   8:select ate_gmt7,
   9:time_gmt7,
  10:hour_gmt7, 
  11:timestamp_gmt7,
  12:event_name,
  13:ga_session_id,
  14:user_id,
  15:event_params_page_title,
  16:event_params_product,
  17:action_type_cleaned, count(1) total
  18:from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  19:group by group by date_gmt7,
  20:time_gmt7,
  21:hour_gmt7, 
  22:timestamp_gmt7,
  23:event_name,
  24:ga_session_id,
  25:user_id,
  26:event_params_page_title,
  27:event_params_product,
  28:action_type_cleaned
  29:having count(1) > 1;
  30:
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 22, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Unexpected keyword GROUP at [19:10]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:07:13.420326 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '846c5124-a806-46d4-b052-248cdc038d7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bec6046d0>]}
2021-12-21 10:07:13.421127 (Thread-1): 17:07:13 | 2 of 2 ERROR creating view model dataset_dbt.dqa_uniqueness_2........ [ERROR in 1.16s]
2021-12-21 10:07:13.421587 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:07:13.423843 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:07:13.424622 (MainThread): 17:07:13 | 
2021-12-21 10:07:13.425051 (MainThread): 17:07:13 | Finished running 1 table model, 1 view model in 6.39s.
2021-12-21 10:07:13.425463 (MainThread): Connection 'master' was properly closed.
2021-12-21 10:07:13.425780 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-21 10:07:13.432991 (MainThread): 
2021-12-21 10:07:13.433254 (MainThread): Completed with 1 error and 0 warnings:
2021-12-21 10:07:13.433539 (MainThread): 
2021-12-21 10:07:13.433733 (MainThread): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
2021-12-21 10:07:13.433899 (MainThread):   Syntax error: Unexpected keyword GROUP at [19:10]
2021-12-21 10:07:13.434077 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:07:13.434331 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
2021-12-21 10:07:13.434682 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bee09a6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bedf7a8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bedfae6d0>]}
2021-12-21 10:07:13.435039 (MainThread): Flushing usage events
2021-12-21 10:08:24.747798 (MainThread): Running with dbt=0.21.0
2021-12-21 10:08:24.895357 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 10:08:24.895827 (MainThread): Tracking: tracking
2021-12-21 10:08:24.901865 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45c303ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45c12d1790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45c12d17c0>]}
2021-12-21 10:08:24.908166 (MainThread): Partial parsing not enabled
2021-12-21 10:08:24.912115 (MainThread): Parsing macros/catalog.sql
2021-12-21 10:08:24.916540 (MainThread): Parsing macros/etc.sql
2021-12-21 10:08:24.917879 (MainThread): Parsing macros/adapters.sql
2021-12-21 10:08:24.930630 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 10:08:24.932611 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 10:08:24.934204 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 10:08:24.935658 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 10:08:24.936673 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 10:08:24.942086 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 10:08:24.950580 (MainThread): Parsing macros/core.sql
2021-12-21 10:08:24.952745 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 10:08:24.953774 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 10:08:24.955056 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 10:08:24.955811 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 10:08:24.956713 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 10:08:24.960777 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 10:08:24.966211 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 10:08:24.978613 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 10:08:24.982559 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 10:08:24.984785 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 10:08:24.995676 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 10:08:24.996731 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 10:08:25.002519 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 10:08:25.010351 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 10:08:25.014522 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 10:08:25.023853 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 10:08:25.024894 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 10:08:25.042581 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 10:08:25.043492 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 10:08:25.044397 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 10:08:25.045329 (MainThread): Parsing macros/etc/query.sql
2021-12-21 10:08:25.045936 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 10:08:25.047259 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 10:08:25.048249 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 10:08:25.053099 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 10:08:25.192872 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:08:25.200082 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:08:25.214870 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:08:25.215894 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:08:25.216894 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:08:25.217983 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:08:25.226397 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5a00a3bc-ecdd-4b3f-876d-dcf573a03757', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45c11c8ac0>]}
2021-12-21 10:08:25.229160 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 10:08:25.229405 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5a00a3bc-ecdd-4b3f-876d-dcf573a03757', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45c118d3d0>]}
2021-12-21 10:08:25.229599 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 10:08:25.230497 (MainThread): 
2021-12-21 10:08:25.230723 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:08:25.231310 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 10:08:25.231554 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 10:08:25.741900 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-21 10:08:25.742641 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 10:08:25.754547 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:08:26.100486 (MainThread): 17:08:26 | Concurrency: 1 threads (target='prod')
2021-12-21 10:08:26.101052 (MainThread): 17:08:26 | 
2021-12-21 10:08:26.105397 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:08:26.106140 (Thread-1): 17:08:26 | 1 of 2 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-21 10:08:26.106862 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:08:26.107475 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-21 10:08:26.112507 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:08:26.113196 (Thread-1): finished collecting timing info
2021-12-21 10:08:26.132024 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:08:26.135588 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:08:26.494154 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49598), raddr=('172.217.194.95', 443)>
2021-12-21 10:08:26.494602 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 50140), raddr=('34.101.5.42', 443)>
2021-12-21 10:08:26.494841 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49602), raddr=('172.217.194.95', 443)>
2021-12-21 10:08:26.495048 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 50144), raddr=('34.101.5.42', 443)>
2021-12-21 10:08:26.505507 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:08:26.505758 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 10:08:30.014565 (Thread-1): finished collecting timing info
2021-12-21 10:08:30.015442 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5a00a3bc-ecdd-4b3f-876d-dcf573a03757', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45c10df100>]}
2021-12-21 10:08:30.016165 (Thread-1): 17:08:30 | 1 of 2 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (7.8k rows, 1018.8 KB processed) in 3.91s]
2021-12-21 10:08:30.016545 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:08:30.017741 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:08:30.018332 (Thread-1): 17:08:30 | 2 of 2 START view model dataset_dbt.dqa_uniqueness_2................. [RUN]
2021-12-21 10:08:30.018778 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:08:30.019135 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-21 10:08:30.022006 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:08:30.022374 (Thread-1): finished collecting timing info
2021-12-21 10:08:30.032929 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:08:30.033243 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:08:30.036442 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select ate_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned
having count(1) > 1;


2021-12-21 10:08:32.038062 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]')
2021-12-21 10:08:35.230780 (Thread-1): finished collecting timing info
2021-12-21 10:08:35.231805 (Thread-1): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]

(job ID: 6e123bc9-669a-43f5-a341-365c2bb34dc4)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */
   2:
   3:
   4:  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
   5:  OPTIONS()
   6:  as -- Use the `ref` function to select from other models
   7:
   8:select ate_gmt7,
   9:time_gmt7,
  10:hour_gmt7, 
  11:timestamp_gmt7,
  12:event_name,
  13:ga_session_id,
  14:user_id,
  15:event_params_page_title,
  16:event_params_product,
  17:action_type_cleaned, count(1) total
  18:from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  19:group by date_gmt7,
  20:time_gmt7,
  21:hour_gmt7, 
  22:timestamp_gmt7,
  23:event_name,
  24:ga_session_id,
  25:user_id,
  26:event_params_page_title,
  27:event_params_product,
  28:action_type_cleaned
  29:having count(1) > 1;
  30:
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 22, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:08:35.236403 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5a00a3bc-ecdd-4b3f-876d-dcf573a03757', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45bafcb6d0>]}
2021-12-21 10:08:35.237219 (Thread-1): 17:08:35 | 2 of 2 ERROR creating view model dataset_dbt.dqa_uniqueness_2........ [ERROR in 5.22s]
2021-12-21 10:08:35.237719 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:08:35.239933 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:08:35.240729 (MainThread): 17:08:35 | 
2021-12-21 10:08:35.241173 (MainThread): 17:08:35 | Finished running 1 table model, 1 view model in 10.01s.
2021-12-21 10:08:35.241804 (MainThread): Connection 'master' was properly closed.
2021-12-21 10:08:35.242342 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-21 10:08:35.252047 (MainThread): 
2021-12-21 10:08:35.252534 (MainThread): Completed with 1 error and 0 warnings:
2021-12-21 10:08:35.252943 (MainThread): 
2021-12-21 10:08:35.253347 (MainThread): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
2021-12-21 10:08:35.253726 (MainThread):   Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]
2021-12-21 10:08:35.254091 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:08:35.254472 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
2021-12-21 10:08:35.254961 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45c12f6b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45c12c6070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45c1219e20>]}
2021-12-21 10:08:35.255483 (MainThread): Flushing usage events
2021-12-21 10:08:58.639162 (MainThread): Running with dbt=0.21.0
2021-12-21 10:08:58.792266 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 10:08:58.792825 (MainThread): Tracking: tracking
2021-12-21 10:08:58.801034 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff395961a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff393b796a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff393b79700>]}
2021-12-21 10:08:58.807446 (MainThread): Partial parsing not enabled
2021-12-21 10:08:58.811409 (MainThread): Parsing macros/catalog.sql
2021-12-21 10:08:58.815442 (MainThread): Parsing macros/etc.sql
2021-12-21 10:08:58.816653 (MainThread): Parsing macros/adapters.sql
2021-12-21 10:08:58.829935 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 10:08:58.831554 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 10:08:58.833349 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 10:08:58.834996 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 10:08:58.836024 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 10:08:58.841564 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 10:08:58.850234 (MainThread): Parsing macros/core.sql
2021-12-21 10:08:58.852475 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 10:08:58.853532 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 10:08:58.854964 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 10:08:58.855701 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 10:08:58.856556 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 10:08:58.860749 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 10:08:58.866338 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 10:08:58.879611 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 10:08:58.883861 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 10:08:58.886188 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 10:08:58.897692 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 10:08:58.898748 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 10:08:58.904714 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 10:08:58.912772 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 10:08:58.917484 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 10:08:58.927369 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 10:08:58.928378 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 10:08:58.946718 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 10:08:58.947711 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 10:08:58.948714 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 10:08:58.949634 (MainThread): Parsing macros/etc/query.sql
2021-12-21 10:08:58.950232 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 10:08:58.951631 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 10:08:58.952738 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 10:08:58.957804 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 10:08:59.104114 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:08:59.111839 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:08:59.127368 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:08:59.128478 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:08:59.129552 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:08:59.130708 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:08:59.139132 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6bdfc691-c93b-4aa9-977b-82234c02e1c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff393aee9a0>]}
2021-12-21 10:08:59.141917 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 10:08:59.142205 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6bdfc691-c93b-4aa9-977b-82234c02e1c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff393a64130>]}
2021-12-21 10:08:59.142397 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 10:08:59.143304 (MainThread): 
2021-12-21 10:08:59.143535 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:08:59.144227 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 10:08:59.144446 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 10:08:59.720759 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-21 10:08:59.721490 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 10:08:59.733920 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:09:00.004939 (MainThread): 17:08:59 | Concurrency: 1 threads (target='prod')
2021-12-21 10:09:00.005475 (MainThread): 17:09:00 | 
2021-12-21 10:09:00.009593 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:09:00.010413 (Thread-1): 17:09:00 | 1 of 2 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-21 10:09:00.011292 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:09:00.011909 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-21 10:09:00.017670 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:09:00.018378 (Thread-1): finished collecting timing info
2021-12-21 10:09:00.035706 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:09:00.039090 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:09:00.382402 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49620), raddr=('172.217.194.95', 443)>
2021-12-21 10:09:00.382885 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37582), raddr=('34.101.5.74', 443)>
2021-12-21 10:09:00.383197 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49624), raddr=('172.217.194.95', 443)>
2021-12-21 10:09:00.383472 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37586), raddr=('34.101.5.74', 443)>
2021-12-21 10:09:00.394097 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:09:00.394360 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 10:09:02.337668 (Thread-1): finished collecting timing info
2021-12-21 10:09:02.338012 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bdfc691-c93b-4aa9-977b-82234c02e1c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff393c0c250>]}
2021-12-21 10:09:02.338289 (Thread-1): 17:09:02 | 1 of 2 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (7.8k rows, 1018.8 KB processed) in 2.33s]
2021-12-21 10:09:02.338422 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:09:02.338934 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:09:02.339122 (Thread-1): 17:09:02 | 2 of 2 START view model dataset_dbt.dqa_uniqueness_2................. [RUN]
2021-12-21 10:09:02.339320 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:09:02.339444 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-21 10:09:02.340966 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:09:02.341204 (Thread-1): finished collecting timing info
2021-12-21 10:09:02.351501 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:09:02.351814 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:09:02.355001 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select ate_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned
having count(1) > 1;


2021-12-21 10:09:03.368777 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]')
2021-12-21 10:09:04.190580 (Thread-1): finished collecting timing info
2021-12-21 10:09:04.191494 (Thread-1): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]

(job ID: bb2bd56e-beea-4b6b-8824-63527805c273)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */
   2:
   3:
   4:  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
   5:  OPTIONS()
   6:  as -- Use the `ref` function to select from other models
   7:
   8:select ate_gmt7,
   9:time_gmt7,
  10:hour_gmt7, 
  11:timestamp_gmt7,
  12:event_name,
  13:ga_session_id,
  14:user_id,
  15:event_params_page_title,
  16:event_params_product,
  17:action_type_cleaned, count(1) total
  18:from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  19:group by date_gmt7,
  20:time_gmt7,
  21:hour_gmt7, 
  22:timestamp_gmt7,
  23:event_name,
  24:ga_session_id,
  25:user_id,
  26:event_params_page_title,
  27:event_params_product,
  28:action_type_cleaned
  29:having count(1) > 1;
  30:
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 22, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:09:04.195764 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bdfc691-c93b-4aa9-977b-82234c02e1c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff391914490>]}
2021-12-21 10:09:04.196524 (Thread-1): 17:09:04 | 2 of 2 ERROR creating view model dataset_dbt.dqa_uniqueness_2........ [ERROR in 1.86s]
2021-12-21 10:09:04.196932 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:09:04.199123 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:09:04.200127 (MainThread): 17:09:04 | 
2021-12-21 10:09:04.200679 (MainThread): 17:09:04 | Finished running 1 table model, 1 view model in 5.06s.
2021-12-21 10:09:04.201295 (MainThread): Connection 'master' was properly closed.
2021-12-21 10:09:04.201822 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-21 10:09:04.205736 (MainThread): 
2021-12-21 10:09:04.205928 (MainThread): Completed with 1 error and 0 warnings:
2021-12-21 10:09:04.206032 (MainThread): 
2021-12-21 10:09:04.206161 (MainThread): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
2021-12-21 10:09:04.206258 (MainThread):   Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]
2021-12-21 10:09:04.206393 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:09:04.206562 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
2021-12-21 10:09:04.206789 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff393b796a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff393a641f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff393b5c880>]}
2021-12-21 10:09:04.206962 (MainThread): Flushing usage events
2021-12-21 10:09:20.197116 (MainThread): Running with dbt=0.21.0
2021-12-21 10:09:20.349059 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 10:09:20.349662 (MainThread): Tracking: tracking
2021-12-21 10:09:20.355143 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f4105b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f2357700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f2357760>]}
2021-12-21 10:09:20.361393 (MainThread): Partial parsing not enabled
2021-12-21 10:09:20.365403 (MainThread): Parsing macros/catalog.sql
2021-12-21 10:09:20.369487 (MainThread): Parsing macros/etc.sql
2021-12-21 10:09:20.370713 (MainThread): Parsing macros/adapters.sql
2021-12-21 10:09:20.384088 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 10:09:20.385718 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 10:09:20.387371 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 10:09:20.388905 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 10:09:20.389938 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 10:09:20.395585 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 10:09:20.404234 (MainThread): Parsing macros/core.sql
2021-12-21 10:09:20.406442 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 10:09:20.407429 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 10:09:20.408765 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 10:09:20.409518 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 10:09:20.410322 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 10:09:20.414756 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 10:09:20.420482 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 10:09:20.434134 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 10:09:20.438346 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 10:09:20.440653 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 10:09:20.452458 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 10:09:20.453637 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 10:09:20.459958 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 10:09:20.468161 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 10:09:20.472575 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 10:09:20.482577 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 10:09:20.483577 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 10:09:20.501780 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 10:09:20.502815 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 10:09:20.503785 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 10:09:20.504754 (MainThread): Parsing macros/etc/query.sql
2021-12-21 10:09:20.505372 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 10:09:20.506774 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 10:09:20.507809 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 10:09:20.512971 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 10:09:20.659536 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:09:20.667340 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:09:20.683211 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:09:20.684342 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:09:20.685468 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:09:20.686684 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:09:20.695063 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0bc95c76-f001-476b-a280-7da1666cf2a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f2207bb0>]}
2021-12-21 10:09:20.698020 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 10:09:20.698299 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0bc95c76-f001-476b-a280-7da1666cf2a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f2292a30>]}
2021-12-21 10:09:20.698480 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 10:09:20.699343 (MainThread): 
2021-12-21 10:09:20.699569 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:09:20.700244 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 10:09:20.700466 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 10:09:21.247814 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-21 10:09:21.248457 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 10:09:21.260362 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:09:21.650757 (MainThread): 17:09:21 | Concurrency: 1 threads (target='prod')
2021-12-21 10:09:21.651298 (MainThread): 17:09:21 | 
2021-12-21 10:09:21.655614 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:09:21.656412 (Thread-1): 17:09:21 | 1 of 2 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-21 10:09:21.657315 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:09:21.657970 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-21 10:09:21.664018 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:09:21.664751 (Thread-1): finished collecting timing info
2021-12-21 10:09:21.683002 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:09:21.686421 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:09:22.050224 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49642), raddr=('172.217.194.95', 443)>
2021-12-21 10:09:22.050523 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37604), raddr=('34.101.5.74', 443)>
2021-12-21 10:09:22.050710 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49646), raddr=('172.217.194.95', 443)>
2021-12-21 10:09:22.050862 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37608), raddr=('34.101.5.74', 443)>
2021-12-21 10:09:22.059689 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:09:22.059990 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 10:09:24.174556 (Thread-1): finished collecting timing info
2021-12-21 10:09:24.175468 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bc95c76-f001-476b-a280-7da1666cf2a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f00ed130>]}
2021-12-21 10:09:24.176216 (Thread-1): 17:09:24 | 1 of 2 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (7.8k rows, 1018.8 KB processed) in 2.52s]
2021-12-21 10:09:24.176671 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:09:24.178032 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:09:24.178610 (Thread-1): 17:09:24 | 2 of 2 START view model dataset_dbt.dqa_uniqueness_2................. [RUN]
2021-12-21 10:09:24.179185 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:09:24.179540 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-21 10:09:24.183681 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:09:24.184166 (Thread-1): finished collecting timing info
2021-12-21 10:09:24.195249 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:09:24.195630 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:09:24.198851 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select ate_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned
having count(1) > 1;


2021-12-21 10:09:26.358878 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]')
2021-12-21 10:09:27.265297 (Thread-1): finished collecting timing info
2021-12-21 10:09:27.266285 (Thread-1): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]

(job ID: 5bbfa6cd-9c83-4e12-aaf7-4eba21e805c0)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */
   2:
   3:
   4:  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
   5:  OPTIONS()
   6:  as -- Use the `ref` function to select from other models
   7:
   8:select ate_gmt7,
   9:time_gmt7,
  10:hour_gmt7, 
  11:timestamp_gmt7,
  12:event_name,
  13:ga_session_id,
  14:user_id,
  15:event_params_page_title,
  16:event_params_product,
  17:action_type_cleaned, count(1) total
  18:from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  19:group by date_gmt7,
  20:time_gmt7,
  21:hour_gmt7, 
  22:timestamp_gmt7,
  23:event_name,
  24:ga_session_id,
  25:user_id,
  26:event_params_page_title,
  27:event_params_product,
  28:action_type_cleaned
  29:having count(1) > 1;
  30:
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 22, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:09:27.271156 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bc95c76-f001-476b-a280-7da1666cf2a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f00b8fd0>]}
2021-12-21 10:09:27.271955 (Thread-1): 17:09:27 | 2 of 2 ERROR creating view model dataset_dbt.dqa_uniqueness_2........ [ERROR in 3.09s]
2021-12-21 10:09:27.272382 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:09:27.274653 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:09:27.275503 (MainThread): 17:09:27 | 
2021-12-21 10:09:27.275968 (MainThread): 17:09:27 | Finished running 1 table model, 1 view model in 6.58s.
2021-12-21 10:09:27.276408 (MainThread): Connection 'master' was properly closed.
2021-12-21 10:09:27.276892 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-21 10:09:27.282164 (MainThread): 
2021-12-21 10:09:27.282363 (MainThread): Completed with 1 error and 0 warnings:
2021-12-21 10:09:27.282482 (MainThread): 
2021-12-21 10:09:27.282594 (MainThread): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
2021-12-21 10:09:27.282687 (MainThread):   Unrecognized name: ate_gmt7; Did you mean date_gmt7? at [8:8]
2021-12-21 10:09:27.282797 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-21 10:09:27.282902 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
2021-12-21 10:09:27.283074 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f5a9bb80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f23696a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f2357700>]}
2021-12-21 10:09:27.283254 (MainThread): Flushing usage events
2021-12-21 10:09:43.373997 (MainThread): Running with dbt=0.21.0
2021-12-21 10:09:43.529422 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-21 10:09:43.530009 (MainThread): Tracking: tracking
2021-12-21 10:09:43.535654 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6981e31f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6963fb730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6963fb790>]}
2021-12-21 10:09:43.541979 (MainThread): Partial parsing not enabled
2021-12-21 10:09:43.546134 (MainThread): Parsing macros/catalog.sql
2021-12-21 10:09:43.550269 (MainThread): Parsing macros/etc.sql
2021-12-21 10:09:43.551526 (MainThread): Parsing macros/adapters.sql
2021-12-21 10:09:43.565026 (MainThread): Parsing macros/materializations/seed.sql
2021-12-21 10:09:43.566655 (MainThread): Parsing macros/materializations/copy.sql
2021-12-21 10:09:43.568321 (MainThread): Parsing macros/materializations/view.sql
2021-12-21 10:09:43.570175 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-21 10:09:43.571211 (MainThread): Parsing macros/materializations/table.sql
2021-12-21 10:09:43.576914 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-21 10:09:43.585927 (MainThread): Parsing macros/core.sql
2021-12-21 10:09:43.588260 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-21 10:09:43.589343 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-21 10:09:43.590746 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-21 10:09:43.591512 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-21 10:09:43.592365 (MainThread): Parsing macros/materializations/test.sql
2021-12-21 10:09:43.596742 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-21 10:09:43.602721 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-21 10:09:43.616449 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-21 10:09:43.620843 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-21 10:09:43.623192 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-21 10:09:43.634973 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-21 10:09:43.636219 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-21 10:09:43.642700 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-21 10:09:43.652931 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-21 10:09:43.658256 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-21 10:09:43.670844 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-21 10:09:43.672130 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-21 10:09:43.694778 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-21 10:09:43.696057 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-21 10:09:43.697331 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-21 10:09:43.698505 (MainThread): Parsing macros/etc/query.sql
2021-12-21 10:09:43.699318 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-21 10:09:43.701200 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-21 10:09:43.702535 (MainThread): Parsing macros/etc/datetime.sql
2021-12-21 10:09:43.708585 (MainThread): Parsing macros/adapters/common.sql
2021-12-21 10:09:43.898072 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:09:43.907735 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:09:43.929140 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:09:43.930618 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:09:43.932086 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:09:43.933615 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-21 10:09:43.945186 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e03a2644-2daf-4d06-95bf-0d128c7ff82b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6962e6dc0>]}
2021-12-21 10:09:43.948695 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-21 10:09:43.948995 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e03a2644-2daf-4d06-95bf-0d128c7ff82b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6963726d0>]}
2021-12-21 10:09:43.949214 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-21 10:09:43.950213 (MainThread): 
2021-12-21 10:09:43.950508 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:09:43.951234 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-21 10:09:43.951411 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-21 10:09:44.469183 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-21 10:09:44.469895 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-21 10:09:44.482140 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:09:44.831833 (MainThread): 17:09:44 | Concurrency: 1 threads (target='prod')
2021-12-21 10:09:44.832385 (MainThread): 17:09:44 | 
2021-12-21 10:09:44.836898 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:09:44.838002 (Thread-1): 17:09:44 | 1 of 2 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-21 10:09:44.839142 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-21 10:09:44.839783 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-21 10:09:44.845599 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:09:44.846385 (Thread-1): finished collecting timing info
2021-12-21 10:09:44.864309 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:09:44.867684 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-21 10:09:45.257592 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49662), raddr=('172.217.194.95', 443)>
2021-12-21 10:09:45.258006 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37624), raddr=('34.101.5.74', 443)>
2021-12-21 10:09:45.258280 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 49666), raddr=('172.217.194.95', 443)>
2021-12-21 10:09:45.258518 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 37628), raddr=('34.101.5.74', 443)>
2021-12-21 10:09:45.268727 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-21 10:09:45.269033 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select *
from x

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-21 10:09:48.567935 (Thread-1): finished collecting timing info
2021-12-21 10:09:48.568292 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e03a2644-2daf-4d06-95bf-0d128c7ff82b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6941dc760>]}
2021-12-21 10:09:48.568554 (Thread-1): 17:09:48 | 1 of 2 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (7.8k rows, 1018.8 KB processed) in 3.73s]
2021-12-21 10:09:48.568683 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-21 10:09:48.569202 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:09:48.569529 (Thread-1): 17:09:48 | 2 of 2 START view model dataset_dbt.dqa_uniqueness_2................. [RUN]
2021-12-21 10:09:48.569874 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-21 10:09:48.570057 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-21 10:09:48.572493 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:09:48.572875 (Thread-1): finished collecting timing info
2021-12-21 10:09:48.588690 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-21 10:09:48.589200 (Thread-1): Opening a new connection, currently in state closed
2021-12-21 10:09:48.594057 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned
having count(1) > 1;


2021-12-21 10:09:49.504961 (Thread-1): finished collecting timing info
2021-12-21 10:09:49.506020 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e03a2644-2daf-4d06-95bf-0d128c7ff82b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6941dc340>]}
2021-12-21 10:09:49.506901 (Thread-1): 17:09:49 | 2 of 2 OK created view model dataset_dbt.dqa_uniqueness_2............ [OK in 0.94s]
2021-12-21 10:09:49.507387 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-21 10:09:49.509758 (MainThread): Acquiring new bigquery connection "master".
2021-12-21 10:09:49.510801 (MainThread): 17:09:49 | 
2021-12-21 10:09:49.511441 (MainThread): 17:09:49 | Finished running 1 table model, 1 view model in 5.56s.
2021-12-21 10:09:49.512045 (MainThread): Connection 'master' was properly closed.
2021-12-21 10:09:49.512531 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-21 10:09:49.521428 (MainThread): 
2021-12-21 10:09:49.521795 (MainThread): Completed successfully
2021-12-21 10:09:49.522120 (MainThread): 
Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
2021-12-21 10:09:49.522458 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6963f1610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6941ec9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb699b7bbb0>]}
2021-12-21 10:09:49.522802 (MainThread): Flushing usage events
2021-12-23 06:32:50.759417 (MainThread): Running with dbt=0.21.0
2021-12-23 06:32:51.012406 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-23 06:32:51.013209 (MainThread): Tracking: tracking
2021-12-23 06:32:51.020889 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe09ff75a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe09e1c66d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe09e1c6730>]}
2021-12-23 06:32:51.027960 (MainThread): Partial parsing not enabled
2021-12-23 06:32:51.037225 (MainThread): Parsing macros/catalog.sql
2021-12-23 06:32:51.041714 (MainThread): Parsing macros/etc.sql
2021-12-23 06:32:51.043017 (MainThread): Parsing macros/adapters.sql
2021-12-23 06:32:51.056274 (MainThread): Parsing macros/materializations/seed.sql
2021-12-23 06:32:51.057862 (MainThread): Parsing macros/materializations/copy.sql
2021-12-23 06:32:51.059577 (MainThread): Parsing macros/materializations/view.sql
2021-12-23 06:32:51.061094 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-23 06:32:51.062210 (MainThread): Parsing macros/materializations/table.sql
2021-12-23 06:32:51.068255 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-23 06:32:51.077261 (MainThread): Parsing macros/core.sql
2021-12-23 06:32:51.079646 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-23 06:32:51.080709 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-23 06:32:51.082417 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-23 06:32:51.083222 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-23 06:32:51.084143 (MainThread): Parsing macros/materializations/test.sql
2021-12-23 06:32:51.088499 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-23 06:32:51.094242 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-23 06:32:51.107463 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-23 06:32:51.111550 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-23 06:32:51.113983 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-23 06:32:51.126131 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-23 06:32:51.127288 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-23 06:32:51.133560 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-23 06:32:51.141790 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-23 06:32:51.146159 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-23 06:32:51.156655 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-23 06:32:51.157720 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-23 06:32:51.176086 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-23 06:32:51.177246 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-23 06:32:51.178336 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-23 06:32:51.179298 (MainThread): Parsing macros/etc/query.sql
2021-12-23 06:32:51.179949 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-23 06:32:51.181542 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-23 06:32:51.182702 (MainThread): Parsing macros/etc/datetime.sql
2021-12-23 06:32:51.187879 (MainThread): Parsing macros/adapters/common.sql
2021-12-23 06:32:51.336816 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-23 06:32:51.344870 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-23 06:32:51.361124 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-23 06:32:51.362372 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-23 06:32:51.363570 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-23 06:32:51.364821 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-23 06:32:51.374831 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5081ea6e-4d5d-479a-90ac-25e45e9876ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe09e0790d0>]}
2021-12-23 06:32:51.377702 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-23 06:32:51.377962 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5081ea6e-4d5d-479a-90ac-25e45e9876ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe09e079460>]}
2021-12-23 06:32:51.378165 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-23 06:32:51.379213 (MainThread): 
2021-12-23 06:32:51.379448 (MainThread): Acquiring new bigquery connection "master".
2021-12-23 06:32:51.380136 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-23 06:32:51.380397 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-23 06:32:52.077930 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-23 06:32:52.078531 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-23 06:32:52.089388 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-23 06:32:52.458131 (MainThread): 13:32:52 | Concurrency: 1 threads (target='prod')
2021-12-23 06:32:52.458577 (MainThread): 13:32:52 | 
2021-12-23 06:32:52.466170 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-23 06:32:52.466616 (Thread-1): 13:32:52 | 1 of 2 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-23 06:32:52.467041 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-23 06:32:52.467253 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-23 06:32:52.469658 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-23 06:32:52.470009 (Thread-1): finished collecting timing info
2021-12-23 06:32:52.483210 (Thread-1): Opening a new connection, currently in state closed
2021-12-23 06:32:52.486990 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-23 06:32:52.876525 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 35248), raddr=('74.125.200.95', 443)>
2021-12-23 06:32:52.876867 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 39318), raddr=('34.101.5.74', 443)>
2021-12-23 06:32:52.877064 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 35252), raddr=('74.125.200.95', 443)>
2021-12-23 06:32:52.877292 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 39322), raddr=('34.101.5.74', 443)>
2021-12-23 06:32:52.887068 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-23 06:32:52.887536 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-23 06:32:56.212415 (Thread-1): finished collecting timing info
2021-12-23 06:32:56.213366 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5081ea6e-4d5d-479a-90ac-25e45e9876ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe09e19cfa0>]}
2021-12-23 06:32:56.214348 (Thread-1): 13:32:56 | 1 of 2 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 3.75s]
2021-12-23 06:32:56.214802 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-23 06:32:56.216138 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-23 06:32:56.216703 (Thread-1): 13:32:56 | 2 of 2 START view model dataset_dbt.dqa_uniqueness_2................. [RUN]
2021-12-23 06:32:56.217328 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-23 06:32:56.217744 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-23 06:32:56.220908 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-23 06:32:56.221397 (Thread-1): finished collecting timing info
2021-12-23 06:32:56.232551 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-23 06:32:56.232909 (Thread-1): Opening a new connection, currently in state closed
2021-12-23 06:32:56.236744 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned
having count(1) > 1;


2021-12-23 06:32:58.228237 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Unrecognized name: date_gmt7 at [8:8]')
2021-12-23 06:32:59.653725 (Thread-1): finished collecting timing info
2021-12-23 06:32:59.654648 (Thread-1): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Unrecognized name: date_gmt7 at [8:8]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Unrecognized name: date_gmt7 at [8:8]

(job ID: 063e8fa6-379a-4492-884c-1dd9fefc1dbe)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */
   2:
   3:
   4:  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
   5:  OPTIONS()
   6:  as -- Use the `ref` function to select from other models
   7:
   8:select date_gmt7,
   9:time_gmt7,
  10:hour_gmt7, 
  11:timestamp_gmt7,
  12:event_name,
  13:ga_session_id,
  14:user_id,
  15:event_params_page_title,
  16:event_params_product,
  17:action_type_cleaned, count(1) total
  18:from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  19:group by date_gmt7,
  20:time_gmt7,
  21:hour_gmt7, 
  22:timestamp_gmt7,
  23:event_name,
  24:ga_session_id,
  25:user_id,
  26:event_params_page_title,
  27:event_params_product,
  28:action_type_cleaned
  29:having count(1) > 1;
  30:
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 22, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Unrecognized name: date_gmt7 at [8:8]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-23 06:32:59.667654 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5081ea6e-4d5d-479a-90ac-25e45e9876ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe09c72b700>]}
2021-12-23 06:32:59.668560 (Thread-1): 13:32:59 | 2 of 2 ERROR creating view model dataset_dbt.dqa_uniqueness_2........ [ERROR in 3.45s]
2021-12-23 06:32:59.668954 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-23 06:32:59.670560 (MainThread): Acquiring new bigquery connection "master".
2021-12-23 06:32:59.671221 (MainThread): 13:32:59 | 
2021-12-23 06:32:59.671546 (MainThread): 13:32:59 | Finished running 1 table model, 1 view model in 8.29s.
2021-12-23 06:32:59.671919 (MainThread): Connection 'master' was properly closed.
2021-12-23 06:32:59.672282 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-23 06:32:59.678967 (MainThread): 
2021-12-23 06:32:59.679451 (MainThread): Completed with 1 error and 0 warnings:
2021-12-23 06:32:59.679641 (MainThread): 
2021-12-23 06:32:59.679823 (MainThread): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
2021-12-23 06:32:59.679987 (MainThread):   Unrecognized name: date_gmt7 at [8:8]
2021-12-23 06:32:59.680141 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-23 06:32:59.680305 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
2021-12-23 06:32:59.680549 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0a190db20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe09e0c4c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe09e113160>]}
2021-12-23 06:32:59.680798 (MainThread): Flushing usage events
2021-12-23 06:34:40.771285 (MainThread): Running with dbt=0.21.0
2021-12-23 06:34:40.931485 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-23 06:34:40.932119 (MainThread): Tracking: tracking
2021-12-23 06:34:40.938019 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc318c55a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc316ea66d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc316ea6730>]}
2021-12-23 06:34:40.944596 (MainThread): Partial parsing not enabled
2021-12-23 06:34:40.948929 (MainThread): Parsing macros/catalog.sql
2021-12-23 06:34:40.953159 (MainThread): Parsing macros/etc.sql
2021-12-23 06:34:40.954525 (MainThread): Parsing macros/adapters.sql
2021-12-23 06:34:40.967564 (MainThread): Parsing macros/materializations/seed.sql
2021-12-23 06:34:40.969112 (MainThread): Parsing macros/materializations/copy.sql
2021-12-23 06:34:40.970824 (MainThread): Parsing macros/materializations/view.sql
2021-12-23 06:34:40.972571 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-23 06:34:40.973663 (MainThread): Parsing macros/materializations/table.sql
2021-12-23 06:34:40.979249 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-23 06:34:40.987621 (MainThread): Parsing macros/core.sql
2021-12-23 06:34:40.989928 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-23 06:34:40.991020 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-23 06:34:40.992418 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-23 06:34:40.993199 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-23 06:34:40.994070 (MainThread): Parsing macros/materializations/test.sql
2021-12-23 06:34:40.998414 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-23 06:34:41.004062 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-23 06:34:41.017824 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-23 06:34:41.022182 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-23 06:34:41.024703 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-23 06:34:41.036557 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-23 06:34:41.037732 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-23 06:34:41.044114 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-23 06:34:41.052684 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-23 06:34:41.057062 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-23 06:34:41.067310 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-23 06:34:41.068340 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-23 06:34:41.086751 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-23 06:34:41.087836 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-23 06:34:41.088905 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-23 06:34:41.089897 (MainThread): Parsing macros/etc/query.sql
2021-12-23 06:34:41.090521 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-23 06:34:41.092054 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-23 06:34:41.093615 (MainThread): Parsing macros/etc/datetime.sql
2021-12-23 06:34:41.099817 (MainThread): Parsing macros/adapters/common.sql
2021-12-23 06:34:41.249900 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-23 06:34:41.258351 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-23 06:34:41.274687 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-23 06:34:41.275927 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-23 06:34:41.277053 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-23 06:34:41.278263 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-23 06:34:41.287057 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6683ebd8-8e3c-4ddc-867c-0f3ce8100c1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc316d580d0>]}
2021-12-23 06:34:41.290072 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-23 06:34:41.290343 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6683ebd8-8e3c-4ddc-867c-0f3ce8100c1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc316d58460>]}
2021-12-23 06:34:41.290532 (MainThread): Found 2 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-23 06:34:41.291771 (MainThread): 
2021-12-23 06:34:41.292135 (MainThread): Acquiring new bigquery connection "master".
2021-12-23 06:34:41.292918 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-23 06:34:41.293185 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-23 06:34:41.821759 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-23 06:34:41.822097 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-23 06:34:41.827042 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-23 06:34:42.253159 (MainThread): 13:34:42 | Concurrency: 1 threads (target='prod')
2021-12-23 06:34:42.253360 (MainThread): 13:34:42 | 
2021-12-23 06:34:42.256314 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-23 06:34:42.257019 (Thread-1): 13:34:42 | 1 of 2 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-23 06:34:42.257669 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-23 06:34:42.257997 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-23 06:34:42.261895 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-23 06:34:42.262517 (Thread-1): finished collecting timing info
2021-12-23 06:34:42.275558 (Thread-1): Opening a new connection, currently in state closed
2021-12-23 06:34:42.278895 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-23 06:34:42.660755 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 35276), raddr=('74.125.200.95', 443)>
2021-12-23 06:34:42.661109 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 39346), raddr=('34.101.5.74', 443)>
2021-12-23 06:34:42.661265 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 35282), raddr=('74.125.200.95', 443)>
2021-12-23 06:34:42.661394 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 39352), raddr=('34.101.5.74', 443)>
2021-12-23 06:34:42.669922 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-23 06:34:42.670237 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-23 06:34:46.190608 (Thread-1): finished collecting timing info
2021-12-23 06:34:46.191412 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6683ebd8-8e3c-4ddc-867c-0f3ce8100c1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc316d583d0>]}
2021-12-23 06:34:46.192055 (Thread-1): 13:34:46 | 1 of 2 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 3.93s]
2021-12-23 06:34:46.192392 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-23 06:34:46.193669 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-23 06:34:46.194219 (Thread-1): 13:34:46 | 2 of 2 START view model dataset_dbt.dqa_uniqueness_2................. [RUN]
2021-12-23 06:34:46.194712 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-23 06:34:46.194976 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-23 06:34:46.198029 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-23 06:34:46.198551 (Thread-1): finished collecting timing info
2021-12-23 06:34:46.213446 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-23 06:34:46.213835 (Thread-1): Opening a new connection, currently in state closed
2021-12-23 06:34:46.217473 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`;


2021-12-23 06:34:47.138681 (Thread-1): finished collecting timing info
2021-12-23 06:34:47.139597 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6683ebd8-8e3c-4ddc-867c-0f3ce8100c1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc316d10b50>]}
2021-12-23 06:34:47.140338 (Thread-1): 13:34:47 | 2 of 2 OK created view model dataset_dbt.dqa_uniqueness_2............ [OK in 0.94s]
2021-12-23 06:34:47.140751 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-23 06:34:47.142789 (MainThread): Acquiring new bigquery connection "master".
2021-12-23 06:34:47.143649 (MainThread): 13:34:47 | 
2021-12-23 06:34:47.144046 (MainThread): 13:34:47 | Finished running 1 table model, 1 view model in 5.85s.
2021-12-23 06:34:47.144423 (MainThread): Connection 'master' was properly closed.
2021-12-23 06:34:47.144788 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-23 06:34:47.152629 (MainThread): 
2021-12-23 06:34:47.152986 (MainThread): Completed successfully
2021-12-23 06:34:47.153264 (MainThread): 
Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
2021-12-23 06:34:47.153614 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc316d58460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc316deb880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc316df4730>]}
2021-12-23 06:34:47.154018 (MainThread): Flushing usage events
2021-12-24 04:01:33.905669 (MainThread): Running with dbt=0.21.0
2021-12-24 04:01:34.156142 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 04:01:34.157030 (MainThread): Tracking: tracking
2021-12-24 04:01:34.164051 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37b0d3cac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37aef8b700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37aef8b760>]}
2021-12-24 04:01:34.170398 (MainThread): Partial parsing not enabled
2021-12-24 04:01:34.179338 (MainThread): Parsing macros/catalog.sql
2021-12-24 04:01:34.183449 (MainThread): Parsing macros/etc.sql
2021-12-24 04:01:34.184764 (MainThread): Parsing macros/adapters.sql
2021-12-24 04:01:34.197014 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 04:01:34.198485 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 04:01:34.200062 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 04:01:34.201535 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 04:01:34.202491 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 04:01:34.207783 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 04:01:34.216291 (MainThread): Parsing macros/core.sql
2021-12-24 04:01:34.218576 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 04:01:34.219642 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 04:01:34.221142 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 04:01:34.221869 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 04:01:34.222733 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 04:01:34.226843 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 04:01:34.232406 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 04:01:34.245075 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 04:01:34.249089 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 04:01:34.251296 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 04:01:34.262452 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 04:01:34.263562 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 04:01:34.270171 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 04:01:34.278283 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 04:01:34.282519 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 04:01:34.292049 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 04:01:34.293011 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 04:01:34.310608 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 04:01:34.311599 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 04:01:34.312743 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 04:01:34.313941 (MainThread): Parsing macros/etc/query.sql
2021-12-24 04:01:34.314731 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 04:01:34.316621 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 04:01:34.317907 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 04:01:34.324270 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 04:01:34.501730 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-24 04:01:34.510763 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:01:34.516254 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-24 04:01:34.536969 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:01:34.538347 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:01:34.539592 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:01:34.540831 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:01:34.551614 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b197abf4-8226-4ae2-aef3-f1340953adc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37aee620a0>]}
2021-12-24 04:01:34.554923 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 04:01:34.555252 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b197abf4-8226-4ae2-aef3-f1340953adc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37aee621c0>]}
2021-12-24 04:01:34.555488 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 04:01:34.556613 (MainThread): 
2021-12-24 04:01:34.556902 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:01:34.557724 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 04:01:34.557956 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 04:01:35.253879 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 04:01:35.254572 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 04:01:35.267166 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:01:35.662045 (MainThread): 11:01:35 | Concurrency: 1 threads (target='prod')
2021-12-24 04:01:35.662660 (MainThread): 11:01:35 | 
2021-12-24 04:01:35.666909 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 04:01:35.667183 (Thread-1): 11:01:35 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 04:01:35.667424 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:01:35.667558 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 04:01:35.674534 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:01:35.674923 (Thread-1): finished collecting timing info
2021-12-24 04:01:35.677041 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 40044), raddr=('74.125.24.95', 443)>
2021-12-24 04:01:35.677259 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55468), raddr=('34.101.5.42', 443)>
2021-12-24 04:01:35.677402 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 40048), raddr=('74.125.24.95', 443)>
2021-12-24 04:01:35.677526 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55472), raddr=('34.101.5.42', 443)>
2021-12-24 04:01:35.697186 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:01:35.697491 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:01:35.700778 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `satu-data-staging.fact_merdeka_mengajar.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(time_gmt7, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(time_gmt7, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result21.criteria, 
  result21.metrics, 
  result21.total_data, 
  result21.good_data, 
  result21.bad_data, 
  cast(
    cast(
      result21.good_data * 100 as decimal
    )/ result21.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
  ) as percentage_bad_data, 
  "save_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank save_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'save_reflection'  
  ) result21 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result22.criteria, 
  result22.metrics, 
  result22.total_data, 
  result22.good_data, 
  result22.bad_data, 
  cast(
    cast(
      result22.good_data * 100 as decimal
    )/ result22.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
  ) as percentage_bad_data, 
  "post_test_result" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank post_test_result' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(post_test_result, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(post_test_result, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'finished_post_test'  
  ) result22 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result23.criteria, 
  result23.metrics, 
  result23.total_data, 
  result23.good_data, 
  result23.bad_data, 
  cast(
    cast(
      result23.good_data * 100 as decimal
    )/ result23.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_source" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_source' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_source, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_source, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result23 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result24.criteria, 
  result24.metrics, 
  result24.total_data, 
  result24.good_data, 
  result24.bad_data, 
  cast(
    cast(
      result24.good_data * 100 as decimal
    )/ result24.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_location" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_location' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_location, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result24 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result25.criteria, 
  result25.metrics, 
  result25.total_data, 
  result25.good_data, 
  result25.bad_data, 
  cast(
    cast(
      result25.good_data * 100 as decimal
    )/ result25.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_referrer" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_referrer' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result25 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result26.criteria, 
  result26.metrics, 
  result26.total_data, 
  result26.good_data, 
  result26.bad_data, 
  cast(
    cast(
      result26.good_data * 100 as decimal
    )/ result26.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_path" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_path' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_path, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result26 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result27.criteria, 
  result27.metrics, 
  result27.total_data, 
  result27.good_data, 
  result27.bad_data, 
  cast(
    cast(
      result27.good_data * 100 as decimal
    )/ result27.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_title" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_title' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_title, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result27 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result28.criteria, 
  result28.metrics, 
  result28.total_data, 
  result28.good_data, 
  result28.bad_data, 
  cast(
    cast(
      result28.good_data * 100 as decimal
    )/ result28.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_product" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_product' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_product, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_product, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result28
  );
    
2021-12-24 04:01:37.060723 (Thread-1): finished collecting timing info
2021-12-24 04:01:37.061017 (Thread-1): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  Access Denied: Table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar: User does not have permission to query table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar.
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.Forbidden: 403 Access Denied: Table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar: User does not have permission to query table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar.

(job ID: 3522db59-f53c-4dde-a26c-67db9e0c156f)

                                                                               -----Query Job SQL Follows-----                                                                                

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    /*
  10:    Welcome to your first dbt model!
  11:    Did you know that you can also configure models directly within SQL files?
  12:    This will override configurations stated in dbt_project.yml
  13:
  14:    Try changing "table" to "view" below
  15:*/
  16:
  17:
  18:
  19:with x as (
  20:  select 
  21:    ga_session_id, 
  22:    user_id, 
  23:    user_pseudo_id, 
  24:    event_name, 
  25:    action_type_cleaned, 
  26:    user_first_timestamp_gmt7, 
  27:    timestamp_gmt7, 
  28:    date_gmt7, 
  29:    time_gmt7, 
  30:    hour_gmt7, 
  31:    device_category_cleaned, 
  32:    engagement_time_msec, 
  33:    material_id, 
  34:    module_id, 
  35:    video_id, 
  36:    video_inspirasi_id, 
  37:    video_inspirasi_playslist_id, 
  38:    quiz_id, 
  39:    text_id, 
  40:    open_reflection_id, 
  41:    save_reflection_id, 
  42:    post_test_result, 
  43:    event_params_source, 
  44:    event_params_page_location, 
  45:    event_params_page_referrer, 
  46:    event_params_page_path, 
  47:    event_params_page_title, 
  48:    event_params_product 
  49:  FROM 
  50:    `satu-data-staging.fact_merdeka_mengajar.event_tracker_belajar`
  51:) 
  52:select 
  53:  datetime(
  54:    current_timestamp(), 
  55:    "Asia/Jakarta"
  56:  ) as time_execution, 
  57:  result1.criteria, 
  58:  result1.metrics, 
  59:  result1.total_data, 
  60:  result1.good_data, 
  61:  result1.bad_data, 
  62:  cast(
  63:    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  64:  ) as percentage_good_data, 
  65:  cast(
  66:    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  67:  ) as percentage_bad_data, 
  68:  "ga_session_id" field_name_checking 
  69:from 
  70:  (
  71:    select 
  72:      'Completeness' criteria, 
  73:      'field mandatory is null/blank ga_session_id' metrics, 
  74:      count(ga_session_id) total_data, 
  75:      count(
  76:        case when ifnull(
  77:          cast(ga_session_id as string), 
  78:          ''
  79:        )<> '' then 'pass' end
  80:      ) as good_data, 
  81:      count(
  82:        case when ifnull(
  83:          cast(ga_session_id as string), 
  84:          ''
  85:        )= '' then 'fail' end
  86:      ) as bad_data 
  87:    from 
  88:      x
  89:  ) result1 
  90:union all 
  91:select 
  92:  datetime(
  93:    current_timestamp(), 
  94:    "Asia/Jakarta"
  95:  ) as time_execution, 
  96:  result2.criteria, 
  97:  result2.metrics, 
  98:  result2.total_data, 
  99:  result2.good_data, 
 100:  result2.bad_data, 
 101:  cast(
 102:    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
 103:  ) as percentage_good_data, 
 104:  cast(
 105:    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
 106:  ) as percentage_bad_data, 
 107:  "user_id" field_name_checking 
 108:from 
 109:  (
 110:    select 
 111:      'Completeness' criteria, 
 112:      'field mandatory is null/blank user_id' metrics, 
 113:      count(ga_session_id) total_data, 
 114:      count(
 115:        case when ifnull(user_id, '')<> '' then 'pass' end
 116:      ) as good_data, 
 117:      count(
 118:        case when ifnull(user_id, '')= '' then 'fail' end
 119:      ) as bad_data 
 120:    from 
 121:      x
 122:  ) result2 
 123:union all 
 124:select 
 125:  datetime(
 126:    current_timestamp(), 
 127:    "Asia/Jakarta"
 128:  ) as time_execution, 
 129:  result3.criteria, 
 130:  result3.metrics, 
 131:  result3.total_data, 
 132:  result3.good_data, 
 133:  result3.bad_data, 
 134:  cast(
 135:    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
 136:  ) as percentage_good_data, 
 137:  cast(
 138:    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
 139:  ) as percentage_bad_data, 
 140:  "user_pseudo_id" field_name_checking 
 141:from 
 142:  (
 143:    select 
 144:      'Completeness' criteria, 
 145:      'field mandatory is null/blank user_pseudo_id' metrics, 
 146:      count(ga_session_id) total_data, 
 147:      count(
 148:        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
 149:      ) as good_data, 
 150:      count(
 151:        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
 152:      ) as bad_data 
 153:    from 
 154:      x
 155:  ) result3 
 156:union all 
 157:select 
 158:  datetime(
 159:    current_timestamp(), 
 160:    "Asia/Jakarta"
 161:  ) as time_execution, 
 162:  result4.criteria, 
 163:  result4.metrics, 
 164:  result4.total_data, 
 165:  result4.good_data, 
 166:  result4.bad_data, 
 167:  cast(
 168:    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
 169:  ) as percentage_good_data, 
 170:  cast(
 171:    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
 172:  ) as percentage_bad_data, 
 173:  "event_name" field_name_checking 
 174:from 
 175:  (
 176:    select 
 177:      'Completeness' criteria, 
 178:      'field mandatory is null/blank event_name' metrics, 
 179:      count(ga_session_id) total_data, 
 180:      count(
 181:        case when ifnull(event_name, '')<> '' then 'pass' end
 182:      ) as good_data, 
 183:      count(
 184:        case when ifnull(event_name, '')= '' then 'fail' end
 185:      ) as bad_data 
 186:    from 
 187:      x
 188:  ) result4 
 189:union all 
 190:select 
 191:  datetime(
 192:    current_timestamp(), 
 193:    "Asia/Jakarta"
 194:  ) as time_execution, 
 195:  result5.criteria, 
 196:  result5.metrics, 
 197:  result5.total_data, 
 198:  result5.good_data, 
 199:  result5.bad_data, 
 200:  cast(
 201:    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
 202:  ) as percentage_good_data, 
 203:  cast(
 204:    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
 205:  ) as percentage_bad_data, 
 206:  "action_type_cleaned" field_name_checking 
 207:from 
 208:  (
 209:    select 
 210:      'Completeness' criteria, 
 211:      'field mandatory is null/blank action_type_cleaned' metrics, 
 212:      count(ga_session_id) total_data, 
 213:      count(
 214:        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
 215:      ) as good_data, 
 216:      count(
 217:        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
 218:      ) as bad_data 
 219:    from 
 220:      x
 221:  ) result5 
 222:union all 
 223:select 
 224:  datetime(
 225:    current_timestamp(), 
 226:    "Asia/Jakarta"
 227:  ) as time_execution, 
 228:  result6.criteria, 
 229:  result6.metrics, 
 230:  result6.total_data, 
 231:  result6.good_data, 
 232:  result6.bad_data, 
 233:  cast(
 234:    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
 235:  ) as percentage_good_data, 
 236:  cast(
 237:    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
 238:  ) as percentage_bad_data, 
 239:  "user_first_timestamp_gmt7" field_name_checking 
 240:from 
 241:  (
 242:    select 
 243:      'Completeness' criteria, 
 244:      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
 245:      count(ga_session_id) total_data, 
 246:      count(
 247:        case when ifnull(
 248:          cast(
 249:            user_first_timestamp_gmt7 as string
 250:          ), 
 251:          ''
 252:        )<> '' then 'pass' end
 253:      ) as good_data, 
 254:      count(
 255:        case when ifnull(
 256:          cast(
 257:            user_first_timestamp_gmt7 as string
 258:          ), 
 259:          ''
 260:        )= '' then 'fail' end
 261:      ) as bad_data 
 262:    from 
 263:      x
 264:  ) result6 
 265:union all 
 266:select 
 267:  datetime(
 268:    current_timestamp(), 
 269:    "Asia/Jakarta"
 270:  ) as time_execution, 
 271:  result7.criteria, 
 272:  result7.metrics, 
 273:  result7.total_data, 
 274:  result7.good_data, 
 275:  result7.bad_data, 
 276:  cast(
 277:    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
 278:  ) as percentage_good_data, 
 279:  cast(
 280:    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
 281:  ) as percentage_bad_data, 
 282:  "timestamp_gmt7" field_name_checking 
 283:from 
 284:  (
 285:    select 
 286:      'Completeness' criteria, 
 287:      'field mandatory is null/blank timestamp_gmt7' metrics, 
 288:      count(ga_session_id) total_data, 
 289:      count(
 290:        case when ifnull(
 291:          cast(timestamp_gmt7 as string), 
 292:          ''
 293:        )<> '' then 'pass' end
 294:      ) as good_data, 
 295:      count(
 296:        case when ifnull(
 297:          cast(timestamp_gmt7 as string), 
 298:          ''
 299:        )= '' then 'fail' end
 300:      ) as bad_data 
 301:    from 
 302:      x
 303:  ) result7 
 304:union all 
 305:select 
 306:  datetime(
 307:    current_timestamp(), 
 308:    "Asia/Jakarta"
 309:  ) as time_execution, 
 310:  result8.criteria, 
 311:  result8.metrics, 
 312:  result8.total_data, 
 313:  result8.good_data, 
 314:  result8.bad_data, 
 315:  cast(
 316:    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
 317:  ) as percentage_good_data, 
 318:  cast(
 319:    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
 320:  ) as percentage_bad_data, 
 321:  "date_gmt7" field_name_checking 
 322:from 
 323:  (
 324:    select 
 325:      'Completeness' criteria, 
 326:      'field mandatory is null/blank date_gmt7' metrics, 
 327:      count(ga_session_id) total_data, 
 328:      count(
 329:        case when ifnull(
 330:          cast(date_gmt7 as string), 
 331:          ''
 332:        )<> '' then 'pass' end
 333:      ) as good_data, 
 334:      count(
 335:        case when ifnull(
 336:          cast(date_gmt7 as string), 
 337:          ''
 338:        )= '' then 'fail' end
 339:      ) as bad_data 
 340:    from 
 341:      x
 342:  ) result8 
 343:union all 
 344:select 
 345:  datetime(
 346:    current_timestamp(), 
 347:    "Asia/Jakarta"
 348:  ) as time_execution, 
 349:  result9.criteria, 
 350:  result9.metrics, 
 351:  result9.total_data, 
 352:  result9.good_data, 
 353:  result9.bad_data, 
 354:  cast(
 355:    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
 356:  ) as percentage_good_data, 
 357:  cast(
 358:    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
 359:  ) as percentage_bad_data, 
 360:  "time_gmt7" field_name_checking 
 361:from 
 362:  (
 363:    select 
 364:      'Completeness' criteria, 
 365:      'field mandatory is null/blank time_gmt7' metrics, 
 366:      count(ga_session_id) total_data, 
 367:      count(
 368:        case when ifnull(time_gmt7, '')<> '' then 'pass' end
 369:      ) as good_data, 
 370:      count(
 371:        case when ifnull(time_gmt7, '')= '' then 'fail' end
 372:      ) as bad_data 
 373:    from 
 374:      x
 375:  ) result9 
 376:union all 
 377:select 
 378:  datetime(
 379:    current_timestamp(), 
 380:    "Asia/Jakarta"
 381:  ) as time_execution, 
 382:  result10.criteria, 
 383:  result10.metrics, 
 384:  result10.total_data, 
 385:  result10.good_data, 
 386:  result10.bad_data, 
 387:  cast(
 388:    cast(
 389:      result10.good_data * 100 as decimal
 390:    )/ result10.total_data as numeric
 391:  ) as percentage_good_data, 
 392:  cast(
 393:    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
 394:  ) as percentage_bad_data, 
 395:  "hour_gmt7" field_name_checking 
 396:from 
 397:  (
 398:    select 
 399:      'Completeness' criteria, 
 400:      'field mandatory is null/blank hour_gmt7' metrics, 
 401:      count(ga_session_id) total_data, 
 402:      count(
 403:        case when ifnull(
 404:          cast(hour_gmt7 as string), 
 405:          ''
 406:        )<> '' then 'pass' end
 407:      ) as good_data, 
 408:      count(
 409:        case when ifnull(
 410:          cast(hour_gmt7 as string), 
 411:          ''
 412:        )= '' then 'fail' end
 413:      ) as bad_data 
 414:    from 
 415:      x
 416:  ) result10 
 417:union all 
 418:select 
 419:  datetime(
 420:    current_timestamp(), 
 421:    "Asia/Jakarta"
 422:  ) as time_execution, 
 423:  result11.criteria, 
 424:  result11.metrics, 
 425:  result11.total_data, 
 426:  result11.good_data, 
 427:  result11.bad_data, 
 428:  cast(
 429:    cast(
 430:      result11.good_data * 100 as decimal
 431:    )/ result11.total_data as numeric
 432:  ) as percentage_good_data, 
 433:  cast(
 434:    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
 435:  ) as percentage_bad_data, 
 436:  "device_category_cleaned" field_name_checking 
 437:from 
 438:  (
 439:    select 
 440:      'Completeness' criteria, 
 441:      'field mandatory is null/blank device_category_cleaned' metrics, 
 442:      count(ga_session_id) total_data, 
 443:      count(
 444:        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
 445:      ) as good_data, 
 446:      count(
 447:        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
 448:      ) as bad_data 
 449:    from 
 450:      x
 451:  ) result11 
 452:union all 
 453:select 
 454:  datetime(
 455:    current_timestamp(), 
 456:    "Asia/Jakarta"
 457:  ) as time_execution, 
 458:  result12.criteria, 
 459:  result12.metrics, 
 460:  result12.total_data, 
 461:  result12.good_data, 
 462:  result12.bad_data, 
 463:  cast(
 464:    cast(
 465:      result12.good_data * 100 as decimal
 466:    )/ result12.total_data as numeric
 467:  ) as percentage_good_data, 
 468:  cast(
 469:    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
 470:  ) as percentage_bad_data, 
 471:  "engagement_time_msec" field_name_checking 
 472:from 
 473:  (
 474:    select 
 475:      'Completeness' criteria, 
 476:      'field mandatory is null/blank engagement_time_msec' metrics, 
 477:      count(ga_session_id) total_data, 
 478:      count(
 479:        case when ifnull(
 480:          cast(engagement_time_msec as string), 
 481:          ''
 482:        )<> '' then 'pass' end
 483:      ) as good_data, 
 484:      count(
 485:        case when ifnull(
 486:          cast(engagement_time_msec as string), 
 487:          ''
 488:        )= '' then 'fail' end
 489:      ) as bad_data 
 490:    from 
 491:      x
 492:  ) result12 
 493:union all 
 494:select 
 495:  datetime(
 496:    current_timestamp(), 
 497:    "Asia/Jakarta"
 498:  ) as time_execution, 
 499:  result13.criteria, 
 500:  result13.metrics, 
 501:  result13.total_data, 
 502:  result13.good_data, 
 503:  result13.bad_data, 
 504:  cast(
 505:    cast(
 506:      result13.good_data * 100 as decimal
 507:    )/ result13.total_data as numeric
 508:  ) as percentage_good_data, 
 509:  cast(
 510:    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
 511:  ) as percentage_bad_data, 
 512:  "material_id" field_name_checking 
 513:from 
 514:  (
 515:    select 
 516:      'Completeness' criteria, 
 517:      'field mandatory is null/blank material_id' metrics, 
 518:      count(ga_session_id) total_data, 
 519:      count(
 520:        case when ifnull(
 521:          cast(material_id as string), 
 522:          ''
 523:        )<> '' then 'pass' end
 524:      ) as good_data, 
 525:      count(
 526:        case when ifnull(
 527:          cast(material_id as string), 
 528:          ''
 529:        )= '' then 'fail' end
 530:      ) as bad_data 
 531:    from 
 532:      x
 533:    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
 534:  ) result13 
 535:union all 
 536:select 
 537:  datetime(
 538:    current_timestamp(), 
 539:    "Asia/Jakarta"
 540:  ) as time_execution, 
 541:  result14.criteria, 
 542:  result14.metrics, 
 543:  result14.total_data, 
 544:  result14.good_data, 
 545:  result14.bad_data, 
 546:  cast(
 547:    cast(
 548:      result14.good_data * 100 as decimal
 549:    )/ result14.total_data as numeric
 550:  ) as percentage_good_data, 
 551:  cast(
 552:    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
 553:  ) as percentage_bad_data, 
 554:  "module_id" field_name_checking 
 555:from 
 556:  (
 557:    select 
 558:      'Completeness' criteria, 
 559:      'field mandatory is null/blank module_id' metrics, 
 560:      count(ga_session_id) total_data, 
 561:      count(
 562:        case when ifnull(
 563:          cast(module_id as string), 
 564:          ''
 565:        )<> '' then 'pass' end
 566:      ) as good_data, 
 567:      count(
 568:        case when ifnull(
 569:          cast(module_id as string), 
 570:          ''
 571:        )= '' then 'fail' end
 572:      ) as bad_data 
 573:    from 
 574:      x
 575:    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
 576:  ) result14 
 577:union all 
 578:select 
 579:  datetime(
 580:    current_timestamp(), 
 581:    "Asia/Jakarta"
 582:  ) as time_execution, 
 583:  result15.criteria, 
 584:  result15.metrics, 
 585:  result15.total_data, 
 586:  result15.good_data, 
 587:  result15.bad_data, 
 588:  cast(
 589:    cast(
 590:      result15.good_data * 100 as decimal
 591:    )/ result15.total_data as numeric
 592:  ) as percentage_good_data, 
 593:  cast(
 594:    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
 595:  ) as percentage_bad_data, 
 596:  "video_id" field_name_checking 
 597:from 
 598:  (
 599:    select 
 600:      'Completeness' criteria, 
 601:      'field mandatory is null/blank video_id' metrics, 
 602:      count(ga_session_id) total_data, 
 603:      count(
 604:        case when ifnull(
 605:          cast(video_id as string), 
 606:          ''
 607:        )<> '' then 'pass' end
 608:      ) as good_data, 
 609:      count(
 610:        case when ifnull(
 611:          cast(video_id as string), 
 612:          ''
 613:        )= '' then 'fail' end
 614:      ) as bad_data 
 615:    from 
 616:      x
 617:    where action_type_cleaned = "play_video"  
 618:  ) result15 
 619:union all 
 620:select 
 621:  datetime(
 622:    current_timestamp(), 
 623:    "Asia/Jakarta"
 624:  ) as time_execution, 
 625:  result16.criteria, 
 626:  result16.metrics, 
 627:  result16.total_data, 
 628:  result16.good_data, 
 629:  result16.bad_data, 
 630:  cast(
 631:    cast(
 632:      result16.good_data * 100 as decimal
 633:    )/ result16.total_data as numeric
 634:  ) as percentage_good_data, 
 635:  cast(
 636:    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
 637:  ) as percentage_bad_data, 
 638:  "video_inspirasi_id" field_name_checking 
 639:from 
 640:  (
 641:    select 
 642:      'Completeness' criteria, 
 643:      'field mandatory is null/blank video_inspirasi_id' metrics, 
 644:      count(ga_session_id) total_data, 
 645:      count(
 646:        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
 647:      ) as good_data, 
 648:      count(
 649:        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
 650:      ) as bad_data 
 651:    from 
 652:      x
 653:    where action_type_cleaned = "play_video_inspirasi"  
 654:  ) result16 
 655:union all 
 656:select 
 657:  datetime(
 658:    current_timestamp(), 
 659:    "Asia/Jakarta"
 660:  ) as time_execution, 
 661:  result17.criteria, 
 662:  result17.metrics, 
 663:  result17.total_data, 
 664:  result17.good_data, 
 665:  result17.bad_data, 
 666:  cast(
 667:    cast(
 668:      result17.good_data * 100 as decimal
 669:    )/ result17.total_data as numeric
 670:  ) as percentage_good_data, 
 671:  cast(
 672:    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
 673:  ) as percentage_bad_data, 
 674:  "video_inspirasi_playslist_id" field_name_checking 
 675:from 
 676:  (
 677:    select 
 678:      'Completeness' criteria, 
 679:      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
 680:      count(ga_session_id) total_data, 
 681:      count(
 682:        case when ifnull(
 683:          cast(
 684:            video_inspirasi_playslist_id as string
 685:          ), 
 686:          ''
 687:        )<> '' then 'pass' end
 688:      ) as good_data, 
 689:      count(
 690:        case when ifnull(
 691:          cast(
 692:            video_inspirasi_playslist_id as string
 693:          ), 
 694:          ''
 695:        )= '' then 'fail' end
 696:      ) as bad_data 
 697:    from 
 698:      x
 699:    where action_type_cleaned = "play_video_inspirasi"  
 700:  ) result17 
 701:union all 
 702:select 
 703:  datetime(
 704:    current_timestamp(), 
 705:    "Asia/Jakarta"
 706:  ) as time_execution, 
 707:  result18.criteria, 
 708:  result18.metrics, 
 709:  result18.total_data, 
 710:  result18.good_data, 
 711:  result18.bad_data, 
 712:  cast(
 713:    cast(
 714:      result18.good_data * 100 as decimal
 715:    )/ result18.total_data as numeric
 716:  ) as percentage_good_data, 
 717:  cast(
 718:    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
 719:  ) as percentage_bad_data, 
 720:  "quiz_id" field_name_checking 
 721:from 
 722:  (
 723:    select 
 724:      'Completeness' criteria, 
 725:      'field mandatory is null/blank quiz_id' metrics, 
 726:      count(ga_session_id) total_data, 
 727:      count(
 728:        case when ifnull(
 729:          cast(quiz_id as string), 
 730:          ''
 731:        )<> '' then 'pass' end
 732:      ) as good_data, 
 733:      count(
 734:        case when ifnull(
 735:          cast(quiz_id as string), 
 736:          ''
 737:        )= '' then 'fail' end
 738:      ) as bad_data 
 739:    from 
 740:      x
 741:    where action_type_cleaned = 'open_quiz'  
 742:  ) result18 
 743:union all 
 744:select 
 745:  datetime(
 746:    current_timestamp(), 
 747:    "Asia/Jakarta"
 748:  ) as time_execution, 
 749:  result19.criteria, 
 750:  result19.metrics, 
 751:  result19.total_data, 
 752:  result19.good_data, 
 753:  result19.bad_data, 
 754:  cast(
 755:    cast(
 756:      result19.good_data * 100 as decimal
 757:    )/ result19.total_data as numeric
 758:  ) as percentage_good_data, 
 759:  cast(
 760:    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
 761:  ) as percentage_bad_data, 
 762:  "text_id" field_name_checking 
 763:from 
 764:  (
 765:    select 
 766:      'Completeness' criteria, 
 767:      'field mandatory is null/blank text_id' metrics, 
 768:      count(ga_session_id) total_data, 
 769:      count(
 770:        case when ifnull(
 771:          cast(text_id as string), 
 772:          ''
 773:        )<> '' then 'pass' end
 774:      ) as good_data, 
 775:      count(
 776:        case when ifnull(
 777:          cast(text_id as string), 
 778:          ''
 779:        )= '' then 'fail' end
 780:      ) as bad_data 
 781:    from 
 782:      x
 783:    where action_type_cleaned = 'open_text'  
 784:  ) result19 
 785:union all 
 786:select 
 787:  datetime(
 788:    current_timestamp(), 
 789:    "Asia/Jakarta"
 790:  ) as time_execution, 
 791:  result20.criteria, 
 792:  result20.metrics, 
 793:  result20.total_data, 
 794:  result20.good_data, 
 795:  result20.bad_data, 
 796:  cast(
 797:    cast(
 798:      result20.good_data * 100 as decimal
 799:    )/ result20.total_data as numeric
 800:  ) as percentage_good_data, 
 801:  cast(
 802:    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
 803:  ) as percentage_bad_data, 
 804:  "open_reflection_id" field_name_checking 
 805:from 
 806:  (
 807:    select 
 808:      'Completeness' criteria, 
 809:      'field mandatory is null/blank open_reflection_id' metrics, 
 810:      count(ga_session_id) total_data, 
 811:      count(
 812:        case when ifnull(
 813:          cast(open_reflection_id as string), 
 814:          ''
 815:        )<> '' then 'pass' end
 816:      ) as good_data, 
 817:      count(
 818:        case when ifnull(
 819:          cast(open_reflection_id as string), 
 820:          ''
 821:        )= '' then 'fail' end
 822:      ) as bad_data 
 823:    from 
 824:      x
 825:    where action_type_cleaned = "open_reflection"  
 826:  ) result20 
 827:union all 
 828:select 
 829:  datetime(
 830:    current_timestamp(), 
 831:    "Asia/Jakarta"
 832:  ) as time_execution, 
 833:  result21.criteria, 
 834:  result21.metrics, 
 835:  result21.total_data, 
 836:  result21.good_data, 
 837:  result21.bad_data, 
 838:  cast(
 839:    cast(
 840:      result21.good_data * 100 as decimal
 841:    )/ result21.total_data as numeric
 842:  ) as percentage_good_data, 
 843:  cast(
 844:    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
 845:  ) as percentage_bad_data, 
 846:  "save_reflection_id" field_name_checking 
 847:from 
 848:  (
 849:    select 
 850:      'Completeness' criteria, 
 851:      'field mandatory is null/blank save_reflection_id' metrics, 
 852:      count(ga_session_id) total_data, 
 853:      count(
 854:        case when ifnull(
 855:          cast(save_reflection_id as string), 
 856:          ''
 857:        )<> '' then 'pass' end
 858:      ) as good_data, 
 859:      count(
 860:        case when ifnull(
 861:          cast(save_reflection_id as string), 
 862:          ''
 863:        )= '' then 'fail' end
 864:      ) as bad_data 
 865:    from 
 866:      x
 867:    where action_type_cleaned = 'save_reflection'  
 868:  ) result21 
 869:union all 
 870:select 
 871:  datetime(
 872:    current_timestamp(), 
 873:    "Asia/Jakarta"
 874:  ) as time_execution, 
 875:  result22.criteria, 
 876:  result22.metrics, 
 877:  result22.total_data, 
 878:  result22.good_data, 
 879:  result22.bad_data, 
 880:  cast(
 881:    cast(
 882:      result22.good_data * 100 as decimal
 883:    )/ result22.total_data as numeric
 884:  ) as percentage_good_data, 
 885:  cast(
 886:    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
 887:  ) as percentage_bad_data, 
 888:  "post_test_result" field_name_checking 
 889:from 
 890:  (
 891:    select 
 892:      'Completeness' criteria, 
 893:      'field mandatory is null/blank post_test_result' metrics, 
 894:      count(ga_session_id) total_data, 
 895:      count(
 896:        case when ifnull(post_test_result, '')<> '' then 'pass' end
 897:      ) as good_data, 
 898:      count(
 899:        case when ifnull(post_test_result, '')= '' then 'fail' end
 900:      ) as bad_data 
 901:    from 
 902:      x
 903:    where action_type_cleaned = 'finished_post_test'  
 904:  ) result22 
 905:union all 
 906:select 
 907:  datetime(
 908:    current_timestamp(), 
 909:    "Asia/Jakarta"
 910:  ) as time_execution, 
 911:  result23.criteria, 
 912:  result23.metrics, 
 913:  result23.total_data, 
 914:  result23.good_data, 
 915:  result23.bad_data, 
 916:  cast(
 917:    cast(
 918:      result23.good_data * 100 as decimal
 919:    )/ result23.total_data as numeric
 920:  ) as percentage_good_data, 
 921:  cast(
 922:    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
 923:  ) as percentage_bad_data, 
 924:  "event_params_source" field_name_checking 
 925:from 
 926:  (
 927:    select 
 928:      'Completeness' criteria, 
 929:      'field mandatory is null/blank event_params_source' metrics, 
 930:      count(ga_session_id) total_data, 
 931:      count(
 932:        case when ifnull(event_params_source, '')<> '' then 'pass' end
 933:      ) as good_data, 
 934:      count(
 935:        case when ifnull(event_params_source, '')= '' then 'fail' end
 936:      ) as bad_data 
 937:    from 
 938:      x
 939:  ) result23 
 940:union all 
 941:select 
 942:  datetime(
 943:    current_timestamp(), 
 944:    "Asia/Jakarta"
 945:  ) as time_execution, 
 946:  result24.criteria, 
 947:  result24.metrics, 
 948:  result24.total_data, 
 949:  result24.good_data, 
 950:  result24.bad_data, 
 951:  cast(
 952:    cast(
 953:      result24.good_data * 100 as decimal
 954:    )/ result24.total_data as numeric
 955:  ) as percentage_good_data, 
 956:  cast(
 957:    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
 958:  ) as percentage_bad_data, 
 959:  "event_params_page_location" field_name_checking 
 960:from 
 961:  (
 962:    select 
 963:      'Completeness' criteria, 
 964:      'field mandatory is null/blank event_params_page_location' metrics, 
 965:      count(ga_session_id) total_data, 
 966:      count(
 967:        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
 968:      ) as good_data, 
 969:      count(
 970:        case when ifnull(event_params_page_location, '')= '' then 'fail' end
 971:      ) as bad_data 
 972:    from 
 973:      x
 974:  ) result24 
 975:union all 
 976:select 
 977:  datetime(
 978:    current_timestamp(), 
 979:    "Asia/Jakarta"
 980:  ) as time_execution, 
 981:  result25.criteria, 
 982:  result25.metrics, 
 983:  result25.total_data, 
 984:  result25.good_data, 
 985:  result25.bad_data, 
 986:  cast(
 987:    cast(
 988:      result25.good_data * 100 as decimal
 989:    )/ result25.total_data as numeric
 990:  ) as percentage_good_data, 
 991:  cast(
 992:    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
 993:  ) as percentage_bad_data, 
 994:  "event_params_page_referrer" field_name_checking 
 995:from 
 996:  (
 997:    select 
 998:      'Completeness' criteria, 
 999:      'field mandatory is null/blank event_params_page_referrer' metrics, 
1000:      count(ga_session_id) total_data, 
1001:      count(
1002:        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
1003:      ) as good_data, 
1004:      count(
1005:        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
1006:      ) as bad_data 
1007:    from 
1008:      x
1009:  ) result25 
1010:union all 
1011:select 
1012:  datetime(
1013:    current_timestamp(), 
1014:    "Asia/Jakarta"
1015:  ) as time_execution, 
1016:  result26.criteria, 
1017:  result26.metrics, 
1018:  result26.total_data, 
1019:  result26.good_data, 
1020:  result26.bad_data, 
1021:  cast(
1022:    cast(
1023:      result26.good_data * 100 as decimal
1024:    )/ result26.total_data as numeric
1025:  ) as percentage_good_data, 
1026:  cast(
1027:    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
1028:  ) as percentage_bad_data, 
1029:  "event_params_page_path" field_name_checking 
1030:from 
1031:  (
1032:    select 
1033:      'Completeness' criteria, 
1034:      'field mandatory is null/blank event_params_page_path' metrics, 
1035:      count(ga_session_id) total_data, 
1036:      count(
1037:        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
1038:      ) as good_data, 
1039:      count(
1040:        case when ifnull(event_params_page_path, '')= '' then 'fail' end
1041:      ) as bad_data 
1042:    from 
1043:      x
1044:  ) result26 
1045:union all 
1046:select 
1047:  datetime(
1048:    current_timestamp(), 
1049:    "Asia/Jakarta"
1050:  ) as time_execution, 
1051:  result27.criteria, 
1052:  result27.metrics, 
1053:  result27.total_data, 
1054:  result27.good_data, 
1055:  result27.bad_data, 
1056:  cast(
1057:    cast(
1058:      result27.good_data * 100 as decimal
1059:    )/ result27.total_data as numeric
1060:  ) as percentage_good_data, 
1061:  cast(
1062:    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
1063:  ) as percentage_bad_data, 
1064:  "event_params_page_title" field_name_checking 
1065:from 
1066:  (
1067:    select 
1068:      'Completeness' criteria, 
1069:      'field mandatory is null/blank event_params_page_title' metrics, 
1070:      count(ga_session_id) total_data, 
1071:      count(
1072:        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
1073:      ) as good_data, 
1074:      count(
1075:        case when ifnull(event_params_page_title, '')= '' then 'fail' end
1076:      ) as bad_data 
1077:    from 
1078:      x
1079:  ) result27 
1080:union all 
1081:select 
1082:  datetime(
1083:    current_timestamp(), 
1084:    "Asia/Jakarta"
1085:  ) as time_execution, 
1086:  result28.criteria, 
1087:  result28.metrics, 
1088:  result28.total_data, 
1089:  result28.good_data, 
1090:  result28.bad_data, 
1091:  cast(
1092:    cast(
1093:      result28.good_data * 100 as decimal
1094:    )/ result28.total_data as numeric
1095:  ) as percentage_good_data, 
1096:  cast(
1097:    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
1098:  ) as percentage_bad_data, 
1099:  "event_params_product" field_name_checking 
1100:from 
1101:  (
1102:    select 
1103:      'Completeness' criteria, 
1104:      'field mandatory is null/blank event_params_product' metrics, 
1105:      count(ga_session_id) total_data, 
1106:      count(
1107:        case when ifnull(event_params_product, '')<> '' then 'pass' end
1108:      ) as good_data, 
1109:      count(
1110:        case when ifnull(event_params_product, '')= '' then 'fail' end
1111:      ) as bad_data 
1112:    from 
1113:      x
1114:  ) result28
1115:  );
1116:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 171, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  Access Denied: Table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar: User does not have permission to query table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar.
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 04:01:37.073692 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b197abf4-8226-4ae2-aef3-f1340953adc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37aedf5ac0>]}
2021-12-24 04:01:37.074600 (Thread-1): 11:01:37 | 1 of 3 ERROR creating table model dataset_dbt.dqa_completeness....... [ERROR in 1.41s]
2021-12-24 04:01:37.075102 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 04:01:37.075596 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-24 04:01:37.076272 (Thread-1): 11:01:37 | 2 of 3 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-24 04:01:37.077405 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-24 04:01:37.077884 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-24 04:01:37.082250 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-24 04:01:37.082733 (Thread-1): finished collecting timing info
2021-12-24 04:01:37.083781 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:01:37.087698 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:01:37.457409 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-24 04:01:37.458419 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 04:01:41.426158 (Thread-1): finished collecting timing info
2021-12-24 04:01:41.427175 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b197abf4-8226-4ae2-aef3-f1340953adc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37aedf1c70>]}
2021-12-24 04:01:41.428105 (Thread-1): 11:01:41 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 4.35s]
2021-12-24 04:01:41.428626 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-24 04:01:41.430099 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-24 04:01:41.430619 (Thread-1): 11:01:41 | 3 of 3 SKIP relation dataset_dbt.dqa_uniqueness_2.................... [SKIP]
2021-12-24 04:01:41.431014 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-24 04:01:41.432509 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:01:41.433122 (MainThread): 11:01:41 | 
2021-12-24 04:01:41.433454 (MainThread): 11:01:41 | Finished running 2 table models, 1 view model in 6.88s.
2021-12-24 04:01:41.433835 (MainThread): Connection 'master' was properly closed.
2021-12-24 04:01:41.434084 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_1' was properly closed.
2021-12-24 04:01:41.438184 (MainThread): 
2021-12-24 04:01:41.438328 (MainThread): Completed with 1 error and 0 warnings:
2021-12-24 04:01:41.438445 (MainThread): 
2021-12-24 04:01:41.438563 (MainThread): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
2021-12-24 04:01:41.438667 (MainThread):   Access Denied: Table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar: User does not have permission to query table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar.
2021-12-24 04:01:41.438770 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 04:01:41.438887 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
2021-12-24 04:01:41.439048 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37aee62220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37b26d3b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37aefb1b20>]}
2021-12-24 04:01:41.439218 (MainThread): Flushing usage events
2021-12-24 04:03:10.778295 (MainThread): Running with dbt=0.21.0
2021-12-24 04:03:10.926703 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 04:03:10.927249 (MainThread): Tracking: tracking
2021-12-24 04:03:10.932583 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae06509a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae047a16a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae047a1700>]}
2021-12-24 04:03:10.938548 (MainThread): Partial parsing not enabled
2021-12-24 04:03:10.942566 (MainThread): Parsing macros/catalog.sql
2021-12-24 04:03:10.946441 (MainThread): Parsing macros/etc.sql
2021-12-24 04:03:10.947732 (MainThread): Parsing macros/adapters.sql
2021-12-24 04:03:10.960058 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 04:03:10.961534 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 04:03:10.963073 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 04:03:10.964623 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 04:03:10.965698 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 04:03:10.971126 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 04:03:10.979540 (MainThread): Parsing macros/core.sql
2021-12-24 04:03:10.981739 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 04:03:10.982690 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 04:03:10.984081 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 04:03:10.984855 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 04:03:10.985644 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 04:03:10.989864 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 04:03:10.995294 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 04:03:11.008130 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 04:03:11.012206 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 04:03:11.014507 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 04:03:11.025511 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 04:03:11.026539 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 04:03:11.032340 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 04:03:11.039945 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 04:03:11.044016 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 04:03:11.053527 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 04:03:11.054493 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 04:03:11.071737 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 04:03:11.072669 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 04:03:11.073602 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 04:03:11.074470 (MainThread): Parsing macros/etc/query.sql
2021-12-24 04:03:11.075031 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 04:03:11.076543 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 04:03:11.077654 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 04:03:11.082364 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 04:03:11.223172 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-24 04:03:11.230571 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:03:11.234394 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-24 04:03:11.249060 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:03:11.250207 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:03:11.251191 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:03:11.252232 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:03:11.260224 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '53551bc3-fbbd-4a4c-99a2-e857d467a787', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae0462e040>]}
2021-12-24 04:03:11.262916 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 04:03:11.263185 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '53551bc3-fbbd-4a4c-99a2-e857d467a787', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae04695ac0>]}
2021-12-24 04:03:11.263364 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 04:03:11.264351 (MainThread): 
2021-12-24 04:03:11.264586 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:03:11.265290 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 04:03:11.265559 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 04:03:11.782029 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 04:03:11.782845 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 04:03:11.795161 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:03:12.133033 (MainThread): 11:03:12 | Concurrency: 1 threads (target='prod')
2021-12-24 04:03:12.133621 (MainThread): 11:03:12 | 
2021-12-24 04:03:12.138126 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 04:03:12.138932 (Thread-1): 11:03:12 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 04:03:12.139757 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:03:12.140458 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 04:03:12.146791 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:03:12.147466 (Thread-1): finished collecting timing info
2021-12-24 04:03:12.150878 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 43434), raddr=('142.251.10.95', 443)>
2021-12-24 04:03:12.151147 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55498), raddr=('34.101.5.42', 443)>
2021-12-24 04:03:12.151332 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 43440), raddr=('142.251.10.95', 443)>
2021-12-24 04:03:12.151488 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55502), raddr=('34.101.5.42', 443)>
2021-12-24 04:03:12.171924 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:03:12.172291 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:03:12.175374 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(time_gmt7, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(time_gmt7, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result21.criteria, 
  result21.metrics, 
  result21.total_data, 
  result21.good_data, 
  result21.bad_data, 
  cast(
    cast(
      result21.good_data * 100 as decimal
    )/ result21.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
  ) as percentage_bad_data, 
  "save_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank save_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'save_reflection'  
  ) result21 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result22.criteria, 
  result22.metrics, 
  result22.total_data, 
  result22.good_data, 
  result22.bad_data, 
  cast(
    cast(
      result22.good_data * 100 as decimal
    )/ result22.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
  ) as percentage_bad_data, 
  "post_test_result" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank post_test_result' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(post_test_result, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(post_test_result, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'finished_post_test'  
  ) result22 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result23.criteria, 
  result23.metrics, 
  result23.total_data, 
  result23.good_data, 
  result23.bad_data, 
  cast(
    cast(
      result23.good_data * 100 as decimal
    )/ result23.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_source" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_source' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_source, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_source, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result23 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result24.criteria, 
  result24.metrics, 
  result24.total_data, 
  result24.good_data, 
  result24.bad_data, 
  cast(
    cast(
      result24.good_data * 100 as decimal
    )/ result24.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_location" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_location' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_location, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result24 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result25.criteria, 
  result25.metrics, 
  result25.total_data, 
  result25.good_data, 
  result25.bad_data, 
  cast(
    cast(
      result25.good_data * 100 as decimal
    )/ result25.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_referrer" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_referrer' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result25 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result26.criteria, 
  result26.metrics, 
  result26.total_data, 
  result26.good_data, 
  result26.bad_data, 
  cast(
    cast(
      result26.good_data * 100 as decimal
    )/ result26.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_path" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_path' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_path, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result26 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result27.criteria, 
  result27.metrics, 
  result27.total_data, 
  result27.good_data, 
  result27.bad_data, 
  cast(
    cast(
      result27.good_data * 100 as decimal
    )/ result27.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_title" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_title' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_title, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result27 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result28.criteria, 
  result28.metrics, 
  result28.total_data, 
  result28.good_data, 
  result28.bad_data, 
  cast(
    cast(
      result28.good_data * 100 as decimal
    )/ result28.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_product" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_product' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_product, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_product, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result28
  );
    
2021-12-24 04:03:14.921721 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]')
2021-12-24 04:03:17.933730 (Thread-1): finished collecting timing info
2021-12-24 04:03:17.934840 (Thread-1): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]

(job ID: e23e58f8-3559-4e2f-8923-22d6008f13bd)

                                                                               -----Query Job SQL Follows-----                                                                                

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    /*
  10:    Welcome to your first dbt model!
  11:    Did you know that you can also configure models directly within SQL files?
  12:    This will override configurations stated in dbt_project.yml
  13:
  14:    Try changing "table" to "view" below
  15:*/
  16:
  17:
  18:
  19:with x as (
  20:  select 
  21:    ga_session_id, 
  22:    user_id, 
  23:    user_pseudo_id, 
  24:    event_name, 
  25:    action_type_cleaned, 
  26:    user_first_timestamp_gmt7, 
  27:    timestamp_gmt7, 
  28:    date_gmt7, 
  29:    time_gmt7, 
  30:    hour_gmt7, 
  31:    device_category_cleaned, 
  32:    engagement_time_msec, 
  33:    material_id, 
  34:    module_id, 
  35:    video_id, 
  36:    video_inspirasi_id, 
  37:    video_inspirasi_playslist_id, 
  38:    quiz_id, 
  39:    text_id, 
  40:    open_reflection_id, 
  41:    save_reflection_id, 
  42:    post_test_result, 
  43:    event_params_source, 
  44:    event_params_page_location, 
  45:    event_params_page_referrer, 
  46:    event_params_page_path, 
  47:    event_params_page_title, 
  48:    event_params_product 
  49:  FROM 
  50:    `playground-325606.dataset_dbt.event_tracker_belajar`
  51:) 
  52:select 
  53:  datetime(
  54:    current_timestamp(), 
  55:    "Asia/Jakarta"
  56:  ) as time_execution, 
  57:  result1.criteria, 
  58:  result1.metrics, 
  59:  result1.total_data, 
  60:  result1.good_data, 
  61:  result1.bad_data, 
  62:  cast(
  63:    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  64:  ) as percentage_good_data, 
  65:  cast(
  66:    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  67:  ) as percentage_bad_data, 
  68:  "ga_session_id" field_name_checking 
  69:from 
  70:  (
  71:    select 
  72:      'Completeness' criteria, 
  73:      'field mandatory is null/blank ga_session_id' metrics, 
  74:      count(ga_session_id) total_data, 
  75:      count(
  76:        case when ifnull(
  77:          cast(ga_session_id as string), 
  78:          ''
  79:        )<> '' then 'pass' end
  80:      ) as good_data, 
  81:      count(
  82:        case when ifnull(
  83:          cast(ga_session_id as string), 
  84:          ''
  85:        )= '' then 'fail' end
  86:      ) as bad_data 
  87:    from 
  88:      x
  89:  ) result1 
  90:union all 
  91:select 
  92:  datetime(
  93:    current_timestamp(), 
  94:    "Asia/Jakarta"
  95:  ) as time_execution, 
  96:  result2.criteria, 
  97:  result2.metrics, 
  98:  result2.total_data, 
  99:  result2.good_data, 
 100:  result2.bad_data, 
 101:  cast(
 102:    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
 103:  ) as percentage_good_data, 
 104:  cast(
 105:    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
 106:  ) as percentage_bad_data, 
 107:  "user_id" field_name_checking 
 108:from 
 109:  (
 110:    select 
 111:      'Completeness' criteria, 
 112:      'field mandatory is null/blank user_id' metrics, 
 113:      count(ga_session_id) total_data, 
 114:      count(
 115:        case when ifnull(user_id, '')<> '' then 'pass' end
 116:      ) as good_data, 
 117:      count(
 118:        case when ifnull(user_id, '')= '' then 'fail' end
 119:      ) as bad_data 
 120:    from 
 121:      x
 122:  ) result2 
 123:union all 
 124:select 
 125:  datetime(
 126:    current_timestamp(), 
 127:    "Asia/Jakarta"
 128:  ) as time_execution, 
 129:  result3.criteria, 
 130:  result3.metrics, 
 131:  result3.total_data, 
 132:  result3.good_data, 
 133:  result3.bad_data, 
 134:  cast(
 135:    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
 136:  ) as percentage_good_data, 
 137:  cast(
 138:    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
 139:  ) as percentage_bad_data, 
 140:  "user_pseudo_id" field_name_checking 
 141:from 
 142:  (
 143:    select 
 144:      'Completeness' criteria, 
 145:      'field mandatory is null/blank user_pseudo_id' metrics, 
 146:      count(ga_session_id) total_data, 
 147:      count(
 148:        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
 149:      ) as good_data, 
 150:      count(
 151:        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
 152:      ) as bad_data 
 153:    from 
 154:      x
 155:  ) result3 
 156:union all 
 157:select 
 158:  datetime(
 159:    current_timestamp(), 
 160:    "Asia/Jakarta"
 161:  ) as time_execution, 
 162:  result4.criteria, 
 163:  result4.metrics, 
 164:  result4.total_data, 
 165:  result4.good_data, 
 166:  result4.bad_data, 
 167:  cast(
 168:    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
 169:  ) as percentage_good_data, 
 170:  cast(
 171:    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
 172:  ) as percentage_bad_data, 
 173:  "event_name" field_name_checking 
 174:from 
 175:  (
 176:    select 
 177:      'Completeness' criteria, 
 178:      'field mandatory is null/blank event_name' metrics, 
 179:      count(ga_session_id) total_data, 
 180:      count(
 181:        case when ifnull(event_name, '')<> '' then 'pass' end
 182:      ) as good_data, 
 183:      count(
 184:        case when ifnull(event_name, '')= '' then 'fail' end
 185:      ) as bad_data 
 186:    from 
 187:      x
 188:  ) result4 
 189:union all 
 190:select 
 191:  datetime(
 192:    current_timestamp(), 
 193:    "Asia/Jakarta"
 194:  ) as time_execution, 
 195:  result5.criteria, 
 196:  result5.metrics, 
 197:  result5.total_data, 
 198:  result5.good_data, 
 199:  result5.bad_data, 
 200:  cast(
 201:    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
 202:  ) as percentage_good_data, 
 203:  cast(
 204:    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
 205:  ) as percentage_bad_data, 
 206:  "action_type_cleaned" field_name_checking 
 207:from 
 208:  (
 209:    select 
 210:      'Completeness' criteria, 
 211:      'field mandatory is null/blank action_type_cleaned' metrics, 
 212:      count(ga_session_id) total_data, 
 213:      count(
 214:        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
 215:      ) as good_data, 
 216:      count(
 217:        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
 218:      ) as bad_data 
 219:    from 
 220:      x
 221:  ) result5 
 222:union all 
 223:select 
 224:  datetime(
 225:    current_timestamp(), 
 226:    "Asia/Jakarta"
 227:  ) as time_execution, 
 228:  result6.criteria, 
 229:  result6.metrics, 
 230:  result6.total_data, 
 231:  result6.good_data, 
 232:  result6.bad_data, 
 233:  cast(
 234:    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
 235:  ) as percentage_good_data, 
 236:  cast(
 237:    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
 238:  ) as percentage_bad_data, 
 239:  "user_first_timestamp_gmt7" field_name_checking 
 240:from 
 241:  (
 242:    select 
 243:      'Completeness' criteria, 
 244:      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
 245:      count(ga_session_id) total_data, 
 246:      count(
 247:        case when ifnull(
 248:          cast(
 249:            user_first_timestamp_gmt7 as string
 250:          ), 
 251:          ''
 252:        )<> '' then 'pass' end
 253:      ) as good_data, 
 254:      count(
 255:        case when ifnull(
 256:          cast(
 257:            user_first_timestamp_gmt7 as string
 258:          ), 
 259:          ''
 260:        )= '' then 'fail' end
 261:      ) as bad_data 
 262:    from 
 263:      x
 264:  ) result6 
 265:union all 
 266:select 
 267:  datetime(
 268:    current_timestamp(), 
 269:    "Asia/Jakarta"
 270:  ) as time_execution, 
 271:  result7.criteria, 
 272:  result7.metrics, 
 273:  result7.total_data, 
 274:  result7.good_data, 
 275:  result7.bad_data, 
 276:  cast(
 277:    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
 278:  ) as percentage_good_data, 
 279:  cast(
 280:    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
 281:  ) as percentage_bad_data, 
 282:  "timestamp_gmt7" field_name_checking 
 283:from 
 284:  (
 285:    select 
 286:      'Completeness' criteria, 
 287:      'field mandatory is null/blank timestamp_gmt7' metrics, 
 288:      count(ga_session_id) total_data, 
 289:      count(
 290:        case when ifnull(
 291:          cast(timestamp_gmt7 as string), 
 292:          ''
 293:        )<> '' then 'pass' end
 294:      ) as good_data, 
 295:      count(
 296:        case when ifnull(
 297:          cast(timestamp_gmt7 as string), 
 298:          ''
 299:        )= '' then 'fail' end
 300:      ) as bad_data 
 301:    from 
 302:      x
 303:  ) result7 
 304:union all 
 305:select 
 306:  datetime(
 307:    current_timestamp(), 
 308:    "Asia/Jakarta"
 309:  ) as time_execution, 
 310:  result8.criteria, 
 311:  result8.metrics, 
 312:  result8.total_data, 
 313:  result8.good_data, 
 314:  result8.bad_data, 
 315:  cast(
 316:    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
 317:  ) as percentage_good_data, 
 318:  cast(
 319:    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
 320:  ) as percentage_bad_data, 
 321:  "date_gmt7" field_name_checking 
 322:from 
 323:  (
 324:    select 
 325:      'Completeness' criteria, 
 326:      'field mandatory is null/blank date_gmt7' metrics, 
 327:      count(ga_session_id) total_data, 
 328:      count(
 329:        case when ifnull(
 330:          cast(date_gmt7 as string), 
 331:          ''
 332:        )<> '' then 'pass' end
 333:      ) as good_data, 
 334:      count(
 335:        case when ifnull(
 336:          cast(date_gmt7 as string), 
 337:          ''
 338:        )= '' then 'fail' end
 339:      ) as bad_data 
 340:    from 
 341:      x
 342:  ) result8 
 343:union all 
 344:select 
 345:  datetime(
 346:    current_timestamp(), 
 347:    "Asia/Jakarta"
 348:  ) as time_execution, 
 349:  result9.criteria, 
 350:  result9.metrics, 
 351:  result9.total_data, 
 352:  result9.good_data, 
 353:  result9.bad_data, 
 354:  cast(
 355:    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
 356:  ) as percentage_good_data, 
 357:  cast(
 358:    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
 359:  ) as percentage_bad_data, 
 360:  "time_gmt7" field_name_checking 
 361:from 
 362:  (
 363:    select 
 364:      'Completeness' criteria, 
 365:      'field mandatory is null/blank time_gmt7' metrics, 
 366:      count(ga_session_id) total_data, 
 367:      count(
 368:        case when ifnull(time_gmt7, '')<> '' then 'pass' end
 369:      ) as good_data, 
 370:      count(
 371:        case when ifnull(time_gmt7, '')= '' then 'fail' end
 372:      ) as bad_data 
 373:    from 
 374:      x
 375:  ) result9 
 376:union all 
 377:select 
 378:  datetime(
 379:    current_timestamp(), 
 380:    "Asia/Jakarta"
 381:  ) as time_execution, 
 382:  result10.criteria, 
 383:  result10.metrics, 
 384:  result10.total_data, 
 385:  result10.good_data, 
 386:  result10.bad_data, 
 387:  cast(
 388:    cast(
 389:      result10.good_data * 100 as decimal
 390:    )/ result10.total_data as numeric
 391:  ) as percentage_good_data, 
 392:  cast(
 393:    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
 394:  ) as percentage_bad_data, 
 395:  "hour_gmt7" field_name_checking 
 396:from 
 397:  (
 398:    select 
 399:      'Completeness' criteria, 
 400:      'field mandatory is null/blank hour_gmt7' metrics, 
 401:      count(ga_session_id) total_data, 
 402:      count(
 403:        case when ifnull(
 404:          cast(hour_gmt7 as string), 
 405:          ''
 406:        )<> '' then 'pass' end
 407:      ) as good_data, 
 408:      count(
 409:        case when ifnull(
 410:          cast(hour_gmt7 as string), 
 411:          ''
 412:        )= '' then 'fail' end
 413:      ) as bad_data 
 414:    from 
 415:      x
 416:  ) result10 
 417:union all 
 418:select 
 419:  datetime(
 420:    current_timestamp(), 
 421:    "Asia/Jakarta"
 422:  ) as time_execution, 
 423:  result11.criteria, 
 424:  result11.metrics, 
 425:  result11.total_data, 
 426:  result11.good_data, 
 427:  result11.bad_data, 
 428:  cast(
 429:    cast(
 430:      result11.good_data * 100 as decimal
 431:    )/ result11.total_data as numeric
 432:  ) as percentage_good_data, 
 433:  cast(
 434:    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
 435:  ) as percentage_bad_data, 
 436:  "device_category_cleaned" field_name_checking 
 437:from 
 438:  (
 439:    select 
 440:      'Completeness' criteria, 
 441:      'field mandatory is null/blank device_category_cleaned' metrics, 
 442:      count(ga_session_id) total_data, 
 443:      count(
 444:        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
 445:      ) as good_data, 
 446:      count(
 447:        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
 448:      ) as bad_data 
 449:    from 
 450:      x
 451:  ) result11 
 452:union all 
 453:select 
 454:  datetime(
 455:    current_timestamp(), 
 456:    "Asia/Jakarta"
 457:  ) as time_execution, 
 458:  result12.criteria, 
 459:  result12.metrics, 
 460:  result12.total_data, 
 461:  result12.good_data, 
 462:  result12.bad_data, 
 463:  cast(
 464:    cast(
 465:      result12.good_data * 100 as decimal
 466:    )/ result12.total_data as numeric
 467:  ) as percentage_good_data, 
 468:  cast(
 469:    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
 470:  ) as percentage_bad_data, 
 471:  "engagement_time_msec" field_name_checking 
 472:from 
 473:  (
 474:    select 
 475:      'Completeness' criteria, 
 476:      'field mandatory is null/blank engagement_time_msec' metrics, 
 477:      count(ga_session_id) total_data, 
 478:      count(
 479:        case when ifnull(
 480:          cast(engagement_time_msec as string), 
 481:          ''
 482:        )<> '' then 'pass' end
 483:      ) as good_data, 
 484:      count(
 485:        case when ifnull(
 486:          cast(engagement_time_msec as string), 
 487:          ''
 488:        )= '' then 'fail' end
 489:      ) as bad_data 
 490:    from 
 491:      x
 492:  ) result12 
 493:union all 
 494:select 
 495:  datetime(
 496:    current_timestamp(), 
 497:    "Asia/Jakarta"
 498:  ) as time_execution, 
 499:  result13.criteria, 
 500:  result13.metrics, 
 501:  result13.total_data, 
 502:  result13.good_data, 
 503:  result13.bad_data, 
 504:  cast(
 505:    cast(
 506:      result13.good_data * 100 as decimal
 507:    )/ result13.total_data as numeric
 508:  ) as percentage_good_data, 
 509:  cast(
 510:    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
 511:  ) as percentage_bad_data, 
 512:  "material_id" field_name_checking 
 513:from 
 514:  (
 515:    select 
 516:      'Completeness' criteria, 
 517:      'field mandatory is null/blank material_id' metrics, 
 518:      count(ga_session_id) total_data, 
 519:      count(
 520:        case when ifnull(
 521:          cast(material_id as string), 
 522:          ''
 523:        )<> '' then 'pass' end
 524:      ) as good_data, 
 525:      count(
 526:        case when ifnull(
 527:          cast(material_id as string), 
 528:          ''
 529:        )= '' then 'fail' end
 530:      ) as bad_data 
 531:    from 
 532:      x
 533:    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
 534:  ) result13 
 535:union all 
 536:select 
 537:  datetime(
 538:    current_timestamp(), 
 539:    "Asia/Jakarta"
 540:  ) as time_execution, 
 541:  result14.criteria, 
 542:  result14.metrics, 
 543:  result14.total_data, 
 544:  result14.good_data, 
 545:  result14.bad_data, 
 546:  cast(
 547:    cast(
 548:      result14.good_data * 100 as decimal
 549:    )/ result14.total_data as numeric
 550:  ) as percentage_good_data, 
 551:  cast(
 552:    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
 553:  ) as percentage_bad_data, 
 554:  "module_id" field_name_checking 
 555:from 
 556:  (
 557:    select 
 558:      'Completeness' criteria, 
 559:      'field mandatory is null/blank module_id' metrics, 
 560:      count(ga_session_id) total_data, 
 561:      count(
 562:        case when ifnull(
 563:          cast(module_id as string), 
 564:          ''
 565:        )<> '' then 'pass' end
 566:      ) as good_data, 
 567:      count(
 568:        case when ifnull(
 569:          cast(module_id as string), 
 570:          ''
 571:        )= '' then 'fail' end
 572:      ) as bad_data 
 573:    from 
 574:      x
 575:    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
 576:  ) result14 
 577:union all 
 578:select 
 579:  datetime(
 580:    current_timestamp(), 
 581:    "Asia/Jakarta"
 582:  ) as time_execution, 
 583:  result15.criteria, 
 584:  result15.metrics, 
 585:  result15.total_data, 
 586:  result15.good_data, 
 587:  result15.bad_data, 
 588:  cast(
 589:    cast(
 590:      result15.good_data * 100 as decimal
 591:    )/ result15.total_data as numeric
 592:  ) as percentage_good_data, 
 593:  cast(
 594:    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
 595:  ) as percentage_bad_data, 
 596:  "video_id" field_name_checking 
 597:from 
 598:  (
 599:    select 
 600:      'Completeness' criteria, 
 601:      'field mandatory is null/blank video_id' metrics, 
 602:      count(ga_session_id) total_data, 
 603:      count(
 604:        case when ifnull(
 605:          cast(video_id as string), 
 606:          ''
 607:        )<> '' then 'pass' end
 608:      ) as good_data, 
 609:      count(
 610:        case when ifnull(
 611:          cast(video_id as string), 
 612:          ''
 613:        )= '' then 'fail' end
 614:      ) as bad_data 
 615:    from 
 616:      x
 617:    where action_type_cleaned = "play_video"  
 618:  ) result15 
 619:union all 
 620:select 
 621:  datetime(
 622:    current_timestamp(), 
 623:    "Asia/Jakarta"
 624:  ) as time_execution, 
 625:  result16.criteria, 
 626:  result16.metrics, 
 627:  result16.total_data, 
 628:  result16.good_data, 
 629:  result16.bad_data, 
 630:  cast(
 631:    cast(
 632:      result16.good_data * 100 as decimal
 633:    )/ result16.total_data as numeric
 634:  ) as percentage_good_data, 
 635:  cast(
 636:    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
 637:  ) as percentage_bad_data, 
 638:  "video_inspirasi_id" field_name_checking 
 639:from 
 640:  (
 641:    select 
 642:      'Completeness' criteria, 
 643:      'field mandatory is null/blank video_inspirasi_id' metrics, 
 644:      count(ga_session_id) total_data, 
 645:      count(
 646:        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
 647:      ) as good_data, 
 648:      count(
 649:        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
 650:      ) as bad_data 
 651:    from 
 652:      x
 653:    where action_type_cleaned = "play_video_inspirasi"  
 654:  ) result16 
 655:union all 
 656:select 
 657:  datetime(
 658:    current_timestamp(), 
 659:    "Asia/Jakarta"
 660:  ) as time_execution, 
 661:  result17.criteria, 
 662:  result17.metrics, 
 663:  result17.total_data, 
 664:  result17.good_data, 
 665:  result17.bad_data, 
 666:  cast(
 667:    cast(
 668:      result17.good_data * 100 as decimal
 669:    )/ result17.total_data as numeric
 670:  ) as percentage_good_data, 
 671:  cast(
 672:    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
 673:  ) as percentage_bad_data, 
 674:  "video_inspirasi_playslist_id" field_name_checking 
 675:from 
 676:  (
 677:    select 
 678:      'Completeness' criteria, 
 679:      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
 680:      count(ga_session_id) total_data, 
 681:      count(
 682:        case when ifnull(
 683:          cast(
 684:            video_inspirasi_playslist_id as string
 685:          ), 
 686:          ''
 687:        )<> '' then 'pass' end
 688:      ) as good_data, 
 689:      count(
 690:        case when ifnull(
 691:          cast(
 692:            video_inspirasi_playslist_id as string
 693:          ), 
 694:          ''
 695:        )= '' then 'fail' end
 696:      ) as bad_data 
 697:    from 
 698:      x
 699:    where action_type_cleaned = "play_video_inspirasi"  
 700:  ) result17 
 701:union all 
 702:select 
 703:  datetime(
 704:    current_timestamp(), 
 705:    "Asia/Jakarta"
 706:  ) as time_execution, 
 707:  result18.criteria, 
 708:  result18.metrics, 
 709:  result18.total_data, 
 710:  result18.good_data, 
 711:  result18.bad_data, 
 712:  cast(
 713:    cast(
 714:      result18.good_data * 100 as decimal
 715:    )/ result18.total_data as numeric
 716:  ) as percentage_good_data, 
 717:  cast(
 718:    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
 719:  ) as percentage_bad_data, 
 720:  "quiz_id" field_name_checking 
 721:from 
 722:  (
 723:    select 
 724:      'Completeness' criteria, 
 725:      'field mandatory is null/blank quiz_id' metrics, 
 726:      count(ga_session_id) total_data, 
 727:      count(
 728:        case when ifnull(
 729:          cast(quiz_id as string), 
 730:          ''
 731:        )<> '' then 'pass' end
 732:      ) as good_data, 
 733:      count(
 734:        case when ifnull(
 735:          cast(quiz_id as string), 
 736:          ''
 737:        )= '' then 'fail' end
 738:      ) as bad_data 
 739:    from 
 740:      x
 741:    where action_type_cleaned = 'open_quiz'  
 742:  ) result18 
 743:union all 
 744:select 
 745:  datetime(
 746:    current_timestamp(), 
 747:    "Asia/Jakarta"
 748:  ) as time_execution, 
 749:  result19.criteria, 
 750:  result19.metrics, 
 751:  result19.total_data, 
 752:  result19.good_data, 
 753:  result19.bad_data, 
 754:  cast(
 755:    cast(
 756:      result19.good_data * 100 as decimal
 757:    )/ result19.total_data as numeric
 758:  ) as percentage_good_data, 
 759:  cast(
 760:    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
 761:  ) as percentage_bad_data, 
 762:  "text_id" field_name_checking 
 763:from 
 764:  (
 765:    select 
 766:      'Completeness' criteria, 
 767:      'field mandatory is null/blank text_id' metrics, 
 768:      count(ga_session_id) total_data, 
 769:      count(
 770:        case when ifnull(
 771:          cast(text_id as string), 
 772:          ''
 773:        )<> '' then 'pass' end
 774:      ) as good_data, 
 775:      count(
 776:        case when ifnull(
 777:          cast(text_id as string), 
 778:          ''
 779:        )= '' then 'fail' end
 780:      ) as bad_data 
 781:    from 
 782:      x
 783:    where action_type_cleaned = 'open_text'  
 784:  ) result19 
 785:union all 
 786:select 
 787:  datetime(
 788:    current_timestamp(), 
 789:    "Asia/Jakarta"
 790:  ) as time_execution, 
 791:  result20.criteria, 
 792:  result20.metrics, 
 793:  result20.total_data, 
 794:  result20.good_data, 
 795:  result20.bad_data, 
 796:  cast(
 797:    cast(
 798:      result20.good_data * 100 as decimal
 799:    )/ result20.total_data as numeric
 800:  ) as percentage_good_data, 
 801:  cast(
 802:    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
 803:  ) as percentage_bad_data, 
 804:  "open_reflection_id" field_name_checking 
 805:from 
 806:  (
 807:    select 
 808:      'Completeness' criteria, 
 809:      'field mandatory is null/blank open_reflection_id' metrics, 
 810:      count(ga_session_id) total_data, 
 811:      count(
 812:        case when ifnull(
 813:          cast(open_reflection_id as string), 
 814:          ''
 815:        )<> '' then 'pass' end
 816:      ) as good_data, 
 817:      count(
 818:        case when ifnull(
 819:          cast(open_reflection_id as string), 
 820:          ''
 821:        )= '' then 'fail' end
 822:      ) as bad_data 
 823:    from 
 824:      x
 825:    where action_type_cleaned = "open_reflection"  
 826:  ) result20 
 827:union all 
 828:select 
 829:  datetime(
 830:    current_timestamp(), 
 831:    "Asia/Jakarta"
 832:  ) as time_execution, 
 833:  result21.criteria, 
 834:  result21.metrics, 
 835:  result21.total_data, 
 836:  result21.good_data, 
 837:  result21.bad_data, 
 838:  cast(
 839:    cast(
 840:      result21.good_data * 100 as decimal
 841:    )/ result21.total_data as numeric
 842:  ) as percentage_good_data, 
 843:  cast(
 844:    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
 845:  ) as percentage_bad_data, 
 846:  "save_reflection_id" field_name_checking 
 847:from 
 848:  (
 849:    select 
 850:      'Completeness' criteria, 
 851:      'field mandatory is null/blank save_reflection_id' metrics, 
 852:      count(ga_session_id) total_data, 
 853:      count(
 854:        case when ifnull(
 855:          cast(save_reflection_id as string), 
 856:          ''
 857:        )<> '' then 'pass' end
 858:      ) as good_data, 
 859:      count(
 860:        case when ifnull(
 861:          cast(save_reflection_id as string), 
 862:          ''
 863:        )= '' then 'fail' end
 864:      ) as bad_data 
 865:    from 
 866:      x
 867:    where action_type_cleaned = 'save_reflection'  
 868:  ) result21 
 869:union all 
 870:select 
 871:  datetime(
 872:    current_timestamp(), 
 873:    "Asia/Jakarta"
 874:  ) as time_execution, 
 875:  result22.criteria, 
 876:  result22.metrics, 
 877:  result22.total_data, 
 878:  result22.good_data, 
 879:  result22.bad_data, 
 880:  cast(
 881:    cast(
 882:      result22.good_data * 100 as decimal
 883:    )/ result22.total_data as numeric
 884:  ) as percentage_good_data, 
 885:  cast(
 886:    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
 887:  ) as percentage_bad_data, 
 888:  "post_test_result" field_name_checking 
 889:from 
 890:  (
 891:    select 
 892:      'Completeness' criteria, 
 893:      'field mandatory is null/blank post_test_result' metrics, 
 894:      count(ga_session_id) total_data, 
 895:      count(
 896:        case when ifnull(post_test_result, '')<> '' then 'pass' end
 897:      ) as good_data, 
 898:      count(
 899:        case when ifnull(post_test_result, '')= '' then 'fail' end
 900:      ) as bad_data 
 901:    from 
 902:      x
 903:    where action_type_cleaned = 'finished_post_test'  
 904:  ) result22 
 905:union all 
 906:select 
 907:  datetime(
 908:    current_timestamp(), 
 909:    "Asia/Jakarta"
 910:  ) as time_execution, 
 911:  result23.criteria, 
 912:  result23.metrics, 
 913:  result23.total_data, 
 914:  result23.good_data, 
 915:  result23.bad_data, 
 916:  cast(
 917:    cast(
 918:      result23.good_data * 100 as decimal
 919:    )/ result23.total_data as numeric
 920:  ) as percentage_good_data, 
 921:  cast(
 922:    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
 923:  ) as percentage_bad_data, 
 924:  "event_params_source" field_name_checking 
 925:from 
 926:  (
 927:    select 
 928:      'Completeness' criteria, 
 929:      'field mandatory is null/blank event_params_source' metrics, 
 930:      count(ga_session_id) total_data, 
 931:      count(
 932:        case when ifnull(event_params_source, '')<> '' then 'pass' end
 933:      ) as good_data, 
 934:      count(
 935:        case when ifnull(event_params_source, '')= '' then 'fail' end
 936:      ) as bad_data 
 937:    from 
 938:      x
 939:  ) result23 
 940:union all 
 941:select 
 942:  datetime(
 943:    current_timestamp(), 
 944:    "Asia/Jakarta"
 945:  ) as time_execution, 
 946:  result24.criteria, 
 947:  result24.metrics, 
 948:  result24.total_data, 
 949:  result24.good_data, 
 950:  result24.bad_data, 
 951:  cast(
 952:    cast(
 953:      result24.good_data * 100 as decimal
 954:    )/ result24.total_data as numeric
 955:  ) as percentage_good_data, 
 956:  cast(
 957:    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
 958:  ) as percentage_bad_data, 
 959:  "event_params_page_location" field_name_checking 
 960:from 
 961:  (
 962:    select 
 963:      'Completeness' criteria, 
 964:      'field mandatory is null/blank event_params_page_location' metrics, 
 965:      count(ga_session_id) total_data, 
 966:      count(
 967:        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
 968:      ) as good_data, 
 969:      count(
 970:        case when ifnull(event_params_page_location, '')= '' then 'fail' end
 971:      ) as bad_data 
 972:    from 
 973:      x
 974:  ) result24 
 975:union all 
 976:select 
 977:  datetime(
 978:    current_timestamp(), 
 979:    "Asia/Jakarta"
 980:  ) as time_execution, 
 981:  result25.criteria, 
 982:  result25.metrics, 
 983:  result25.total_data, 
 984:  result25.good_data, 
 985:  result25.bad_data, 
 986:  cast(
 987:    cast(
 988:      result25.good_data * 100 as decimal
 989:    )/ result25.total_data as numeric
 990:  ) as percentage_good_data, 
 991:  cast(
 992:    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
 993:  ) as percentage_bad_data, 
 994:  "event_params_page_referrer" field_name_checking 
 995:from 
 996:  (
 997:    select 
 998:      'Completeness' criteria, 
 999:      'field mandatory is null/blank event_params_page_referrer' metrics, 
1000:      count(ga_session_id) total_data, 
1001:      count(
1002:        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
1003:      ) as good_data, 
1004:      count(
1005:        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
1006:      ) as bad_data 
1007:    from 
1008:      x
1009:  ) result25 
1010:union all 
1011:select 
1012:  datetime(
1013:    current_timestamp(), 
1014:    "Asia/Jakarta"
1015:  ) as time_execution, 
1016:  result26.criteria, 
1017:  result26.metrics, 
1018:  result26.total_data, 
1019:  result26.good_data, 
1020:  result26.bad_data, 
1021:  cast(
1022:    cast(
1023:      result26.good_data * 100 as decimal
1024:    )/ result26.total_data as numeric
1025:  ) as percentage_good_data, 
1026:  cast(
1027:    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
1028:  ) as percentage_bad_data, 
1029:  "event_params_page_path" field_name_checking 
1030:from 
1031:  (
1032:    select 
1033:      'Completeness' criteria, 
1034:      'field mandatory is null/blank event_params_page_path' metrics, 
1035:      count(ga_session_id) total_data, 
1036:      count(
1037:        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
1038:      ) as good_data, 
1039:      count(
1040:        case when ifnull(event_params_page_path, '')= '' then 'fail' end
1041:      ) as bad_data 
1042:    from 
1043:      x
1044:  ) result26 
1045:union all 
1046:select 
1047:  datetime(
1048:    current_timestamp(), 
1049:    "Asia/Jakarta"
1050:  ) as time_execution, 
1051:  result27.criteria, 
1052:  result27.metrics, 
1053:  result27.total_data, 
1054:  result27.good_data, 
1055:  result27.bad_data, 
1056:  cast(
1057:    cast(
1058:      result27.good_data * 100 as decimal
1059:    )/ result27.total_data as numeric
1060:  ) as percentage_good_data, 
1061:  cast(
1062:    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
1063:  ) as percentage_bad_data, 
1064:  "event_params_page_title" field_name_checking 
1065:from 
1066:  (
1067:    select 
1068:      'Completeness' criteria, 
1069:      'field mandatory is null/blank event_params_page_title' metrics, 
1070:      count(ga_session_id) total_data, 
1071:      count(
1072:        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
1073:      ) as good_data, 
1074:      count(
1075:        case when ifnull(event_params_page_title, '')= '' then 'fail' end
1076:      ) as bad_data 
1077:    from 
1078:      x
1079:  ) result27 
1080:union all 
1081:select 
1082:  datetime(
1083:    current_timestamp(), 
1084:    "Asia/Jakarta"
1085:  ) as time_execution, 
1086:  result28.criteria, 
1087:  result28.metrics, 
1088:  result28.total_data, 
1089:  result28.good_data, 
1090:  result28.bad_data, 
1091:  cast(
1092:    cast(
1093:      result28.good_data * 100 as decimal
1094:    )/ result28.total_data as numeric
1095:  ) as percentage_good_data, 
1096:  cast(
1097:    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
1098:  ) as percentage_bad_data, 
1099:  "event_params_product" field_name_checking 
1100:from 
1101:  (
1102:    select 
1103:      'Completeness' criteria, 
1104:      'field mandatory is null/blank event_params_product' metrics, 
1105:      count(ga_session_id) total_data, 
1106:      count(
1107:        case when ifnull(event_params_product, '')<> '' then 'pass' end
1108:      ) as good_data, 
1109:      count(
1110:        case when ifnull(event_params_product, '')= '' then 'fail' end
1111:      ) as bad_data 
1112:    from 
1113:      x
1114:  ) result28
1115:  );
1116:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 04:03:17.939695 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '53551bc3-fbbd-4a4c-99a2-e857d467a787', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae045b7040>]}
2021-12-24 04:03:17.940605 (Thread-1): 11:03:17 | 1 of 3 ERROR creating table model dataset_dbt.dqa_completeness....... [ERROR in 5.80s]
2021-12-24 04:03:17.941047 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 04:03:17.941565 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-24 04:03:17.942316 (Thread-1): 11:03:17 | 2 of 3 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-24 04:03:17.943375 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-24 04:03:17.943865 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-24 04:03:17.948147 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-24 04:03:17.948562 (Thread-1): finished collecting timing info
2021-12-24 04:03:17.949986 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:03:17.953649 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:03:18.316866 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-24 04:03:18.317750 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 04:03:22.132809 (Thread-1): finished collecting timing info
2021-12-24 04:03:22.133096 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '53551bc3-fbbd-4a4c-99a2-e857d467a787', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae04616fd0>]}
2021-12-24 04:03:22.133339 (Thread-1): 11:03:22 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 4.19s]
2021-12-24 04:03:22.133467 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-24 04:03:22.134064 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-24 04:03:22.134232 (Thread-1): 11:03:22 | 3 of 3 SKIP relation dataset_dbt.dqa_uniqueness_2.................... [SKIP]
2021-12-24 04:03:22.134411 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-24 04:03:22.135114 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:03:22.135395 (MainThread): 11:03:22 | 
2021-12-24 04:03:22.135574 (MainThread): 11:03:22 | Finished running 2 table models, 1 view model in 10.87s.
2021-12-24 04:03:22.135731 (MainThread): Connection 'master' was properly closed.
2021-12-24 04:03:22.135877 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_1' was properly closed.
2021-12-24 04:03:22.142360 (MainThread): 
2021-12-24 04:03:22.142532 (MainThread): Completed with 1 error and 0 warnings:
2021-12-24 04:03:22.142641 (MainThread): 
2021-12-24 04:03:22.142745 (MainThread): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
2021-12-24 04:03:22.142855 (MainThread):   No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
2021-12-24 04:03:22.142969 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 04:03:22.143118 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
2021-12-24 04:03:22.143386 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae07ea0af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae04744070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae045d8dc0>]}
2021-12-24 04:03:22.143638 (MainThread): Flushing usage events
2021-12-24 04:08:25.081322 (MainThread): Running with dbt=0.21.0
2021-12-24 04:08:25.230953 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 04:08:25.231508 (MainThread): Tracking: tracking
2021-12-24 04:08:25.237121 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10888637c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1086afc670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1086afc6a0>]}
2021-12-24 04:08:25.243409 (MainThread): Partial parsing not enabled
2021-12-24 04:08:25.247499 (MainThread): Parsing macros/catalog.sql
2021-12-24 04:08:25.251527 (MainThread): Parsing macros/etc.sql
2021-12-24 04:08:25.252796 (MainThread): Parsing macros/adapters.sql
2021-12-24 04:08:25.265556 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 04:08:25.267465 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 04:08:25.269162 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 04:08:25.270763 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 04:08:25.271899 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 04:08:25.277387 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 04:08:25.286034 (MainThread): Parsing macros/core.sql
2021-12-24 04:08:25.288474 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 04:08:25.289611 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 04:08:25.290937 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 04:08:25.291771 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 04:08:25.292632 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 04:08:25.296848 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 04:08:25.302435 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 04:08:25.315648 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 04:08:25.319837 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 04:08:25.322141 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 04:08:25.333686 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 04:08:25.334795 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 04:08:25.340900 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 04:08:25.348916 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 04:08:25.353178 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 04:08:25.362960 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 04:08:25.364021 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 04:08:25.381909 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 04:08:25.382903 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 04:08:25.384027 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 04:08:25.384956 (MainThread): Parsing macros/etc/query.sql
2021-12-24 04:08:25.385575 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 04:08:25.387018 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 04:08:25.388196 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 04:08:25.393297 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 04:08:25.538329 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-24 04:08:25.546091 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:08:25.550103 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-24 04:08:25.565438 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:08:25.566637 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:08:25.567715 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:08:25.568794 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:08:25.577434 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d9b1a4f-d5b5-42c5-8a3d-14bf4895676e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10869890d0>]}
2021-12-24 04:08:25.580350 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 04:08:25.580679 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d9b1a4f-d5b5-42c5-8a3d-14bf4895676e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1086989190>]}
2021-12-24 04:08:25.580969 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 04:08:25.582070 (MainThread): 
2021-12-24 04:08:25.582338 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:08:25.583089 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 04:08:25.583299 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 04:08:26.311695 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 04:08:26.312440 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 04:08:26.328359 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:08:26.695338 (MainThread): 11:08:26 | Concurrency: 1 threads (target='prod')
2021-12-24 04:08:26.695920 (MainThread): 11:08:26 | 
2021-12-24 04:08:26.700485 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 04:08:26.701273 (Thread-1): 11:08:26 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 04:08:26.702106 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:08:26.702685 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 04:08:26.713646 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:08:26.714430 (Thread-1): finished collecting timing info
2021-12-24 04:08:26.718625 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55026), raddr=('172.217.194.95', 443)>
2021-12-24 04:08:26.718958 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 42988), raddr=('34.101.5.74', 443)>
2021-12-24 04:08:26.719159 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55030), raddr=('172.217.194.95', 443)>
2021-12-24 04:08:26.719338 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 42992), raddr=('34.101.5.74', 443)>
2021-12-24 04:08:26.740062 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:08:26.740428 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:08:26.743853 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(time_gmt7, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(time_gmt7, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result21.criteria, 
  result21.metrics, 
  result21.total_data, 
  result21.good_data, 
  result21.bad_data, 
  cast(
    cast(
      result21.good_data * 100 as decimal
    )/ result21.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
  ) as percentage_bad_data, 
  "save_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank save_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'save_reflection'  
  ) result21 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result22.criteria, 
  result22.metrics, 
  result22.total_data, 
  result22.good_data, 
  result22.bad_data, 
  cast(
    cast(
      result22.good_data * 100 as decimal
    )/ result22.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
  ) as percentage_bad_data, 
  "post_test_result" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank post_test_result' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(post_test_result, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(post_test_result, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'finished_post_test'  
  ) result22 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result23.criteria, 
  result23.metrics, 
  result23.total_data, 
  result23.good_data, 
  result23.bad_data, 
  cast(
    cast(
      result23.good_data * 100 as decimal
    )/ result23.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_source" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_source' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_source, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_source, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result23 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result24.criteria, 
  result24.metrics, 
  result24.total_data, 
  result24.good_data, 
  result24.bad_data, 
  cast(
    cast(
      result24.good_data * 100 as decimal
    )/ result24.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_location" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_location' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_location, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result24 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result25.criteria, 
  result25.metrics, 
  result25.total_data, 
  result25.good_data, 
  result25.bad_data, 
  cast(
    cast(
      result25.good_data * 100 as decimal
    )/ result25.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_referrer" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_referrer' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result25 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result26.criteria, 
  result26.metrics, 
  result26.total_data, 
  result26.good_data, 
  result26.bad_data, 
  cast(
    cast(
      result26.good_data * 100 as decimal
    )/ result26.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_path" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_path' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_path, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result26 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result27.criteria, 
  result27.metrics, 
  result27.total_data, 
  result27.good_data, 
  result27.bad_data, 
  cast(
    cast(
      result27.good_data * 100 as decimal
    )/ result27.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_title" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_title' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_title, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result27 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result28.criteria, 
  result28.metrics, 
  result28.total_data, 
  result28.good_data, 
  result28.bad_data, 
  cast(
    cast(
      result28.good_data * 100 as decimal
    )/ result28.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_product" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_product' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_product, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_product, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result28
  );
    
2021-12-24 04:08:27.822477 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]')
2021-12-24 04:08:29.907639 (Thread-1): finished collecting timing info
2021-12-24 04:08:29.908796 (Thread-1): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]

(job ID: 38cbc467-8f9b-434c-9950-2b0405a8671c)

                                                                               -----Query Job SQL Follows-----                                                                                

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    /*
  10:    Welcome to your first dbt model!
  11:    Did you know that you can also configure models directly within SQL files?
  12:    This will override configurations stated in dbt_project.yml
  13:
  14:    Try changing "table" to "view" below
  15:*/
  16:
  17:
  18:
  19:with x as (
  20:  select 
  21:    ga_session_id, 
  22:    user_id, 
  23:    user_pseudo_id, 
  24:    event_name, 
  25:    action_type_cleaned, 
  26:    user_first_timestamp_gmt7, 
  27:    timestamp_gmt7, 
  28:    date_gmt7, 
  29:    time_gmt7, 
  30:    hour_gmt7, 
  31:    device_category_cleaned, 
  32:    engagement_time_msec, 
  33:    material_id, 
  34:    module_id, 
  35:    video_id, 
  36:    video_inspirasi_id, 
  37:    video_inspirasi_playslist_id, 
  38:    quiz_id, 
  39:    text_id, 
  40:    open_reflection_id, 
  41:    save_reflection_id, 
  42:    post_test_result, 
  43:    event_params_source, 
  44:    event_params_page_location, 
  45:    event_params_page_referrer, 
  46:    event_params_page_path, 
  47:    event_params_page_title, 
  48:    event_params_product 
  49:  FROM 
  50:    `playground-325606.dataset_dbt.event_tracker_belajar`
  51:) 
  52:select 
  53:  datetime(
  54:    current_timestamp(), 
  55:    "Asia/Jakarta"
  56:  ) as time_execution, 
  57:  result1.criteria, 
  58:  result1.metrics, 
  59:  result1.total_data, 
  60:  result1.good_data, 
  61:  result1.bad_data, 
  62:  cast(
  63:    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  64:  ) as percentage_good_data, 
  65:  cast(
  66:    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  67:  ) as percentage_bad_data, 
  68:  "ga_session_id" field_name_checking 
  69:from 
  70:  (
  71:    select 
  72:      'Completeness' criteria, 
  73:      'field mandatory is null/blank ga_session_id' metrics, 
  74:      count(ga_session_id) total_data, 
  75:      count(
  76:        case when ifnull(
  77:          cast(ga_session_id as string), 
  78:          ''
  79:        )<> '' then 'pass' end
  80:      ) as good_data, 
  81:      count(
  82:        case when ifnull(
  83:          cast(ga_session_id as string), 
  84:          ''
  85:        )= '' then 'fail' end
  86:      ) as bad_data 
  87:    from 
  88:      x
  89:  ) result1 
  90:union all 
  91:select 
  92:  datetime(
  93:    current_timestamp(), 
  94:    "Asia/Jakarta"
  95:  ) as time_execution, 
  96:  result2.criteria, 
  97:  result2.metrics, 
  98:  result2.total_data, 
  99:  result2.good_data, 
 100:  result2.bad_data, 
 101:  cast(
 102:    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
 103:  ) as percentage_good_data, 
 104:  cast(
 105:    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
 106:  ) as percentage_bad_data, 
 107:  "user_id" field_name_checking 
 108:from 
 109:  (
 110:    select 
 111:      'Completeness' criteria, 
 112:      'field mandatory is null/blank user_id' metrics, 
 113:      count(ga_session_id) total_data, 
 114:      count(
 115:        case when ifnull(user_id, '')<> '' then 'pass' end
 116:      ) as good_data, 
 117:      count(
 118:        case when ifnull(user_id, '')= '' then 'fail' end
 119:      ) as bad_data 
 120:    from 
 121:      x
 122:  ) result2 
 123:union all 
 124:select 
 125:  datetime(
 126:    current_timestamp(), 
 127:    "Asia/Jakarta"
 128:  ) as time_execution, 
 129:  result3.criteria, 
 130:  result3.metrics, 
 131:  result3.total_data, 
 132:  result3.good_data, 
 133:  result3.bad_data, 
 134:  cast(
 135:    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
 136:  ) as percentage_good_data, 
 137:  cast(
 138:    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
 139:  ) as percentage_bad_data, 
 140:  "user_pseudo_id" field_name_checking 
 141:from 
 142:  (
 143:    select 
 144:      'Completeness' criteria, 
 145:      'field mandatory is null/blank user_pseudo_id' metrics, 
 146:      count(ga_session_id) total_data, 
 147:      count(
 148:        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
 149:      ) as good_data, 
 150:      count(
 151:        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
 152:      ) as bad_data 
 153:    from 
 154:      x
 155:  ) result3 
 156:union all 
 157:select 
 158:  datetime(
 159:    current_timestamp(), 
 160:    "Asia/Jakarta"
 161:  ) as time_execution, 
 162:  result4.criteria, 
 163:  result4.metrics, 
 164:  result4.total_data, 
 165:  result4.good_data, 
 166:  result4.bad_data, 
 167:  cast(
 168:    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
 169:  ) as percentage_good_data, 
 170:  cast(
 171:    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
 172:  ) as percentage_bad_data, 
 173:  "event_name" field_name_checking 
 174:from 
 175:  (
 176:    select 
 177:      'Completeness' criteria, 
 178:      'field mandatory is null/blank event_name' metrics, 
 179:      count(ga_session_id) total_data, 
 180:      count(
 181:        case when ifnull(event_name, '')<> '' then 'pass' end
 182:      ) as good_data, 
 183:      count(
 184:        case when ifnull(event_name, '')= '' then 'fail' end
 185:      ) as bad_data 
 186:    from 
 187:      x
 188:  ) result4 
 189:union all 
 190:select 
 191:  datetime(
 192:    current_timestamp(), 
 193:    "Asia/Jakarta"
 194:  ) as time_execution, 
 195:  result5.criteria, 
 196:  result5.metrics, 
 197:  result5.total_data, 
 198:  result5.good_data, 
 199:  result5.bad_data, 
 200:  cast(
 201:    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
 202:  ) as percentage_good_data, 
 203:  cast(
 204:    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
 205:  ) as percentage_bad_data, 
 206:  "action_type_cleaned" field_name_checking 
 207:from 
 208:  (
 209:    select 
 210:      'Completeness' criteria, 
 211:      'field mandatory is null/blank action_type_cleaned' metrics, 
 212:      count(ga_session_id) total_data, 
 213:      count(
 214:        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
 215:      ) as good_data, 
 216:      count(
 217:        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
 218:      ) as bad_data 
 219:    from 
 220:      x
 221:  ) result5 
 222:union all 
 223:select 
 224:  datetime(
 225:    current_timestamp(), 
 226:    "Asia/Jakarta"
 227:  ) as time_execution, 
 228:  result6.criteria, 
 229:  result6.metrics, 
 230:  result6.total_data, 
 231:  result6.good_data, 
 232:  result6.bad_data, 
 233:  cast(
 234:    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
 235:  ) as percentage_good_data, 
 236:  cast(
 237:    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
 238:  ) as percentage_bad_data, 
 239:  "user_first_timestamp_gmt7" field_name_checking 
 240:from 
 241:  (
 242:    select 
 243:      'Completeness' criteria, 
 244:      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
 245:      count(ga_session_id) total_data, 
 246:      count(
 247:        case when ifnull(
 248:          cast(
 249:            user_first_timestamp_gmt7 as string
 250:          ), 
 251:          ''
 252:        )<> '' then 'pass' end
 253:      ) as good_data, 
 254:      count(
 255:        case when ifnull(
 256:          cast(
 257:            user_first_timestamp_gmt7 as string
 258:          ), 
 259:          ''
 260:        )= '' then 'fail' end
 261:      ) as bad_data 
 262:    from 
 263:      x
 264:  ) result6 
 265:union all 
 266:select 
 267:  datetime(
 268:    current_timestamp(), 
 269:    "Asia/Jakarta"
 270:  ) as time_execution, 
 271:  result7.criteria, 
 272:  result7.metrics, 
 273:  result7.total_data, 
 274:  result7.good_data, 
 275:  result7.bad_data, 
 276:  cast(
 277:    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
 278:  ) as percentage_good_data, 
 279:  cast(
 280:    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
 281:  ) as percentage_bad_data, 
 282:  "timestamp_gmt7" field_name_checking 
 283:from 
 284:  (
 285:    select 
 286:      'Completeness' criteria, 
 287:      'field mandatory is null/blank timestamp_gmt7' metrics, 
 288:      count(ga_session_id) total_data, 
 289:      count(
 290:        case when ifnull(
 291:          cast(timestamp_gmt7 as string), 
 292:          ''
 293:        )<> '' then 'pass' end
 294:      ) as good_data, 
 295:      count(
 296:        case when ifnull(
 297:          cast(timestamp_gmt7 as string), 
 298:          ''
 299:        )= '' then 'fail' end
 300:      ) as bad_data 
 301:    from 
 302:      x
 303:  ) result7 
 304:union all 
 305:select 
 306:  datetime(
 307:    current_timestamp(), 
 308:    "Asia/Jakarta"
 309:  ) as time_execution, 
 310:  result8.criteria, 
 311:  result8.metrics, 
 312:  result8.total_data, 
 313:  result8.good_data, 
 314:  result8.bad_data, 
 315:  cast(
 316:    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
 317:  ) as percentage_good_data, 
 318:  cast(
 319:    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
 320:  ) as percentage_bad_data, 
 321:  "date_gmt7" field_name_checking 
 322:from 
 323:  (
 324:    select 
 325:      'Completeness' criteria, 
 326:      'field mandatory is null/blank date_gmt7' metrics, 
 327:      count(ga_session_id) total_data, 
 328:      count(
 329:        case when ifnull(
 330:          cast(date_gmt7 as string), 
 331:          ''
 332:        )<> '' then 'pass' end
 333:      ) as good_data, 
 334:      count(
 335:        case when ifnull(
 336:          cast(date_gmt7 as string), 
 337:          ''
 338:        )= '' then 'fail' end
 339:      ) as bad_data 
 340:    from 
 341:      x
 342:  ) result8 
 343:union all 
 344:select 
 345:  datetime(
 346:    current_timestamp(), 
 347:    "Asia/Jakarta"
 348:  ) as time_execution, 
 349:  result9.criteria, 
 350:  result9.metrics, 
 351:  result9.total_data, 
 352:  result9.good_data, 
 353:  result9.bad_data, 
 354:  cast(
 355:    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
 356:  ) as percentage_good_data, 
 357:  cast(
 358:    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
 359:  ) as percentage_bad_data, 
 360:  "time_gmt7" field_name_checking 
 361:from 
 362:  (
 363:    select 
 364:      'Completeness' criteria, 
 365:      'field mandatory is null/blank time_gmt7' metrics, 
 366:      count(ga_session_id) total_data, 
 367:      count(
 368:        case when ifnull(time_gmt7, '')<> '' then 'pass' end
 369:      ) as good_data, 
 370:      count(
 371:        case when ifnull(time_gmt7, '')= '' then 'fail' end
 372:      ) as bad_data 
 373:    from 
 374:      x
 375:  ) result9 
 376:union all 
 377:select 
 378:  datetime(
 379:    current_timestamp(), 
 380:    "Asia/Jakarta"
 381:  ) as time_execution, 
 382:  result10.criteria, 
 383:  result10.metrics, 
 384:  result10.total_data, 
 385:  result10.good_data, 
 386:  result10.bad_data, 
 387:  cast(
 388:    cast(
 389:      result10.good_data * 100 as decimal
 390:    )/ result10.total_data as numeric
 391:  ) as percentage_good_data, 
 392:  cast(
 393:    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
 394:  ) as percentage_bad_data, 
 395:  "hour_gmt7" field_name_checking 
 396:from 
 397:  (
 398:    select 
 399:      'Completeness' criteria, 
 400:      'field mandatory is null/blank hour_gmt7' metrics, 
 401:      count(ga_session_id) total_data, 
 402:      count(
 403:        case when ifnull(
 404:          cast(hour_gmt7 as string), 
 405:          ''
 406:        )<> '' then 'pass' end
 407:      ) as good_data, 
 408:      count(
 409:        case when ifnull(
 410:          cast(hour_gmt7 as string), 
 411:          ''
 412:        )= '' then 'fail' end
 413:      ) as bad_data 
 414:    from 
 415:      x
 416:  ) result10 
 417:union all 
 418:select 
 419:  datetime(
 420:    current_timestamp(), 
 421:    "Asia/Jakarta"
 422:  ) as time_execution, 
 423:  result11.criteria, 
 424:  result11.metrics, 
 425:  result11.total_data, 
 426:  result11.good_data, 
 427:  result11.bad_data, 
 428:  cast(
 429:    cast(
 430:      result11.good_data * 100 as decimal
 431:    )/ result11.total_data as numeric
 432:  ) as percentage_good_data, 
 433:  cast(
 434:    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
 435:  ) as percentage_bad_data, 
 436:  "device_category_cleaned" field_name_checking 
 437:from 
 438:  (
 439:    select 
 440:      'Completeness' criteria, 
 441:      'field mandatory is null/blank device_category_cleaned' metrics, 
 442:      count(ga_session_id) total_data, 
 443:      count(
 444:        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
 445:      ) as good_data, 
 446:      count(
 447:        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
 448:      ) as bad_data 
 449:    from 
 450:      x
 451:  ) result11 
 452:union all 
 453:select 
 454:  datetime(
 455:    current_timestamp(), 
 456:    "Asia/Jakarta"
 457:  ) as time_execution, 
 458:  result12.criteria, 
 459:  result12.metrics, 
 460:  result12.total_data, 
 461:  result12.good_data, 
 462:  result12.bad_data, 
 463:  cast(
 464:    cast(
 465:      result12.good_data * 100 as decimal
 466:    )/ result12.total_data as numeric
 467:  ) as percentage_good_data, 
 468:  cast(
 469:    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
 470:  ) as percentage_bad_data, 
 471:  "engagement_time_msec" field_name_checking 
 472:from 
 473:  (
 474:    select 
 475:      'Completeness' criteria, 
 476:      'field mandatory is null/blank engagement_time_msec' metrics, 
 477:      count(ga_session_id) total_data, 
 478:      count(
 479:        case when ifnull(
 480:          cast(engagement_time_msec as string), 
 481:          ''
 482:        )<> '' then 'pass' end
 483:      ) as good_data, 
 484:      count(
 485:        case when ifnull(
 486:          cast(engagement_time_msec as string), 
 487:          ''
 488:        )= '' then 'fail' end
 489:      ) as bad_data 
 490:    from 
 491:      x
 492:  ) result12 
 493:union all 
 494:select 
 495:  datetime(
 496:    current_timestamp(), 
 497:    "Asia/Jakarta"
 498:  ) as time_execution, 
 499:  result13.criteria, 
 500:  result13.metrics, 
 501:  result13.total_data, 
 502:  result13.good_data, 
 503:  result13.bad_data, 
 504:  cast(
 505:    cast(
 506:      result13.good_data * 100 as decimal
 507:    )/ result13.total_data as numeric
 508:  ) as percentage_good_data, 
 509:  cast(
 510:    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
 511:  ) as percentage_bad_data, 
 512:  "material_id" field_name_checking 
 513:from 
 514:  (
 515:    select 
 516:      'Completeness' criteria, 
 517:      'field mandatory is null/blank material_id' metrics, 
 518:      count(ga_session_id) total_data, 
 519:      count(
 520:        case when ifnull(
 521:          cast(material_id as string), 
 522:          ''
 523:        )<> '' then 'pass' end
 524:      ) as good_data, 
 525:      count(
 526:        case when ifnull(
 527:          cast(material_id as string), 
 528:          ''
 529:        )= '' then 'fail' end
 530:      ) as bad_data 
 531:    from 
 532:      x
 533:    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
 534:  ) result13 
 535:union all 
 536:select 
 537:  datetime(
 538:    current_timestamp(), 
 539:    "Asia/Jakarta"
 540:  ) as time_execution, 
 541:  result14.criteria, 
 542:  result14.metrics, 
 543:  result14.total_data, 
 544:  result14.good_data, 
 545:  result14.bad_data, 
 546:  cast(
 547:    cast(
 548:      result14.good_data * 100 as decimal
 549:    )/ result14.total_data as numeric
 550:  ) as percentage_good_data, 
 551:  cast(
 552:    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
 553:  ) as percentage_bad_data, 
 554:  "module_id" field_name_checking 
 555:from 
 556:  (
 557:    select 
 558:      'Completeness' criteria, 
 559:      'field mandatory is null/blank module_id' metrics, 
 560:      count(ga_session_id) total_data, 
 561:      count(
 562:        case when ifnull(
 563:          cast(module_id as string), 
 564:          ''
 565:        )<> '' then 'pass' end
 566:      ) as good_data, 
 567:      count(
 568:        case when ifnull(
 569:          cast(module_id as string), 
 570:          ''
 571:        )= '' then 'fail' end
 572:      ) as bad_data 
 573:    from 
 574:      x
 575:    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
 576:  ) result14 
 577:union all 
 578:select 
 579:  datetime(
 580:    current_timestamp(), 
 581:    "Asia/Jakarta"
 582:  ) as time_execution, 
 583:  result15.criteria, 
 584:  result15.metrics, 
 585:  result15.total_data, 
 586:  result15.good_data, 
 587:  result15.bad_data, 
 588:  cast(
 589:    cast(
 590:      result15.good_data * 100 as decimal
 591:    )/ result15.total_data as numeric
 592:  ) as percentage_good_data, 
 593:  cast(
 594:    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
 595:  ) as percentage_bad_data, 
 596:  "video_id" field_name_checking 
 597:from 
 598:  (
 599:    select 
 600:      'Completeness' criteria, 
 601:      'field mandatory is null/blank video_id' metrics, 
 602:      count(ga_session_id) total_data, 
 603:      count(
 604:        case when ifnull(
 605:          cast(video_id as string), 
 606:          ''
 607:        )<> '' then 'pass' end
 608:      ) as good_data, 
 609:      count(
 610:        case when ifnull(
 611:          cast(video_id as string), 
 612:          ''
 613:        )= '' then 'fail' end
 614:      ) as bad_data 
 615:    from 
 616:      x
 617:    where action_type_cleaned = "play_video"  
 618:  ) result15 
 619:union all 
 620:select 
 621:  datetime(
 622:    current_timestamp(), 
 623:    "Asia/Jakarta"
 624:  ) as time_execution, 
 625:  result16.criteria, 
 626:  result16.metrics, 
 627:  result16.total_data, 
 628:  result16.good_data, 
 629:  result16.bad_data, 
 630:  cast(
 631:    cast(
 632:      result16.good_data * 100 as decimal
 633:    )/ result16.total_data as numeric
 634:  ) as percentage_good_data, 
 635:  cast(
 636:    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
 637:  ) as percentage_bad_data, 
 638:  "video_inspirasi_id" field_name_checking 
 639:from 
 640:  (
 641:    select 
 642:      'Completeness' criteria, 
 643:      'field mandatory is null/blank video_inspirasi_id' metrics, 
 644:      count(ga_session_id) total_data, 
 645:      count(
 646:        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
 647:      ) as good_data, 
 648:      count(
 649:        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
 650:      ) as bad_data 
 651:    from 
 652:      x
 653:    where action_type_cleaned = "play_video_inspirasi"  
 654:  ) result16 
 655:union all 
 656:select 
 657:  datetime(
 658:    current_timestamp(), 
 659:    "Asia/Jakarta"
 660:  ) as time_execution, 
 661:  result17.criteria, 
 662:  result17.metrics, 
 663:  result17.total_data, 
 664:  result17.good_data, 
 665:  result17.bad_data, 
 666:  cast(
 667:    cast(
 668:      result17.good_data * 100 as decimal
 669:    )/ result17.total_data as numeric
 670:  ) as percentage_good_data, 
 671:  cast(
 672:    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
 673:  ) as percentage_bad_data, 
 674:  "video_inspirasi_playslist_id" field_name_checking 
 675:from 
 676:  (
 677:    select 
 678:      'Completeness' criteria, 
 679:      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
 680:      count(ga_session_id) total_data, 
 681:      count(
 682:        case when ifnull(
 683:          cast(
 684:            video_inspirasi_playslist_id as string
 685:          ), 
 686:          ''
 687:        )<> '' then 'pass' end
 688:      ) as good_data, 
 689:      count(
 690:        case when ifnull(
 691:          cast(
 692:            video_inspirasi_playslist_id as string
 693:          ), 
 694:          ''
 695:        )= '' then 'fail' end
 696:      ) as bad_data 
 697:    from 
 698:      x
 699:    where action_type_cleaned = "play_video_inspirasi"  
 700:  ) result17 
 701:union all 
 702:select 
 703:  datetime(
 704:    current_timestamp(), 
 705:    "Asia/Jakarta"
 706:  ) as time_execution, 
 707:  result18.criteria, 
 708:  result18.metrics, 
 709:  result18.total_data, 
 710:  result18.good_data, 
 711:  result18.bad_data, 
 712:  cast(
 713:    cast(
 714:      result18.good_data * 100 as decimal
 715:    )/ result18.total_data as numeric
 716:  ) as percentage_good_data, 
 717:  cast(
 718:    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
 719:  ) as percentage_bad_data, 
 720:  "quiz_id" field_name_checking 
 721:from 
 722:  (
 723:    select 
 724:      'Completeness' criteria, 
 725:      'field mandatory is null/blank quiz_id' metrics, 
 726:      count(ga_session_id) total_data, 
 727:      count(
 728:        case when ifnull(
 729:          cast(quiz_id as string), 
 730:          ''
 731:        )<> '' then 'pass' end
 732:      ) as good_data, 
 733:      count(
 734:        case when ifnull(
 735:          cast(quiz_id as string), 
 736:          ''
 737:        )= '' then 'fail' end
 738:      ) as bad_data 
 739:    from 
 740:      x
 741:    where action_type_cleaned = 'open_quiz'  
 742:  ) result18 
 743:union all 
 744:select 
 745:  datetime(
 746:    current_timestamp(), 
 747:    "Asia/Jakarta"
 748:  ) as time_execution, 
 749:  result19.criteria, 
 750:  result19.metrics, 
 751:  result19.total_data, 
 752:  result19.good_data, 
 753:  result19.bad_data, 
 754:  cast(
 755:    cast(
 756:      result19.good_data * 100 as decimal
 757:    )/ result19.total_data as numeric
 758:  ) as percentage_good_data, 
 759:  cast(
 760:    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
 761:  ) as percentage_bad_data, 
 762:  "text_id" field_name_checking 
 763:from 
 764:  (
 765:    select 
 766:      'Completeness' criteria, 
 767:      'field mandatory is null/blank text_id' metrics, 
 768:      count(ga_session_id) total_data, 
 769:      count(
 770:        case when ifnull(
 771:          cast(text_id as string), 
 772:          ''
 773:        )<> '' then 'pass' end
 774:      ) as good_data, 
 775:      count(
 776:        case when ifnull(
 777:          cast(text_id as string), 
 778:          ''
 779:        )= '' then 'fail' end
 780:      ) as bad_data 
 781:    from 
 782:      x
 783:    where action_type_cleaned = 'open_text'  
 784:  ) result19 
 785:union all 
 786:select 
 787:  datetime(
 788:    current_timestamp(), 
 789:    "Asia/Jakarta"
 790:  ) as time_execution, 
 791:  result20.criteria, 
 792:  result20.metrics, 
 793:  result20.total_data, 
 794:  result20.good_data, 
 795:  result20.bad_data, 
 796:  cast(
 797:    cast(
 798:      result20.good_data * 100 as decimal
 799:    )/ result20.total_data as numeric
 800:  ) as percentage_good_data, 
 801:  cast(
 802:    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
 803:  ) as percentage_bad_data, 
 804:  "open_reflection_id" field_name_checking 
 805:from 
 806:  (
 807:    select 
 808:      'Completeness' criteria, 
 809:      'field mandatory is null/blank open_reflection_id' metrics, 
 810:      count(ga_session_id) total_data, 
 811:      count(
 812:        case when ifnull(
 813:          cast(open_reflection_id as string), 
 814:          ''
 815:        )<> '' then 'pass' end
 816:      ) as good_data, 
 817:      count(
 818:        case when ifnull(
 819:          cast(open_reflection_id as string), 
 820:          ''
 821:        )= '' then 'fail' end
 822:      ) as bad_data 
 823:    from 
 824:      x
 825:    where action_type_cleaned = "open_reflection"  
 826:  ) result20 
 827:union all 
 828:select 
 829:  datetime(
 830:    current_timestamp(), 
 831:    "Asia/Jakarta"
 832:  ) as time_execution, 
 833:  result21.criteria, 
 834:  result21.metrics, 
 835:  result21.total_data, 
 836:  result21.good_data, 
 837:  result21.bad_data, 
 838:  cast(
 839:    cast(
 840:      result21.good_data * 100 as decimal
 841:    )/ result21.total_data as numeric
 842:  ) as percentage_good_data, 
 843:  cast(
 844:    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
 845:  ) as percentage_bad_data, 
 846:  "save_reflection_id" field_name_checking 
 847:from 
 848:  (
 849:    select 
 850:      'Completeness' criteria, 
 851:      'field mandatory is null/blank save_reflection_id' metrics, 
 852:      count(ga_session_id) total_data, 
 853:      count(
 854:        case when ifnull(
 855:          cast(save_reflection_id as string), 
 856:          ''
 857:        )<> '' then 'pass' end
 858:      ) as good_data, 
 859:      count(
 860:        case when ifnull(
 861:          cast(save_reflection_id as string), 
 862:          ''
 863:        )= '' then 'fail' end
 864:      ) as bad_data 
 865:    from 
 866:      x
 867:    where action_type_cleaned = 'save_reflection'  
 868:  ) result21 
 869:union all 
 870:select 
 871:  datetime(
 872:    current_timestamp(), 
 873:    "Asia/Jakarta"
 874:  ) as time_execution, 
 875:  result22.criteria, 
 876:  result22.metrics, 
 877:  result22.total_data, 
 878:  result22.good_data, 
 879:  result22.bad_data, 
 880:  cast(
 881:    cast(
 882:      result22.good_data * 100 as decimal
 883:    )/ result22.total_data as numeric
 884:  ) as percentage_good_data, 
 885:  cast(
 886:    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
 887:  ) as percentage_bad_data, 
 888:  "post_test_result" field_name_checking 
 889:from 
 890:  (
 891:    select 
 892:      'Completeness' criteria, 
 893:      'field mandatory is null/blank post_test_result' metrics, 
 894:      count(ga_session_id) total_data, 
 895:      count(
 896:        case when ifnull(post_test_result, '')<> '' then 'pass' end
 897:      ) as good_data, 
 898:      count(
 899:        case when ifnull(post_test_result, '')= '' then 'fail' end
 900:      ) as bad_data 
 901:    from 
 902:      x
 903:    where action_type_cleaned = 'finished_post_test'  
 904:  ) result22 
 905:union all 
 906:select 
 907:  datetime(
 908:    current_timestamp(), 
 909:    "Asia/Jakarta"
 910:  ) as time_execution, 
 911:  result23.criteria, 
 912:  result23.metrics, 
 913:  result23.total_data, 
 914:  result23.good_data, 
 915:  result23.bad_data, 
 916:  cast(
 917:    cast(
 918:      result23.good_data * 100 as decimal
 919:    )/ result23.total_data as numeric
 920:  ) as percentage_good_data, 
 921:  cast(
 922:    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
 923:  ) as percentage_bad_data, 
 924:  "event_params_source" field_name_checking 
 925:from 
 926:  (
 927:    select 
 928:      'Completeness' criteria, 
 929:      'field mandatory is null/blank event_params_source' metrics, 
 930:      count(ga_session_id) total_data, 
 931:      count(
 932:        case when ifnull(event_params_source, '')<> '' then 'pass' end
 933:      ) as good_data, 
 934:      count(
 935:        case when ifnull(event_params_source, '')= '' then 'fail' end
 936:      ) as bad_data 
 937:    from 
 938:      x
 939:  ) result23 
 940:union all 
 941:select 
 942:  datetime(
 943:    current_timestamp(), 
 944:    "Asia/Jakarta"
 945:  ) as time_execution, 
 946:  result24.criteria, 
 947:  result24.metrics, 
 948:  result24.total_data, 
 949:  result24.good_data, 
 950:  result24.bad_data, 
 951:  cast(
 952:    cast(
 953:      result24.good_data * 100 as decimal
 954:    )/ result24.total_data as numeric
 955:  ) as percentage_good_data, 
 956:  cast(
 957:    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
 958:  ) as percentage_bad_data, 
 959:  "event_params_page_location" field_name_checking 
 960:from 
 961:  (
 962:    select 
 963:      'Completeness' criteria, 
 964:      'field mandatory is null/blank event_params_page_location' metrics, 
 965:      count(ga_session_id) total_data, 
 966:      count(
 967:        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
 968:      ) as good_data, 
 969:      count(
 970:        case when ifnull(event_params_page_location, '')= '' then 'fail' end
 971:      ) as bad_data 
 972:    from 
 973:      x
 974:  ) result24 
 975:union all 
 976:select 
 977:  datetime(
 978:    current_timestamp(), 
 979:    "Asia/Jakarta"
 980:  ) as time_execution, 
 981:  result25.criteria, 
 982:  result25.metrics, 
 983:  result25.total_data, 
 984:  result25.good_data, 
 985:  result25.bad_data, 
 986:  cast(
 987:    cast(
 988:      result25.good_data * 100 as decimal
 989:    )/ result25.total_data as numeric
 990:  ) as percentage_good_data, 
 991:  cast(
 992:    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
 993:  ) as percentage_bad_data, 
 994:  "event_params_page_referrer" field_name_checking 
 995:from 
 996:  (
 997:    select 
 998:      'Completeness' criteria, 
 999:      'field mandatory is null/blank event_params_page_referrer' metrics, 
1000:      count(ga_session_id) total_data, 
1001:      count(
1002:        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
1003:      ) as good_data, 
1004:      count(
1005:        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
1006:      ) as bad_data 
1007:    from 
1008:      x
1009:  ) result25 
1010:union all 
1011:select 
1012:  datetime(
1013:    current_timestamp(), 
1014:    "Asia/Jakarta"
1015:  ) as time_execution, 
1016:  result26.criteria, 
1017:  result26.metrics, 
1018:  result26.total_data, 
1019:  result26.good_data, 
1020:  result26.bad_data, 
1021:  cast(
1022:    cast(
1023:      result26.good_data * 100 as decimal
1024:    )/ result26.total_data as numeric
1025:  ) as percentage_good_data, 
1026:  cast(
1027:    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
1028:  ) as percentage_bad_data, 
1029:  "event_params_page_path" field_name_checking 
1030:from 
1031:  (
1032:    select 
1033:      'Completeness' criteria, 
1034:      'field mandatory is null/blank event_params_page_path' metrics, 
1035:      count(ga_session_id) total_data, 
1036:      count(
1037:        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
1038:      ) as good_data, 
1039:      count(
1040:        case when ifnull(event_params_page_path, '')= '' then 'fail' end
1041:      ) as bad_data 
1042:    from 
1043:      x
1044:  ) result26 
1045:union all 
1046:select 
1047:  datetime(
1048:    current_timestamp(), 
1049:    "Asia/Jakarta"
1050:  ) as time_execution, 
1051:  result27.criteria, 
1052:  result27.metrics, 
1053:  result27.total_data, 
1054:  result27.good_data, 
1055:  result27.bad_data, 
1056:  cast(
1057:    cast(
1058:      result27.good_data * 100 as decimal
1059:    )/ result27.total_data as numeric
1060:  ) as percentage_good_data, 
1061:  cast(
1062:    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
1063:  ) as percentage_bad_data, 
1064:  "event_params_page_title" field_name_checking 
1065:from 
1066:  (
1067:    select 
1068:      'Completeness' criteria, 
1069:      'field mandatory is null/blank event_params_page_title' metrics, 
1070:      count(ga_session_id) total_data, 
1071:      count(
1072:        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
1073:      ) as good_data, 
1074:      count(
1075:        case when ifnull(event_params_page_title, '')= '' then 'fail' end
1076:      ) as bad_data 
1077:    from 
1078:      x
1079:  ) result27 
1080:union all 
1081:select 
1082:  datetime(
1083:    current_timestamp(), 
1084:    "Asia/Jakarta"
1085:  ) as time_execution, 
1086:  result28.criteria, 
1087:  result28.metrics, 
1088:  result28.total_data, 
1089:  result28.good_data, 
1090:  result28.bad_data, 
1091:  cast(
1092:    cast(
1093:      result28.good_data * 100 as decimal
1094:    )/ result28.total_data as numeric
1095:  ) as percentage_good_data, 
1096:  cast(
1097:    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
1098:  ) as percentage_bad_data, 
1099:  "event_params_product" field_name_checking 
1100:from 
1101:  (
1102:    select 
1103:      'Completeness' criteria, 
1104:      'field mandatory is null/blank event_params_product' metrics, 
1105:      count(ga_session_id) total_data, 
1106:      count(
1107:        case when ifnull(event_params_product, '')<> '' then 'pass' end
1108:      ) as good_data, 
1109:      count(
1110:        case when ifnull(event_params_product, '')= '' then 'fail' end
1111:      ) as bad_data 
1112:    from 
1113:      x
1114:  ) result28
1115:  );
1116:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 04:08:29.913523 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d9b1a4f-d5b5-42c5-8a3d-14bf4895676e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10869105e0>]}
2021-12-24 04:08:29.914515 (Thread-1): 11:08:29 | 1 of 3 ERROR creating table model dataset_dbt.dqa_completeness....... [ERROR in 3.21s]
2021-12-24 04:08:29.914999 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 04:08:29.915563 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-24 04:08:29.916674 (Thread-1): 11:08:29 | 2 of 3 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-24 04:08:29.917461 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-24 04:08:29.917855 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-24 04:08:29.921272 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-24 04:08:29.921705 (Thread-1): finished collecting timing info
2021-12-24 04:08:29.923227 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:08:29.926833 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:08:30.277604 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-24 04:08:30.278534 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 04:08:33.897567 (Thread-1): finished collecting timing info
2021-12-24 04:08:33.897912 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d9b1a4f-d5b5-42c5-8a3d-14bf4895676e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1086a9e310>]}
2021-12-24 04:08:33.898226 (Thread-1): 11:08:33 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 3.98s]
2021-12-24 04:08:33.898373 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-24 04:08:33.898915 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-24 04:08:33.899150 (Thread-1): 11:08:33 | 3 of 3 SKIP relation dataset_dbt.dqa_uniqueness_2.................... [SKIP]
2021-12-24 04:08:33.899269 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-24 04:08:33.899886 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:08:33.900145 (MainThread): 11:08:33 | 
2021-12-24 04:08:33.900320 (MainThread): 11:08:33 | Finished running 2 table models, 1 view model in 8.32s.
2021-12-24 04:08:33.900531 (MainThread): Connection 'master' was properly closed.
2021-12-24 04:08:33.900669 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_1' was properly closed.
2021-12-24 04:08:33.906295 (MainThread): 
2021-12-24 04:08:33.906483 (MainThread): Completed with 1 error and 0 warnings:
2021-12-24 04:08:33.906594 (MainThread): 
2021-12-24 04:08:33.906695 (MainThread): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
2021-12-24 04:08:33.906827 (MainThread):   No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
2021-12-24 04:08:33.906970 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 04:08:33.907151 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
2021-12-24 04:08:33.907365 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1086a4fd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1086a4ff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1086933d60>]}
2021-12-24 04:08:33.907563 (MainThread): Flushing usage events
2021-12-24 04:38:49.077737 (MainThread): Running with dbt=0.21.0
2021-12-24 04:38:49.293895 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 04:38:49.294930 (MainThread): Tracking: tracking
2021-12-24 04:38:49.301307 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff284f87100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff28319e6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff28319e730>]}
2021-12-24 04:38:49.307757 (MainThread): Partial parsing not enabled
2021-12-24 04:38:49.314686 (MainThread): Parsing macros/catalog.sql
2021-12-24 04:38:49.318839 (MainThread): Parsing macros/etc.sql
2021-12-24 04:38:49.320012 (MainThread): Parsing macros/adapters.sql
2021-12-24 04:38:49.332481 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 04:38:49.333978 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 04:38:49.335656 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 04:38:49.337131 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 04:38:49.338091 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 04:38:49.343325 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 04:38:49.351677 (MainThread): Parsing macros/core.sql
2021-12-24 04:38:49.353849 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 04:38:49.354820 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 04:38:49.356198 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 04:38:49.356909 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 04:38:49.357671 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 04:38:49.361838 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 04:38:49.367238 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 04:38:49.379989 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 04:38:49.383989 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 04:38:49.386155 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 04:38:49.397315 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 04:38:49.398354 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 04:38:49.404095 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 04:38:49.411882 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 04:38:49.416102 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 04:38:49.425624 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 04:38:49.426597 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 04:38:49.444151 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 04:38:49.445081 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 04:38:49.446004 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 04:38:49.446862 (MainThread): Parsing macros/etc/query.sql
2021-12-24 04:38:49.447473 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 04:38:49.448888 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 04:38:49.449898 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 04:38:49.454698 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 04:38:49.596153 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-24 04:38:49.603652 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:38:49.606051 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-24 04:38:49.621003 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:38:49.622137 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:38:49.623108 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:38:49.624138 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:38:49.632377 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '34ad7c85-36a6-44f0-921c-f4d6c1384cc3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2830ac070>]}
2021-12-24 04:38:49.634856 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 04:38:49.635110 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '34ad7c85-36a6-44f0-921c-f4d6c1384cc3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2830ac250>]}
2021-12-24 04:38:49.635310 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 04:38:49.636361 (MainThread): 
2021-12-24 04:38:49.636612 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:38:49.637402 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 04:38:49.637602 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 04:38:50.274996 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 04:38:50.275812 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 04:38:50.290594 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:38:50.694779 (MainThread): 11:38:50 | Concurrency: 1 threads (target='prod')
2021-12-24 04:38:50.695374 (MainThread): 11:38:50 | 
2021-12-24 04:38:50.700294 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 04:38:50.701042 (Thread-1): 11:38:50 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 04:38:50.701820 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:38:50.702268 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 04:38:50.708254 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:38:50.709087 (Thread-1): finished collecting timing info
2021-12-24 04:38:50.714209 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 39094), raddr=('74.125.200.95', 443)>
2021-12-24 04:38:50.714606 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 58006), raddr=('34.101.5.106', 443)>
2021-12-24 04:38:50.714883 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 39098), raddr=('74.125.200.95', 443)>
2021-12-24 04:38:50.715160 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 58010), raddr=('34.101.5.106', 443)>
2021-12-24 04:38:50.736867 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:38:50.737180 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:38:50.740655 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2
  );
    
2021-12-24 04:38:54.185155 (Thread-1): finished collecting timing info
2021-12-24 04:38:54.186163 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '34ad7c85-36a6-44f0-921c-f4d6c1384cc3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff28303f6d0>]}
2021-12-24 04:38:54.186925 (Thread-1): 11:38:54 | 1 of 3 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (2.0 rows, 112.5 KB processed) in 3.48s]
2021-12-24 04:38:54.187336 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 04:38:54.187774 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-24 04:38:54.188779 (Thread-1): 11:38:54 | 2 of 3 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-24 04:38:54.189513 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-24 04:38:54.189895 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-24 04:38:54.193570 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-24 04:38:54.193861 (Thread-1): finished collecting timing info
2021-12-24 04:38:54.195152 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:38:54.198985 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:38:54.557504 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-24 04:38:54.558626 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 04:38:57.830442 (Thread-1): finished collecting timing info
2021-12-24 04:38:57.831662 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '34ad7c85-36a6-44f0-921c-f4d6c1384cc3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff283094e80>]}
2021-12-24 04:38:57.832788 (Thread-1): 11:38:57 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 3.64s]
2021-12-24 04:38:57.833310 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-24 04:38:57.834662 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-24 04:38:57.834883 (Thread-1): 11:38:57 | 3 of 3 START view model dataset_dbt.dqa_uniqueness_2................. [RUN]
2021-12-24 04:38:57.835308 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-24 04:38:57.835438 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-24 04:38:57.840703 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-24 04:38:57.842191 (Thread-1): finished collecting timing info
2021-12-24 04:38:57.856285 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-24 04:38:57.856688 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:38:57.860380 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`

select *
from `playground-325606`.`dataset_dbt`.`dqa_completeness`;


2021-12-24 04:38:58.424318 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected end of input but got keyword SELECT at [11:1]')
2021-12-24 04:38:59.761164 (Thread-1): finished collecting timing info
2021-12-24 04:38:59.762282 (Thread-1): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Expected end of input but got keyword SELECT at [11:1]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Expected end of input but got keyword SELECT at [11:1]

(job ID: d2eb0db0-c315-40fa-95e7-888af75a02e6)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */
   2:
   3:
   4:  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
   5:  OPTIONS()
   6:  as -- Use the `ref` function to select from other models
   7:
   8:select *
   9:from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  10:
  11:select *
  12:from `playground-325606`.`dataset_dbt`.`dqa_completeness`;
  13:
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 22, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
  Syntax error: Expected end of input but got keyword SELECT at [11:1]
  compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-24 04:38:59.775307 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '34ad7c85-36a6-44f0-921c-f4d6c1384cc3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff280f6afa0>]}
2021-12-24 04:38:59.776346 (Thread-1): 11:38:59 | 3 of 3 ERROR creating view model dataset_dbt.dqa_uniqueness_2........ [ERROR in 1.94s]
2021-12-24 04:38:59.776826 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-24 04:38:59.779211 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:38:59.780290 (MainThread): 11:38:59 | 
2021-12-24 04:38:59.780959 (MainThread): 11:38:59 | Finished running 2 table models, 1 view model in 10.14s.
2021-12-24 04:38:59.781555 (MainThread): Connection 'master' was properly closed.
2021-12-24 04:38:59.782080 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-24 04:38:59.793745 (MainThread): 
2021-12-24 04:38:59.794133 (MainThread): Completed with 1 error and 0 warnings:
2021-12-24 04:38:59.794409 (MainThread): 
2021-12-24 04:38:59.794796 (MainThread): Database Error in model dqa_uniqueness_2 (models/example/dqa_uniqueness_2.sql)
2021-12-24 04:38:59.795133 (MainThread):   Syntax error: Expected end of input but got keyword SELECT at [11:1]
2021-12-24 04:38:59.795444 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_uniqueness_2.sql
2021-12-24 04:38:59.795754 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2021-12-24 04:38:59.796097 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff28691eb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2831c1520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff283055df0>]}
2021-12-24 04:38:59.796374 (MainThread): Flushing usage events
2021-12-24 04:39:23.940631 (MainThread): Running with dbt=0.21.0
2021-12-24 04:39:24.090730 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 04:39:24.091273 (MainThread): Tracking: tracking
2021-12-24 04:39:24.096753 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3a6228910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3a447a6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3a447a730>]}
2021-12-24 04:39:24.102909 (MainThread): Partial parsing not enabled
2021-12-24 04:39:24.108198 (MainThread): Parsing macros/catalog.sql
2021-12-24 04:39:24.112576 (MainThread): Parsing macros/etc.sql
2021-12-24 04:39:24.113756 (MainThread): Parsing macros/adapters.sql
2021-12-24 04:39:24.126488 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 04:39:24.128094 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 04:39:24.129655 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 04:39:24.131160 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 04:39:24.132191 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 04:39:24.137626 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 04:39:24.146294 (MainThread): Parsing macros/core.sql
2021-12-24 04:39:24.148531 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 04:39:24.149559 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 04:39:24.150854 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 04:39:24.151593 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 04:39:24.152435 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 04:39:24.156758 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 04:39:24.163080 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 04:39:24.179025 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 04:39:24.184072 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 04:39:24.186849 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 04:39:24.200696 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 04:39:24.202004 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 04:39:24.209322 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 04:39:24.218995 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 04:39:24.224169 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 04:39:24.236313 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 04:39:24.237534 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 04:39:24.258885 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 04:39:24.260123 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 04:39:24.261258 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 04:39:24.262323 (MainThread): Parsing macros/etc/query.sql
2021-12-24 04:39:24.263069 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 04:39:24.264821 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 04:39:24.266249 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 04:39:24.272363 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 04:39:24.453728 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-24 04:39:24.463155 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:39:24.466225 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-24 04:39:24.486367 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:39:24.487770 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:39:24.489040 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:39:24.490282 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:39:24.501317 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b876efa9-1876-4de5-a2b0-45587cc3584d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3a434d190>]}
2021-12-24 04:39:24.504579 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 04:39:24.504858 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b876efa9-1876-4de5-a2b0-45587cc3584d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3a434d1f0>]}
2021-12-24 04:39:24.505082 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 04:39:24.506213 (MainThread): 
2021-12-24 04:39:24.506493 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:39:24.507273 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 04:39:24.507480 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 04:39:25.041118 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 04:39:25.041861 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 04:39:25.048396 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:39:25.415999 (MainThread): 11:39:25 | Concurrency: 1 threads (target='prod')
2021-12-24 04:39:25.416545 (MainThread): 11:39:25 | 
2021-12-24 04:39:25.420948 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 04:39:25.421697 (Thread-1): 11:39:25 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 04:39:25.422440 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:39:25.423060 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 04:39:25.430803 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:39:25.431938 (Thread-1): finished collecting timing info
2021-12-24 04:39:25.437842 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 39122), raddr=('74.125.200.95', 443)>
2021-12-24 04:39:25.438234 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 58034), raddr=('34.101.5.106', 443)>
2021-12-24 04:39:25.438499 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 39126), raddr=('74.125.200.95', 443)>
2021-12-24 04:39:25.438723 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 58038), raddr=('34.101.5.106', 443)>
2021-12-24 04:39:25.450545 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:39:25.453844 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:39:25.828873 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:39:25.829150 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2
  );
    
2021-12-24 04:39:28.064050 (Thread-1): finished collecting timing info
2021-12-24 04:39:28.064350 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b876efa9-1876-4de5-a2b0-45587cc3584d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3a42d6820>]}
2021-12-24 04:39:28.064597 (Thread-1): 11:39:28 | 1 of 3 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (2.0 rows, 112.5 KB processed) in 2.64s]
2021-12-24 04:39:28.064724 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 04:39:28.064845 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_1
2021-12-24 04:39:28.065017 (Thread-1): 11:39:28 | 2 of 3 START table model dataset_dbt.dqa_uniqueness_1................ [RUN]
2021-12-24 04:39:28.065228 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_1".
2021-12-24 04:39:28.065402 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_1
2021-12-24 04:39:28.067015 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-24 04:39:28.067324 (Thread-1): finished collecting timing info
2021-12-24 04:39:28.068312 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:39:28.071688 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:39:28.416729 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_1"
2021-12-24 04:39:28.417649 (Thread-1): On model.my_new_project.dqa_uniqueness_1: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_1"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 04:39:31.918365 (Thread-1): finished collecting timing info
2021-12-24 04:39:31.919326 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b876efa9-1876-4de5-a2b0-45587cc3584d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3a42cc520>]}
2021-12-24 04:39:31.920171 (Thread-1): 11:39:31 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness_1........... [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 3.85s]
2021-12-24 04:39:31.920336 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_1
2021-12-24 04:39:31.921019 (Thread-1): Began running node model.my_new_project.dqa_uniqueness_2
2021-12-24 04:39:31.921208 (Thread-1): 11:39:31 | 3 of 3 START view model dataset_dbt.dqa_uniqueness_2................. [RUN]
2021-12-24 04:39:31.921413 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness_2".
2021-12-24 04:39:31.921520 (Thread-1): Compiling model.my_new_project.dqa_uniqueness_2
2021-12-24 04:39:31.923036 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-24 04:39:31.923372 (Thread-1): finished collecting timing info
2021-12-24 04:39:31.934927 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness_2"
2021-12-24 04:39:31.935236 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:39:31.938486 (Thread-1): On model.my_new_project.dqa_uniqueness_2: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness_2"} */


  create or replace view `playground-325606`.`dataset_dbt`.`dqa_uniqueness_2`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness_1`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_completeness`;


2021-12-24 04:39:40.130791 (Thread-1): finished collecting timing info
2021-12-24 04:39:40.131576 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b876efa9-1876-4de5-a2b0-45587cc3584d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3a21d3bb0>]}
2021-12-24 04:39:40.132261 (Thread-1): 11:39:40 | 3 of 3 OK created view model dataset_dbt.dqa_uniqueness_2............ [SCRIPT (264.0 Bytes processed) in 8.21s]
2021-12-24 04:39:40.132623 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness_2
2021-12-24 04:39:40.134887 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:39:40.136030 (MainThread): 11:39:40 | 
2021-12-24 04:39:40.136577 (MainThread): 11:39:40 | Finished running 2 table models, 1 view model in 15.63s.
2021-12-24 04:39:40.137157 (MainThread): Connection 'master' was properly closed.
2021-12-24 04:39:40.137606 (MainThread): Connection 'model.my_new_project.dqa_uniqueness_2' was properly closed.
2021-12-24 04:39:40.140436 (MainThread): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 39140), raddr=('74.125.200.95', 443)>
2021-12-24 04:39:40.141145 (MainThread): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 58052), raddr=('34.101.5.106', 443)>
2021-12-24 04:39:40.149849 (MainThread): 
2021-12-24 04:39:40.150200 (MainThread): Completed successfully
2021-12-24 04:39:40.150542 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2021-12-24 04:39:40.150949 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3a7bbeb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3a44a1ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc3a432b490>]}
2021-12-24 04:39:40.151311 (MainThread): Flushing usage events
2021-12-24 04:43:25.524851 (MainThread): Running with dbt=0.21.0
2021-12-24 04:43:25.677178 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 04:43:25.677714 (MainThread): Tracking: tracking
2021-12-24 04:43:25.683411 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b96c927c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b94f2c640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b94f2c6a0>]}
2021-12-24 04:43:25.689641 (MainThread): Partial parsing not enabled
2021-12-24 04:43:25.693903 (MainThread): Parsing macros/catalog.sql
2021-12-24 04:43:25.698064 (MainThread): Parsing macros/etc.sql
2021-12-24 04:43:25.699354 (MainThread): Parsing macros/adapters.sql
2021-12-24 04:43:25.712312 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 04:43:25.713878 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 04:43:25.715482 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 04:43:25.717014 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 04:43:25.717997 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 04:43:25.723531 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 04:43:25.732686 (MainThread): Parsing macros/core.sql
2021-12-24 04:43:25.734932 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 04:43:25.735990 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 04:43:25.737346 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 04:43:25.738041 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 04:43:25.738846 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 04:43:25.742978 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 04:43:25.748874 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 04:43:25.762046 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 04:43:25.766186 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 04:43:25.768574 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 04:43:25.780301 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 04:43:25.781381 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 04:43:25.787471 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 04:43:25.795500 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 04:43:25.799776 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 04:43:25.809594 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 04:43:25.810745 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 04:43:25.828618 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 04:43:25.829663 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 04:43:25.830883 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 04:43:25.831879 (MainThread): Parsing macros/etc/query.sql
2021-12-24 04:43:25.832529 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 04:43:25.833945 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 04:43:25.835060 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 04:43:25.840136 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 04:43:25.986877 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 04:43:25.994458 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 04:43:25.996869 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:43:26.013196 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:43:26.014528 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:43:26.015654 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:43:26.016914 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 04:43:26.017255 (MainThread): [WARNING]: Test 'test.my_new_project.unique_dqa_uniqueness_1_id.4bfba4c993' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_1' which was not found
2021-12-24 04:43:26.017458 (MainThread): [WARNING]: Test 'test.my_new_project.not_null_dqa_uniqueness_1_id.7e2b06f4b2' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_1' which was not found
2021-12-24 04:43:26.017635 (MainThread): [WARNING]: Test 'test.my_new_project.unique_dqa_uniqueness_2_id.ab43edfe4d' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_2' which was not found
2021-12-24 04:43:26.017798 (MainThread): [WARNING]: Test 'test.my_new_project.not_null_dqa_uniqueness_2_id.49f2b9f55e' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_2' which was not found
2021-12-24 04:43:26.027924 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f21c9604-6d5c-4e37-bf50-5f1398cd8b7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b94db61c0>]}
2021-12-24 04:43:26.031034 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 04:43:26.031327 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f21c9604-6d5c-4e37-bf50-5f1398cd8b7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b94db60a0>]}
2021-12-24 04:43:26.031538 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 04:43:26.032494 (MainThread): 
2021-12-24 04:43:26.032751 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:43:26.033508 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 04:43:26.033731 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 04:43:26.563015 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 04:43:26.563641 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 04:43:26.575622 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:43:26.915159 (MainThread): 11:43:26 | Concurrency: 1 threads (target='prod')
2021-12-24 04:43:26.915726 (MainThread): 11:43:26 | 
2021-12-24 04:43:26.920227 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 04:43:26.920953 (Thread-1): 11:43:26 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 04:43:26.921794 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 04:43:26.922250 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 04:43:26.928963 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:43:26.929996 (Thread-1): finished collecting timing info
2021-12-24 04:43:26.945939 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:43:26.949468 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 04:43:27.311656 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 43758), raddr=('142.251.10.95', 443)>
2021-12-24 04:43:27.312128 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 58082), raddr=('34.101.5.106', 443)>
2021-12-24 04:43:27.312437 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 43762), raddr=('142.251.10.95', 443)>
2021-12-24 04:43:27.312710 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 58086), raddr=('34.101.5.106', 443)>
2021-12-24 04:43:27.323510 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 04:43:27.323772 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2
  );
    
2021-12-24 04:43:29.607891 (Thread-1): finished collecting timing info
2021-12-24 04:43:29.608831 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f21c9604-6d5c-4e37-bf50-5f1398cd8b7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b94f4e040>]}
2021-12-24 04:43:29.609710 (Thread-1): 11:43:29 | 1 of 3 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (2.0 rows, 112.5 KB processed) in 2.69s]
2021-12-24 04:43:29.610110 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 04:43:29.610618 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 04:43:29.611503 (Thread-1): 11:43:29 | 2 of 3 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 04:43:29.612261 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 04:43:29.612563 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 04:43:29.615986 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 04:43:29.616363 (Thread-1): finished collecting timing info
2021-12-24 04:43:29.618621 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 04:43:29.618920 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:43:29.622679 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 04:43:33.550902 (Thread-1): finished collecting timing info
2021-12-24 04:43:33.551880 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f21c9604-6d5c-4e37-bf50-5f1398cd8b7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b94d9f640>]}
2021-12-24 04:43:33.552680 (Thread-1): 11:43:33 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 3.94s]
2021-12-24 04:43:33.553124 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 04:43:33.554691 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 04:43:33.555377 (Thread-1): 11:43:33 | 3 of 3 START view model dataset_dbt.view_models_dqa.................. [RUN]
2021-12-24 04:43:33.556224 (Thread-1): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 04:43:33.556673 (Thread-1): Compiling model.my_new_project.view_models_dqa
2021-12-24 04:43:33.558098 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 43770), raddr=('142.251.10.95', 443)>
2021-12-24 04:43:33.558643 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 58094), raddr=('34.101.5.106', 443)>
2021-12-24 04:43:33.563679 (Thread-1): Writing injected SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 04:43:33.564333 (Thread-1): finished collecting timing info
2021-12-24 04:43:33.577970 (Thread-1): Writing runtime SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 04:43:33.578258 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 04:43:33.581810 (Thread-1): On model.my_new_project.view_models_dqa: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.view_models_dqa"} */


  create or replace view `playground-325606`.`dataset_dbt`.`view_models_dqa`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_completeness`;


2021-12-24 04:43:38.528048 (Thread-1): finished collecting timing info
2021-12-24 04:43:38.528971 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f21c9604-6d5c-4e37-bf50-5f1398cd8b7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b94d9f640>]}
2021-12-24 04:43:38.529299 (Thread-1): 11:43:38 | 3 of 3 OK created view model dataset_dbt.view_models_dqa............. [SCRIPT (264.0 Bytes processed) in 4.97s]
2021-12-24 04:43:38.529463 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 04:43:38.530376 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 04:43:38.530624 (MainThread): 11:43:38 | 
2021-12-24 04:43:38.530753 (MainThread): 11:43:38 | Finished running 2 table models, 1 view model in 12.50s.
2021-12-24 04:43:38.530999 (MainThread): Connection 'master' was properly closed.
2021-12-24 04:43:38.531356 (MainThread): Connection 'model.my_new_project.view_models_dqa' was properly closed.
2021-12-24 04:43:38.536843 (MainThread): 
2021-12-24 04:43:38.537020 (MainThread): Completed successfully
2021-12-24 04:43:38.537132 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2021-12-24 04:43:38.537294 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b94d5fe80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b94e1fa30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b94df56a0>]}
2021-12-24 04:43:38.537476 (MainThread): Flushing usage events
2021-12-24 07:22:12.559207 (MainThread): Running with dbt=0.21.0
2021-12-24 07:22:12.801729 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:22:12.802501 (MainThread): Tracking: tracking
2021-12-24 07:22:12.809633 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2b29e8730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2b0c806d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2b0c80730>]}
2021-12-24 07:22:12.816948 (MainThread): Partial parsing not enabled
2021-12-24 07:22:12.825340 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:22:12.829409 (MainThread): Parsing macros/etc.sql
2021-12-24 07:22:12.830657 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:22:12.843783 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:22:12.845385 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:22:12.846958 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:22:12.848614 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:22:12.849815 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:22:12.855701 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:22:12.863898 (MainThread): Parsing macros/core.sql
2021-12-24 07:22:12.866098 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:22:12.867192 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:22:12.868539 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:22:12.869264 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:22:12.870057 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:22:12.874279 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:22:12.879770 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:22:12.893003 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:22:12.897106 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:22:12.899578 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:22:12.911057 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:22:12.912149 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:22:12.918023 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:22:12.925936 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:22:12.930127 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:22:12.940174 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:22:12.941175 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:22:12.959106 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:22:12.960100 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:22:12.961033 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:22:12.961956 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:22:12.962593 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:22:12.964092 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:22:12.965097 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:22:12.970127 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:22:13.115484 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:22:13.123135 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:22:13.125481 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:22:13.141597 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:22:13.142876 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:22:13.144096 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:22:13.145262 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:22:13.145566 (MainThread): [WARNING]: Test 'test.my_new_project.unique_dqa_uniqueness_1_id.4bfba4c993' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_1' which was not found
2021-12-24 07:22:13.145722 (MainThread): [WARNING]: Test 'test.my_new_project.not_null_dqa_uniqueness_1_id.7e2b06f4b2' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_1' which was not found
2021-12-24 07:22:13.145842 (MainThread): [WARNING]: Test 'test.my_new_project.unique_dqa_uniqueness_2_id.ab43edfe4d' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_2' which was not found
2021-12-24 07:22:13.145954 (MainThread): [WARNING]: Test 'test.my_new_project.not_null_dqa_uniqueness_2_id.49f2b9f55e' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_2' which was not found
2021-12-24 07:22:13.156233 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '91622928-bb34-449e-89e2-b3af394176b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2b0b0d220>]}
2021-12-24 07:22:13.159280 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:22:13.159568 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '91622928-bb34-449e-89e2-b3af394176b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2b0b0d070>]}
2021-12-24 07:22:13.159775 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:22:13.160760 (MainThread): 
2021-12-24 07:22:13.161041 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:22:13.161929 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:22:13.162163 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:22:18.838676 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:22:18.839296 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:22:18.852098 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:22:19.195779 (MainThread): 14:22:19 | Concurrency: 1 threads (target='prod')
2021-12-24 07:22:19.196368 (MainThread): 14:22:19 | 
2021-12-24 07:22:19.201214 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:22:19.202016 (Thread-1): 14:22:19 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:22:19.202757 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:22:19.203190 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:22:19.210371 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:22:19.211199 (Thread-1): finished collecting timing info
2021-12-24 07:22:19.229222 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:22:19.233021 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:22:19.580749 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44738), raddr=('142.251.10.95', 443)>
2021-12-24 07:22:19.581263 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 56800), raddr=('34.101.5.42', 443)>
2021-12-24 07:22:19.581626 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44742), raddr=('142.251.10.95', 443)>
2021-12-24 07:22:19.582025 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 56804), raddr=('34.101.5.42', 443)>
2021-12-24 07:22:19.599764 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:22:19.600053 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(time_gmt7, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(time_gmt7, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10
  );
    
2021-12-24 07:22:21.530785 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]')
2021-12-24 07:22:23.588221 (Thread-1): finished collecting timing info
2021-12-24 07:22:23.589125 (Thread-1): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]

(job ID: 0cfe3963-83e4-481e-924b-856e69621b7f)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    /*
  10:    Welcome to your first dbt model!
  11:    Did you know that you can also configure models directly within SQL files?
  12:    This will override configurations stated in dbt_project.yml
  13:
  14:    Try changing "table" to "view" below
  15:*/
  16:
  17:
  18:
  19:with x as (
  20:  select 
  21:    ga_session_id, 
  22:    user_id, 
  23:    user_pseudo_id, 
  24:    event_name, 
  25:    action_type_cleaned, 
  26:    user_first_timestamp_gmt7, 
  27:    timestamp_gmt7, 
  28:    date_gmt7, 
  29:    time_gmt7, 
  30:    hour_gmt7, 
  31:    device_category_cleaned, 
  32:    engagement_time_msec, 
  33:    material_id, 
  34:    module_id, 
  35:    video_id, 
  36:    video_inspirasi_id, 
  37:    video_inspirasi_playslist_id, 
  38:    quiz_id, 
  39:    text_id, 
  40:    open_reflection_id, 
  41:    save_reflection_id, 
  42:    post_test_result, 
  43:    event_params_source, 
  44:    event_params_page_location, 
  45:    event_params_page_referrer, 
  46:    event_params_page_path, 
  47:    event_params_page_title, 
  48:    event_params_product 
  49:  FROM 
  50:    `playground-325606.dataset_dbt.event_tracker_belajar`
  51:) 
  52:select 
  53:  datetime(
  54:    current_timestamp(), 
  55:    "Asia/Jakarta"
  56:  ) as time_execution, 
  57:  result1.criteria, 
  58:  result1.metrics, 
  59:  result1.total_data, 
  60:  result1.good_data, 
  61:  result1.bad_data, 
  62:  cast(
  63:    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  64:  ) as percentage_good_data, 
  65:  cast(
  66:    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  67:  ) as percentage_bad_data, 
  68:  "ga_session_id" field_name_checking 
  69:from 
  70:  (
  71:    select 
  72:      'Completeness' criteria, 
  73:      'field mandatory is null/blank ga_session_id' metrics, 
  74:      count(ga_session_id) total_data, 
  75:      count(
  76:        case when ifnull(
  77:          cast(ga_session_id as string), 
  78:          ''
  79:        )<> '' then 'pass' end
  80:      ) as good_data, 
  81:      count(
  82:        case when ifnull(
  83:          cast(ga_session_id as string), 
  84:          ''
  85:        )= '' then 'fail' end
  86:      ) as bad_data 
  87:    from 
  88:      x
  89:  ) result1 
  90:union all 
  91:select 
  92:  datetime(
  93:    current_timestamp(), 
  94:    "Asia/Jakarta"
  95:  ) as time_execution, 
  96:  result2.criteria, 
  97:  result2.metrics, 
  98:  result2.total_data, 
  99:  result2.good_data, 
 100:  result2.bad_data, 
 101:  cast(
 102:    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
 103:  ) as percentage_good_data, 
 104:  cast(
 105:    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
 106:  ) as percentage_bad_data, 
 107:  "user_id" field_name_checking 
 108:from 
 109:  (
 110:    select 
 111:      'Completeness' criteria, 
 112:      'field mandatory is null/blank user_id' metrics, 
 113:      count(ga_session_id) total_data, 
 114:      count(
 115:        case when ifnull(user_id, '')<> '' then 'pass' end
 116:      ) as good_data, 
 117:      count(
 118:        case when ifnull(user_id, '')= '' then 'fail' end
 119:      ) as bad_data 
 120:    from 
 121:      x
 122:  ) result2 
 123:union all 
 124:select 
 125:  datetime(
 126:    current_timestamp(), 
 127:    "Asia/Jakarta"
 128:  ) as time_execution, 
 129:  result3.criteria, 
 130:  result3.metrics, 
 131:  result3.total_data, 
 132:  result3.good_data, 
 133:  result3.bad_data, 
 134:  cast(
 135:    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
 136:  ) as percentage_good_data, 
 137:  cast(
 138:    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
 139:  ) as percentage_bad_data, 
 140:  "user_pseudo_id" field_name_checking 
 141:from 
 142:  (
 143:    select 
 144:      'Completeness' criteria, 
 145:      'field mandatory is null/blank user_pseudo_id' metrics, 
 146:      count(ga_session_id) total_data, 
 147:      count(
 148:        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
 149:      ) as good_data, 
 150:      count(
 151:        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
 152:      ) as bad_data 
 153:    from 
 154:      x
 155:  ) result3 
 156:union all 
 157:select 
 158:  datetime(
 159:    current_timestamp(), 
 160:    "Asia/Jakarta"
 161:  ) as time_execution, 
 162:  result4.criteria, 
 163:  result4.metrics, 
 164:  result4.total_data, 
 165:  result4.good_data, 
 166:  result4.bad_data, 
 167:  cast(
 168:    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
 169:  ) as percentage_good_data, 
 170:  cast(
 171:    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
 172:  ) as percentage_bad_data, 
 173:  "event_name" field_name_checking 
 174:from 
 175:  (
 176:    select 
 177:      'Completeness' criteria, 
 178:      'field mandatory is null/blank event_name' metrics, 
 179:      count(ga_session_id) total_data, 
 180:      count(
 181:        case when ifnull(event_name, '')<> '' then 'pass' end
 182:      ) as good_data, 
 183:      count(
 184:        case when ifnull(event_name, '')= '' then 'fail' end
 185:      ) as bad_data 
 186:    from 
 187:      x
 188:  ) result4 
 189:union all 
 190:select 
 191:  datetime(
 192:    current_timestamp(), 
 193:    "Asia/Jakarta"
 194:  ) as time_execution, 
 195:  result5.criteria, 
 196:  result5.metrics, 
 197:  result5.total_data, 
 198:  result5.good_data, 
 199:  result5.bad_data, 
 200:  cast(
 201:    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
 202:  ) as percentage_good_data, 
 203:  cast(
 204:    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
 205:  ) as percentage_bad_data, 
 206:  "action_type_cleaned" field_name_checking 
 207:from 
 208:  (
 209:    select 
 210:      'Completeness' criteria, 
 211:      'field mandatory is null/blank action_type_cleaned' metrics, 
 212:      count(ga_session_id) total_data, 
 213:      count(
 214:        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
 215:      ) as good_data, 
 216:      count(
 217:        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
 218:      ) as bad_data 
 219:    from 
 220:      x
 221:  ) result5 
 222:union all 
 223:select 
 224:  datetime(
 225:    current_timestamp(), 
 226:    "Asia/Jakarta"
 227:  ) as time_execution, 
 228:  result6.criteria, 
 229:  result6.metrics, 
 230:  result6.total_data, 
 231:  result6.good_data, 
 232:  result6.bad_data, 
 233:  cast(
 234:    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
 235:  ) as percentage_good_data, 
 236:  cast(
 237:    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
 238:  ) as percentage_bad_data, 
 239:  "user_first_timestamp_gmt7" field_name_checking 
 240:from 
 241:  (
 242:    select 
 243:      'Completeness' criteria, 
 244:      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
 245:      count(ga_session_id) total_data, 
 246:      count(
 247:        case when ifnull(
 248:          cast(
 249:            user_first_timestamp_gmt7 as string
 250:          ), 
 251:          ''
 252:        )<> '' then 'pass' end
 253:      ) as good_data, 
 254:      count(
 255:        case when ifnull(
 256:          cast(
 257:            user_first_timestamp_gmt7 as string
 258:          ), 
 259:          ''
 260:        )= '' then 'fail' end
 261:      ) as bad_data 
 262:    from 
 263:      x
 264:  ) result6 
 265:union all 
 266:select 
 267:  datetime(
 268:    current_timestamp(), 
 269:    "Asia/Jakarta"
 270:  ) as time_execution, 
 271:  result7.criteria, 
 272:  result7.metrics, 
 273:  result7.total_data, 
 274:  result7.good_data, 
 275:  result7.bad_data, 
 276:  cast(
 277:    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
 278:  ) as percentage_good_data, 
 279:  cast(
 280:    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
 281:  ) as percentage_bad_data, 
 282:  "timestamp_gmt7" field_name_checking 
 283:from 
 284:  (
 285:    select 
 286:      'Completeness' criteria, 
 287:      'field mandatory is null/blank timestamp_gmt7' metrics, 
 288:      count(ga_session_id) total_data, 
 289:      count(
 290:        case when ifnull(
 291:          cast(timestamp_gmt7 as string), 
 292:          ''
 293:        )<> '' then 'pass' end
 294:      ) as good_data, 
 295:      count(
 296:        case when ifnull(
 297:          cast(timestamp_gmt7 as string), 
 298:          ''
 299:        )= '' then 'fail' end
 300:      ) as bad_data 
 301:    from 
 302:      x
 303:  ) result7 
 304:union all 
 305:select 
 306:  datetime(
 307:    current_timestamp(), 
 308:    "Asia/Jakarta"
 309:  ) as time_execution, 
 310:  result8.criteria, 
 311:  result8.metrics, 
 312:  result8.total_data, 
 313:  result8.good_data, 
 314:  result8.bad_data, 
 315:  cast(
 316:    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
 317:  ) as percentage_good_data, 
 318:  cast(
 319:    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
 320:  ) as percentage_bad_data, 
 321:  "date_gmt7" field_name_checking 
 322:from 
 323:  (
 324:    select 
 325:      'Completeness' criteria, 
 326:      'field mandatory is null/blank date_gmt7' metrics, 
 327:      count(ga_session_id) total_data, 
 328:      count(
 329:        case when ifnull(
 330:          cast(date_gmt7 as string), 
 331:          ''
 332:        )<> '' then 'pass' end
 333:      ) as good_data, 
 334:      count(
 335:        case when ifnull(
 336:          cast(date_gmt7 as string), 
 337:          ''
 338:        )= '' then 'fail' end
 339:      ) as bad_data 
 340:    from 
 341:      x
 342:  ) result8 
 343:union all 
 344:select 
 345:  datetime(
 346:    current_timestamp(), 
 347:    "Asia/Jakarta"
 348:  ) as time_execution, 
 349:  result9.criteria, 
 350:  result9.metrics, 
 351:  result9.total_data, 
 352:  result9.good_data, 
 353:  result9.bad_data, 
 354:  cast(
 355:    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
 356:  ) as percentage_good_data, 
 357:  cast(
 358:    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
 359:  ) as percentage_bad_data, 
 360:  "time_gmt7" field_name_checking 
 361:from 
 362:  (
 363:    select 
 364:      'Completeness' criteria, 
 365:      'field mandatory is null/blank time_gmt7' metrics, 
 366:      count(ga_session_id) total_data, 
 367:      count(
 368:        case when ifnull(time_gmt7, '')<> '' then 'pass' end
 369:      ) as good_data, 
 370:      count(
 371:        case when ifnull(time_gmt7, '')= '' then 'fail' end
 372:      ) as bad_data 
 373:    from 
 374:      x
 375:  ) result9 
 376:union all 
 377:select 
 378:  datetime(
 379:    current_timestamp(), 
 380:    "Asia/Jakarta"
 381:  ) as time_execution, 
 382:  result10.criteria, 
 383:  result10.metrics, 
 384:  result10.total_data, 
 385:  result10.good_data, 
 386:  result10.bad_data, 
 387:  cast(
 388:    cast(
 389:      result10.good_data * 100 as decimal
 390:    )/ result10.total_data as numeric
 391:  ) as percentage_good_data, 
 392:  cast(
 393:    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
 394:  ) as percentage_bad_data, 
 395:  "hour_gmt7" field_name_checking 
 396:from 
 397:  (
 398:    select 
 399:      'Completeness' criteria, 
 400:      'field mandatory is null/blank hour_gmt7' metrics, 
 401:      count(ga_session_id) total_data, 
 402:      count(
 403:        case when ifnull(
 404:          cast(hour_gmt7 as string), 
 405:          ''
 406:        )<> '' then 'pass' end
 407:      ) as good_data, 
 408:      count(
 409:        case when ifnull(
 410:          cast(hour_gmt7 as string), 
 411:          ''
 412:        )= '' then 'fail' end
 413:      ) as bad_data 
 414:    from 
 415:      x
 416:  ) result10
 417:  );
 418:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 07:22:23.601521 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '91622928-bb34-449e-89e2-b3af394176b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2b0a99430>]}
2021-12-24 07:22:23.602347 (Thread-1): 14:22:23 | 1 of 3 ERROR creating table model dataset_dbt.dqa_completeness....... [ERROR in 4.40s]
2021-12-24 07:22:23.602776 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:22:23.603170 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:22:23.604114 (Thread-1): 14:22:23 | 2 of 3 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:22:23.604894 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:22:23.605398 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:22:23.610201 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:22:23.611033 (Thread-1): finished collecting timing info
2021-12-24 07:22:23.614544 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:22:23.621869 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:22:23.976485 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:22:23.977335 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:22:27.273393 (Thread-1): finished collecting timing info
2021-12-24 07:22:27.274308 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '91622928-bb34-449e-89e2-b3af394176b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2b0a9b880>]}
2021-12-24 07:22:27.275089 (Thread-1): 14:22:27 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 3.67s]
2021-12-24 07:22:27.275580 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:22:27.276905 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:22:27.277293 (Thread-1): 14:22:27 | 3 of 3 SKIP relation dataset_dbt.view_models_dqa..................... [SKIP]
2021-12-24 07:22:27.277559 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:22:27.278778 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:22:27.279297 (MainThread): 14:22:27 | 
2021-12-24 07:22:27.279557 (MainThread): 14:22:27 | Finished running 2 table models, 1 view model in 14.12s.
2021-12-24 07:22:27.279806 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:22:27.279994 (MainThread): Connection 'model.my_new_project.dqa_uniqueness' was properly closed.
2021-12-24 07:22:27.284350 (MainThread): 
2021-12-24 07:22:27.284542 (MainThread): Completed with 1 error and 0 warnings:
2021-12-24 07:22:27.284673 (MainThread): 
2021-12-24 07:22:27.284810 (MainThread): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
2021-12-24 07:22:27.284984 (MainThread):   No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
2021-12-24 07:22:27.285179 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 07:22:27.285332 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
2021-12-24 07:22:27.285521 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2b0b4d730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2b0b74730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2b0c13220>]}
2021-12-24 07:22:27.285727 (MainThread): Flushing usage events
2021-12-24 07:22:47.601959 (MainThread): Running with dbt=0.21.0
2021-12-24 07:22:47.756595 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:22:47.757193 (MainThread): Tracking: tracking
2021-12-24 07:22:47.762974 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f398fb6caf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f398de06640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f398de066a0>]}
2021-12-24 07:22:47.769766 (MainThread): Partial parsing not enabled
2021-12-24 07:22:47.774156 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:22:47.778152 (MainThread): Parsing macros/etc.sql
2021-12-24 07:22:47.779302 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:22:47.792362 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:22:47.793906 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:22:47.795512 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:22:47.797060 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:22:47.798075 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:22:47.803695 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:22:47.812329 (MainThread): Parsing macros/core.sql
2021-12-24 07:22:47.814592 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:22:47.815627 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:22:47.817030 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:22:47.817807 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:22:47.818703 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:22:47.822851 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:22:47.828368 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:22:47.841510 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:22:47.845868 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:22:47.848092 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:22:47.859570 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:22:47.860707 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:22:47.866616 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:22:47.875701 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:22:47.879839 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:22:47.889780 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:22:47.890776 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:22:47.908677 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:22:47.909612 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:22:47.910543 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:22:47.911471 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:22:47.912082 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:22:47.913534 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:22:47.914686 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:22:47.919854 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:22:48.070108 (MainThread): 1699: statically parsed example/view_models_dqa.sql
2021-12-24 07:22:48.070317 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:22:48.078445 (MainThread): Sending event: {'category': 'dbt', 'action': 'experimental_parser', 'label': '8024a995-2ae8-4904-a29d-93c6f315ce5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f398dd09310>]}
2021-12-24 07:22:48.079473 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:22:48.082373 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:22:48.099559 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:22:48.100827 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:22:48.102070 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:22:48.103154 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:22:48.103447 (MainThread): [WARNING]: Test 'test.my_new_project.unique_dqa_uniqueness_1_id.4bfba4c993' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_1' which was not found
2021-12-24 07:22:48.103590 (MainThread): [WARNING]: Test 'test.my_new_project.not_null_dqa_uniqueness_1_id.7e2b06f4b2' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_1' which was not found
2021-12-24 07:22:48.103696 (MainThread): [WARNING]: Test 'test.my_new_project.unique_dqa_uniqueness_2_id.ab43edfe4d' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_2' which was not found
2021-12-24 07:22:48.103800 (MainThread): [WARNING]: Test 'test.my_new_project.not_null_dqa_uniqueness_2_id.49f2b9f55e' (models/example/schema.yml) depends on a node named 'dqa_uniqueness_2' which was not found
2021-12-24 07:22:48.113023 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8024a995-2ae8-4904-a29d-93c6f315ce5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f398cc891c0>]}
2021-12-24 07:22:48.116088 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:22:48.116376 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8024a995-2ae8-4904-a29d-93c6f315ce5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f398cc89040>]}
2021-12-24 07:22:48.116577 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:22:48.117577 (MainThread): 
2021-12-24 07:22:48.117825 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:22:48.118479 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:22:48.118689 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:22:48.610401 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:22:48.611102 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:22:48.622915 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:22:48.975647 (MainThread): 14:22:48 | Concurrency: 1 threads (target='prod')
2021-12-24 07:22:48.976404 (MainThread): 14:22:48 | 
2021-12-24 07:22:48.981301 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:22:48.982120 (Thread-1): 14:22:48 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:22:48.982856 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:22:48.983281 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:22:48.989316 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:22:48.990084 (Thread-1): finished collecting timing info
2021-12-24 07:22:49.005590 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:22:49.009166 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:22:49.418572 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44758), raddr=('142.251.10.95', 443)>
2021-12-24 07:22:49.419206 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 56820), raddr=('34.101.5.42', 443)>
2021-12-24 07:22:49.419721 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44762), raddr=('142.251.10.95', 443)>
2021-12-24 07:22:49.420165 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 56824), raddr=('34.101.5.42', 443)>
2021-12-24 07:22:49.437828 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:22:49.438156 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6
  );
    
2021-12-24 07:22:50.843279 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]')
2021-12-24 07:22:52.898593 (Thread-1): finished collecting timing info
2021-12-24 07:22:52.899593 (Thread-1): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]

(job ID: 0f03c3eb-338b-4a74-a40a-131b5264da13)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    /*
  10:    Welcome to your first dbt model!
  11:    Did you know that you can also configure models directly within SQL files?
  12:    This will override configurations stated in dbt_project.yml
  13:
  14:    Try changing "table" to "view" below
  15:*/
  16:
  17:
  18:
  19:with x as (
  20:  select 
  21:    ga_session_id, 
  22:    user_id, 
  23:    user_pseudo_id, 
  24:    event_name, 
  25:    action_type_cleaned, 
  26:    user_first_timestamp_gmt7, 
  27:    timestamp_gmt7, 
  28:    date_gmt7, 
  29:    time_gmt7, 
  30:    hour_gmt7, 
  31:    device_category_cleaned, 
  32:    engagement_time_msec, 
  33:    material_id, 
  34:    module_id, 
  35:    video_id, 
  36:    video_inspirasi_id, 
  37:    video_inspirasi_playslist_id, 
  38:    quiz_id, 
  39:    text_id, 
  40:    open_reflection_id, 
  41:    save_reflection_id, 
  42:    post_test_result, 
  43:    event_params_source, 
  44:    event_params_page_location, 
  45:    event_params_page_referrer, 
  46:    event_params_page_path, 
  47:    event_params_page_title, 
  48:    event_params_product 
  49:  FROM 
  50:    `playground-325606.dataset_dbt.event_tracker_belajar`
  51:) 
  52:select 
  53:  datetime(
  54:    current_timestamp(), 
  55:    "Asia/Jakarta"
  56:  ) as time_execution, 
  57:  result1.criteria, 
  58:  result1.metrics, 
  59:  result1.total_data, 
  60:  result1.good_data, 
  61:  result1.bad_data, 
  62:  cast(
  63:    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  64:  ) as percentage_good_data, 
  65:  cast(
  66:    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  67:  ) as percentage_bad_data, 
  68:  "ga_session_id" field_name_checking 
  69:from 
  70:  (
  71:    select 
  72:      'Completeness' criteria, 
  73:      'field mandatory is null/blank ga_session_id' metrics, 
  74:      count(ga_session_id) total_data, 
  75:      count(
  76:        case when ifnull(
  77:          cast(ga_session_id as string), 
  78:          ''
  79:        )<> '' then 'pass' end
  80:      ) as good_data, 
  81:      count(
  82:        case when ifnull(
  83:          cast(ga_session_id as string), 
  84:          ''
  85:        )= '' then 'fail' end
  86:      ) as bad_data 
  87:    from 
  88:      x
  89:  ) result1 
  90:union all 
  91:select 
  92:  datetime(
  93:    current_timestamp(), 
  94:    "Asia/Jakarta"
  95:  ) as time_execution, 
  96:  result2.criteria, 
  97:  result2.metrics, 
  98:  result2.total_data, 
  99:  result2.good_data, 
 100:  result2.bad_data, 
 101:  cast(
 102:    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
 103:  ) as percentage_good_data, 
 104:  cast(
 105:    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
 106:  ) as percentage_bad_data, 
 107:  "user_id" field_name_checking 
 108:from 
 109:  (
 110:    select 
 111:      'Completeness' criteria, 
 112:      'field mandatory is null/blank user_id' metrics, 
 113:      count(ga_session_id) total_data, 
 114:      count(
 115:        case when ifnull(user_id, '')<> '' then 'pass' end
 116:      ) as good_data, 
 117:      count(
 118:        case when ifnull(user_id, '')= '' then 'fail' end
 119:      ) as bad_data 
 120:    from 
 121:      x
 122:  ) result2 
 123:union all 
 124:select 
 125:  datetime(
 126:    current_timestamp(), 
 127:    "Asia/Jakarta"
 128:  ) as time_execution, 
 129:  result3.criteria, 
 130:  result3.metrics, 
 131:  result3.total_data, 
 132:  result3.good_data, 
 133:  result3.bad_data, 
 134:  cast(
 135:    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
 136:  ) as percentage_good_data, 
 137:  cast(
 138:    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
 139:  ) as percentage_bad_data, 
 140:  "user_pseudo_id" field_name_checking 
 141:from 
 142:  (
 143:    select 
 144:      'Completeness' criteria, 
 145:      'field mandatory is null/blank user_pseudo_id' metrics, 
 146:      count(ga_session_id) total_data, 
 147:      count(
 148:        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
 149:      ) as good_data, 
 150:      count(
 151:        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
 152:      ) as bad_data 
 153:    from 
 154:      x
 155:  ) result3 
 156:union all 
 157:select 
 158:  datetime(
 159:    current_timestamp(), 
 160:    "Asia/Jakarta"
 161:  ) as time_execution, 
 162:  result4.criteria, 
 163:  result4.metrics, 
 164:  result4.total_data, 
 165:  result4.good_data, 
 166:  result4.bad_data, 
 167:  cast(
 168:    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
 169:  ) as percentage_good_data, 
 170:  cast(
 171:    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
 172:  ) as percentage_bad_data, 
 173:  "event_name" field_name_checking 
 174:from 
 175:  (
 176:    select 
 177:      'Completeness' criteria, 
 178:      'field mandatory is null/blank event_name' metrics, 
 179:      count(ga_session_id) total_data, 
 180:      count(
 181:        case when ifnull(event_name, '')<> '' then 'pass' end
 182:      ) as good_data, 
 183:      count(
 184:        case when ifnull(event_name, '')= '' then 'fail' end
 185:      ) as bad_data 
 186:    from 
 187:      x
 188:  ) result4 
 189:union all 
 190:select 
 191:  datetime(
 192:    current_timestamp(), 
 193:    "Asia/Jakarta"
 194:  ) as time_execution, 
 195:  result5.criteria, 
 196:  result5.metrics, 
 197:  result5.total_data, 
 198:  result5.good_data, 
 199:  result5.bad_data, 
 200:  cast(
 201:    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
 202:  ) as percentage_good_data, 
 203:  cast(
 204:    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
 205:  ) as percentage_bad_data, 
 206:  "action_type_cleaned" field_name_checking 
 207:from 
 208:  (
 209:    select 
 210:      'Completeness' criteria, 
 211:      'field mandatory is null/blank action_type_cleaned' metrics, 
 212:      count(ga_session_id) total_data, 
 213:      count(
 214:        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
 215:      ) as good_data, 
 216:      count(
 217:        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
 218:      ) as bad_data 
 219:    from 
 220:      x
 221:  ) result5 
 222:union all 
 223:select 
 224:  datetime(
 225:    current_timestamp(), 
 226:    "Asia/Jakarta"
 227:  ) as time_execution, 
 228:  result6.criteria, 
 229:  result6.metrics, 
 230:  result6.total_data, 
 231:  result6.good_data, 
 232:  result6.bad_data, 
 233:  cast(
 234:    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
 235:  ) as percentage_good_data, 
 236:  cast(
 237:    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
 238:  ) as percentage_bad_data, 
 239:  "user_first_timestamp_gmt7" field_name_checking 
 240:from 
 241:  (
 242:    select 
 243:      'Completeness' criteria, 
 244:      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
 245:      count(ga_session_id) total_data, 
 246:      count(
 247:        case when ifnull(
 248:          cast(
 249:            user_first_timestamp_gmt7 as string
 250:          ), 
 251:          ''
 252:        )<> '' then 'pass' end
 253:      ) as good_data, 
 254:      count(
 255:        case when ifnull(
 256:          cast(
 257:            user_first_timestamp_gmt7 as string
 258:          ), 
 259:          ''
 260:        )= '' then 'fail' end
 261:      ) as bad_data 
 262:    from 
 263:      x
 264:  ) result6
 265:  );
 266:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 07:22:52.904164 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8024a995-2ae8-4904-a29d-93c6f315ce5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f398dd4c0a0>]}
2021-12-24 07:22:52.905019 (Thread-1): 14:22:52 | 1 of 3 ERROR creating table model dataset_dbt.dqa_completeness....... [ERROR in 3.92s]
2021-12-24 07:22:52.905435 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:22:52.905846 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:22:52.906388 (Thread-1): 14:22:52 | 2 of 3 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:22:52.907420 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:22:52.907798 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:22:52.912695 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:22:52.913316 (Thread-1): finished collecting timing info
2021-12-24 07:22:52.916158 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:22:52.920598 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:22:53.243493 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:22:53.244425 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:22:56.567741 (Thread-1): finished collecting timing info
2021-12-24 07:22:56.568188 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8024a995-2ae8-4904-a29d-93c6f315ce5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f398cc18ac0>]}
2021-12-24 07:22:56.568514 (Thread-1): 14:22:56 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 3.66s]
2021-12-24 07:22:56.568697 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:22:56.569329 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:22:56.569545 (Thread-1): 14:22:56 | 3 of 3 SKIP relation dataset_dbt.view_models_dqa..................... [SKIP]
2021-12-24 07:22:56.569699 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:22:56.570469 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:22:56.570801 (MainThread): 14:22:56 | 
2021-12-24 07:22:56.570945 (MainThread): 14:22:56 | Finished running 2 table models, 1 view model in 8.45s.
2021-12-24 07:22:56.571107 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:22:56.571238 (MainThread): Connection 'model.my_new_project.dqa_uniqueness' was properly closed.
2021-12-24 07:22:56.574506 (MainThread): 
2021-12-24 07:22:56.574777 (MainThread): Completed with 1 error and 0 warnings:
2021-12-24 07:22:56.574956 (MainThread): 
2021-12-24 07:22:56.575136 (MainThread): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
2021-12-24 07:22:56.575293 (MainThread):   No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
2021-12-24 07:22:56.575438 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 07:22:56.575619 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
2021-12-24 07:22:56.575859 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3991503ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f398dd57c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f398ccc94f0>]}
2021-12-24 07:22:56.576073 (MainThread): Flushing usage events
2021-12-24 07:37:14.440782 (MainThread): Running with dbt=0.21.0
2021-12-24 07:37:14.655168 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:37:14.655938 (MainThread): Tracking: tracking
2021-12-24 07:37:14.662083 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d09beb910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d07e855e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d07e85640>]}
2021-12-24 07:37:14.668500 (MainThread): Partial parsing not enabled
2021-12-24 07:37:14.672962 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:37:14.676936 (MainThread): Parsing macros/etc.sql
2021-12-24 07:37:14.678148 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:37:14.691357 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:37:14.693069 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:37:14.694726 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:37:14.696417 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:37:14.697398 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:37:14.702966 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:37:14.711498 (MainThread): Parsing macros/core.sql
2021-12-24 07:37:14.713704 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:37:14.714692 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:37:14.716092 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:37:14.716827 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:37:14.717623 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:37:14.721884 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:37:14.727410 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:37:14.740751 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:37:14.745004 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:37:14.747409 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:37:14.758889 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:37:14.759998 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:37:14.766923 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:37:14.775044 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:37:14.779274 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:37:14.789075 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:37:14.790081 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:37:14.808038 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:37:14.808985 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:37:14.809930 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:37:14.810808 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:37:14.811428 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:37:14.812847 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:37:14.813873 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:37:14.818854 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:37:14.965071 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:37:14.972916 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:37:14.975312 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:37:14.990522 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:37:14.991681 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:37:14.992678 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:37:14.993643 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:37:15.001915 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '839e2def-a156-4568-9d3f-79ef0a2a6b05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d07d0f0d0>]}
2021-12-24 07:37:15.005142 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:37:15.005468 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '839e2def-a156-4568-9d3f-79ef0a2a6b05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d07d0f070>]}
2021-12-24 07:37:15.005662 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:37:15.006609 (MainThread): 
2021-12-24 07:37:15.006847 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:37:15.007604 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:37:15.007848 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:37:15.726947 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:37:15.727625 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:37:15.734555 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:37:16.092098 (MainThread): 14:37:16 | Concurrency: 1 threads (target='prod')
2021-12-24 07:37:16.092646 (MainThread): 14:37:16 | 
2021-12-24 07:37:16.097076 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:37:16.098166 (Thread-1): 14:37:16 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:37:16.099233 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:37:16.099882 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:37:16.102877 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:37:16.103179 (Thread-1): finished collecting timing info
2021-12-24 07:37:16.104811 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 53466), raddr=('142.251.12.95', 443)>
2021-12-24 07:37:16.104990 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57020), raddr=('34.101.5.42', 443)>
2021-12-24 07:37:16.105121 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 53470), raddr=('142.251.12.95', 443)>
2021-12-24 07:37:16.105216 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57024), raddr=('34.101.5.42', 443)>
2021-12-24 07:37:16.115794 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:37:16.119033 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:37:16.479564 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:37:16.479897 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6
  );
    
2021-12-24 07:37:18.657587 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]')
2021-12-24 07:37:20.180892 (Thread-1): finished collecting timing info
2021-12-24 07:37:20.181848 (Thread-1): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]

(job ID: b13ce93f-5f2c-4717-a1e2-6d768ba4476e)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    /*
  10:    Welcome to your first dbt model!
  11:    Did you know that you can also configure models directly within SQL files?
  12:    This will override configurations stated in dbt_project.yml
  13:
  14:    Try changing "table" to "view" below
  15:*/
  16:
  17:
  18:
  19:with x as (
  20:  select 
  21:    ga_session_id, 
  22:    user_id, 
  23:    user_pseudo_id, 
  24:    event_name, 
  25:    action_type_cleaned, 
  26:    user_first_timestamp_gmt7, 
  27:    timestamp_gmt7, 
  28:    date_gmt7, 
  29:    time_gmt7, 
  30:    hour_gmt7, 
  31:    device_category_cleaned, 
  32:    engagement_time_msec, 
  33:    material_id, 
  34:    module_id, 
  35:    video_id, 
  36:    video_inspirasi_id, 
  37:    video_inspirasi_playslist_id, 
  38:    quiz_id, 
  39:    text_id, 
  40:    open_reflection_id, 
  41:    save_reflection_id, 
  42:    post_test_result, 
  43:    event_params_source, 
  44:    event_params_page_location, 
  45:    event_params_page_referrer, 
  46:    event_params_page_path, 
  47:    event_params_page_title, 
  48:    event_params_product 
  49:  FROM 
  50:    `playground-325606.dataset_dbt.event_tracker_belajar`
  51:) 
  52:select 
  53:  datetime(
  54:    current_timestamp(), 
  55:    "Asia/Jakarta"
  56:  ) as time_execution, 
  57:  result1.criteria, 
  58:  result1.metrics, 
  59:  result1.total_data, 
  60:  result1.good_data, 
  61:  result1.bad_data, 
  62:  cast(
  63:    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  64:  ) as percentage_good_data, 
  65:  cast(
  66:    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  67:  ) as percentage_bad_data, 
  68:  "ga_session_id" field_name_checking 
  69:from 
  70:  (
  71:    select 
  72:      'Completeness' criteria, 
  73:      'field mandatory is null/blank ga_session_id' metrics, 
  74:      count(ga_session_id) total_data, 
  75:      count(
  76:        case when ifnull(
  77:          cast(ga_session_id as string), 
  78:          ''
  79:        )<> '' then 'pass' end
  80:      ) as good_data, 
  81:      count(
  82:        case when ifnull(
  83:          cast(ga_session_id as string), 
  84:          ''
  85:        )= '' then 'fail' end
  86:      ) as bad_data 
  87:    from 
  88:      x
  89:  ) result1 
  90:union all 
  91:select 
  92:  datetime(
  93:    current_timestamp(), 
  94:    "Asia/Jakarta"
  95:  ) as time_execution, 
  96:  result2.criteria, 
  97:  result2.metrics, 
  98:  result2.total_data, 
  99:  result2.good_data, 
 100:  result2.bad_data, 
 101:  cast(
 102:    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
 103:  ) as percentage_good_data, 
 104:  cast(
 105:    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
 106:  ) as percentage_bad_data, 
 107:  "user_id" field_name_checking 
 108:from 
 109:  (
 110:    select 
 111:      'Completeness' criteria, 
 112:      'field mandatory is null/blank user_id' metrics, 
 113:      count(ga_session_id) total_data, 
 114:      count(
 115:        case when ifnull(user_id, '')<> '' then 'pass' end
 116:      ) as good_data, 
 117:      count(
 118:        case when ifnull(user_id, '')= '' then 'fail' end
 119:      ) as bad_data 
 120:    from 
 121:      x
 122:  ) result2 
 123:union all 
 124:select 
 125:  datetime(
 126:    current_timestamp(), 
 127:    "Asia/Jakarta"
 128:  ) as time_execution, 
 129:  result3.criteria, 
 130:  result3.metrics, 
 131:  result3.total_data, 
 132:  result3.good_data, 
 133:  result3.bad_data, 
 134:  cast(
 135:    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
 136:  ) as percentage_good_data, 
 137:  cast(
 138:    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
 139:  ) as percentage_bad_data, 
 140:  "user_pseudo_id" field_name_checking 
 141:from 
 142:  (
 143:    select 
 144:      'Completeness' criteria, 
 145:      'field mandatory is null/blank user_pseudo_id' metrics, 
 146:      count(ga_session_id) total_data, 
 147:      count(
 148:        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
 149:      ) as good_data, 
 150:      count(
 151:        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
 152:      ) as bad_data 
 153:    from 
 154:      x
 155:  ) result3 
 156:union all 
 157:select 
 158:  datetime(
 159:    current_timestamp(), 
 160:    "Asia/Jakarta"
 161:  ) as time_execution, 
 162:  result4.criteria, 
 163:  result4.metrics, 
 164:  result4.total_data, 
 165:  result4.good_data, 
 166:  result4.bad_data, 
 167:  cast(
 168:    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
 169:  ) as percentage_good_data, 
 170:  cast(
 171:    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
 172:  ) as percentage_bad_data, 
 173:  "event_name" field_name_checking 
 174:from 
 175:  (
 176:    select 
 177:      'Completeness' criteria, 
 178:      'field mandatory is null/blank event_name' metrics, 
 179:      count(ga_session_id) total_data, 
 180:      count(
 181:        case when ifnull(event_name, '')<> '' then 'pass' end
 182:      ) as good_data, 
 183:      count(
 184:        case when ifnull(event_name, '')= '' then 'fail' end
 185:      ) as bad_data 
 186:    from 
 187:      x
 188:  ) result4 
 189:union all 
 190:select 
 191:  datetime(
 192:    current_timestamp(), 
 193:    "Asia/Jakarta"
 194:  ) as time_execution, 
 195:  result5.criteria, 
 196:  result5.metrics, 
 197:  result5.total_data, 
 198:  result5.good_data, 
 199:  result5.bad_data, 
 200:  cast(
 201:    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
 202:  ) as percentage_good_data, 
 203:  cast(
 204:    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
 205:  ) as percentage_bad_data, 
 206:  "action_type_cleaned" field_name_checking 
 207:from 
 208:  (
 209:    select 
 210:      'Completeness' criteria, 
 211:      'field mandatory is null/blank action_type_cleaned' metrics, 
 212:      count(ga_session_id) total_data, 
 213:      count(
 214:        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
 215:      ) as good_data, 
 216:      count(
 217:        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
 218:      ) as bad_data 
 219:    from 
 220:      x
 221:  ) result5 
 222:union all 
 223:select 
 224:  datetime(
 225:    current_timestamp(), 
 226:    "Asia/Jakarta"
 227:  ) as time_execution, 
 228:  result6.criteria, 
 229:  result6.metrics, 
 230:  result6.total_data, 
 231:  result6.good_data, 
 232:  result6.bad_data, 
 233:  cast(
 234:    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
 235:  ) as percentage_good_data, 
 236:  cast(
 237:    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
 238:  ) as percentage_bad_data, 
 239:  "user_first_timestamp_gmt7" field_name_checking 
 240:from 
 241:  (
 242:    select 
 243:      'Completeness' criteria, 
 244:      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
 245:      count(ga_session_id) total_data, 
 246:      count(
 247:        case when ifnull(
 248:          cast(
 249:            user_first_timestamp_gmt7 as string
 250:          ), 
 251:          ''
 252:        )<> '' then 'pass' end
 253:      ) as good_data, 
 254:      count(
 255:        case when ifnull(
 256:          cast(
 257:            user_first_timestamp_gmt7 as string
 258:          ), 
 259:          ''
 260:        )= '' then 'fail' end
 261:      ) as bad_data 
 262:    from 
 263:      x
 264:  ) result6
 265:  );
 266:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 07:37:20.195028 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '839e2def-a156-4568-9d3f-79ef0a2a6b05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d07ca1d30>]}
2021-12-24 07:37:20.195863 (Thread-1): 14:37:20 | 1 of 3 ERROR creating table model dataset_dbt.dqa_completeness....... [ERROR in 4.10s]
2021-12-24 07:37:20.196319 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:37:20.196763 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:37:20.197751 (Thread-1): 14:37:20 | 2 of 3 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:37:20.198557 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:37:20.199177 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:37:20.206190 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:37:20.206722 (Thread-1): finished collecting timing info
2021-12-24 07:37:20.208784 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:37:20.213446 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:37:20.865557 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:37:20.866335 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:37:24.213486 (Thread-1): finished collecting timing info
2021-12-24 07:37:24.214445 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '839e2def-a156-4568-9d3f-79ef0a2a6b05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d07d0d2b0>]}
2021-12-24 07:37:24.215248 (Thread-1): 14:37:24 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 4.02s]
2021-12-24 07:37:24.215694 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:37:24.217183 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:37:24.217685 (Thread-1): 14:37:24 | 3 of 3 SKIP relation dataset_dbt.view_models_dqa..................... [SKIP]
2021-12-24 07:37:24.218228 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:37:24.219965 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:37:24.220565 (MainThread): 14:37:24 | 
2021-12-24 07:37:24.220868 (MainThread): 14:37:24 | Finished running 2 table models, 1 view model in 9.21s.
2021-12-24 07:37:24.221124 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:37:24.221391 (MainThread): Connection 'model.my_new_project.dqa_uniqueness' was properly closed.
2021-12-24 07:37:24.226107 (MainThread): 
2021-12-24 07:37:24.226301 (MainThread): Completed with 1 error and 0 warnings:
2021-12-24 07:37:24.226499 (MainThread): 
2021-12-24 07:37:24.226712 (MainThread): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
2021-12-24 07:37:24.226907 (MainThread):   No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
2021-12-24 07:37:24.227091 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 07:37:24.227281 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
2021-12-24 07:37:24.227554 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d0b589a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d07d3cac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d07cb8d30>]}
2021-12-24 07:37:24.227812 (MainThread): Flushing usage events
2021-12-24 07:37:52.063141 (MainThread): Running with dbt=0.21.0
2021-12-24 07:37:52.216190 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:37:52.216782 (MainThread): Tracking: tracking
2021-12-24 07:37:52.222365 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f212428caf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2122525670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f21225256d0>]}
2021-12-24 07:37:52.228500 (MainThread): Partial parsing not enabled
2021-12-24 07:37:52.232378 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:37:52.236165 (MainThread): Parsing macros/etc.sql
2021-12-24 07:37:52.237316 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:37:52.249817 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:37:52.251391 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:37:52.253033 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:37:52.254688 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:37:52.255755 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:37:52.261312 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:37:52.270038 (MainThread): Parsing macros/core.sql
2021-12-24 07:37:52.272384 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:37:52.273638 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:37:52.275356 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:37:52.276447 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:37:52.277485 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:37:52.281789 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:37:52.287257 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:37:52.300418 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:37:52.304514 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:37:52.306823 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:37:52.317913 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:37:52.319010 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:37:52.325009 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:37:52.332755 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:37:52.336923 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:37:52.347284 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:37:52.348495 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:37:52.369371 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:37:52.370667 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:37:52.371942 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:37:52.373085 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:37:52.373787 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:37:52.375469 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:37:52.376800 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:37:52.382590 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:37:52.561623 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:37:52.570662 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:37:52.573810 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:37:52.593794 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:37:52.595167 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:37:52.596454 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:37:52.597739 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:37:52.609319 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1b74b3cd-150c-4e87-9e4d-7a3900d27607', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f21223b00d0>]}
2021-12-24 07:37:52.612887 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:37:52.613179 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1b74b3cd-150c-4e87-9e4d-7a3900d27607', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f21223b0220>]}
2021-12-24 07:37:52.613401 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:37:52.614472 (MainThread): 
2021-12-24 07:37:52.614739 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:37:52.615482 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:37:52.615704 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:37:53.125388 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:37:53.126133 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:37:53.137657 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:37:53.506920 (MainThread): 14:37:53 | Concurrency: 1 threads (target='prod')
2021-12-24 07:37:53.507460 (MainThread): 14:37:53 | 
2021-12-24 07:37:53.511470 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:37:53.512370 (Thread-1): 14:37:53 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:37:53.513411 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:37:53.513876 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:37:53.520476 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:37:53.521290 (Thread-1): finished collecting timing info
2021-12-24 07:37:53.526568 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 53488), raddr=('142.251.12.95', 443)>
2021-12-24 07:37:53.527175 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57042), raddr=('34.101.5.42', 443)>
2021-12-24 07:37:53.527651 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 53492), raddr=('142.251.12.95', 443)>
2021-12-24 07:37:53.527965 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57046), raddr=('34.101.5.42', 443)>
2021-12-24 07:37:53.540110 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:37:53.543262 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:37:53.945631 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:37:53.945939 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3
  );
    
2021-12-24 07:37:55.885330 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]')
2021-12-24 07:37:56.948072 (Thread-1): finished collecting timing info
2021-12-24 07:37:56.949054 (Thread-1): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]

(job ID: c6bcad68-be8c-46d6-87a3-e700100f5691)

                                                            -----Query Job SQL Follows-----                                                             

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    /*
  10:    Welcome to your first dbt model!
  11:    Did you know that you can also configure models directly within SQL files?
  12:    This will override configurations stated in dbt_project.yml
  13:
  14:    Try changing "table" to "view" below
  15:*/
  16:
  17:
  18:
  19:with x as (
  20:  select 
  21:    ga_session_id, 
  22:    user_id, 
  23:    user_pseudo_id, 
  24:    event_name, 
  25:    action_type_cleaned, 
  26:    user_first_timestamp_gmt7, 
  27:    timestamp_gmt7, 
  28:    date_gmt7, 
  29:    time_gmt7, 
  30:    hour_gmt7, 
  31:    device_category_cleaned, 
  32:    engagement_time_msec, 
  33:    material_id, 
  34:    module_id, 
  35:    video_id, 
  36:    video_inspirasi_id, 
  37:    video_inspirasi_playslist_id, 
  38:    quiz_id, 
  39:    text_id, 
  40:    open_reflection_id, 
  41:    save_reflection_id, 
  42:    post_test_result, 
  43:    event_params_source, 
  44:    event_params_page_location, 
  45:    event_params_page_referrer, 
  46:    event_params_page_path, 
  47:    event_params_page_title, 
  48:    event_params_product 
  49:  FROM 
  50:    `playground-325606.dataset_dbt.event_tracker_belajar`
  51:) 
  52:select 
  53:  datetime(
  54:    current_timestamp(), 
  55:    "Asia/Jakarta"
  56:  ) as time_execution, 
  57:  result1.criteria, 
  58:  result1.metrics, 
  59:  result1.total_data, 
  60:  result1.good_data, 
  61:  result1.bad_data, 
  62:  cast(
  63:    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  64:  ) as percentage_good_data, 
  65:  cast(
  66:    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  67:  ) as percentage_bad_data, 
  68:  "ga_session_id" field_name_checking 
  69:from 
  70:  (
  71:    select 
  72:      'Completeness' criteria, 
  73:      'field mandatory is null/blank ga_session_id' metrics, 
  74:      count(ga_session_id) total_data, 
  75:      count(
  76:        case when ifnull(
  77:          cast(ga_session_id as string), 
  78:          ''
  79:        )<> '' then 'pass' end
  80:      ) as good_data, 
  81:      count(
  82:        case when ifnull(
  83:          cast(ga_session_id as string), 
  84:          ''
  85:        )= '' then 'fail' end
  86:      ) as bad_data 
  87:    from 
  88:      x
  89:  ) result1 
  90:union all 
  91:select 
  92:  datetime(
  93:    current_timestamp(), 
  94:    "Asia/Jakarta"
  95:  ) as time_execution, 
  96:  result2.criteria, 
  97:  result2.metrics, 
  98:  result2.total_data, 
  99:  result2.good_data, 
 100:  result2.bad_data, 
 101:  cast(
 102:    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
 103:  ) as percentage_good_data, 
 104:  cast(
 105:    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
 106:  ) as percentage_bad_data, 
 107:  "user_id" field_name_checking 
 108:from 
 109:  (
 110:    select 
 111:      'Completeness' criteria, 
 112:      'field mandatory is null/blank user_id' metrics, 
 113:      count(ga_session_id) total_data, 
 114:      count(
 115:        case when ifnull(user_id, '')<> '' then 'pass' end
 116:      ) as good_data, 
 117:      count(
 118:        case when ifnull(user_id, '')= '' then 'fail' end
 119:      ) as bad_data 
 120:    from 
 121:      x
 122:  ) result2 
 123:union all 
 124:select 
 125:  datetime(
 126:    current_timestamp(), 
 127:    "Asia/Jakarta"
 128:  ) as time_execution, 
 129:  result3.criteria, 
 130:  result3.metrics, 
 131:  result3.total_data, 
 132:  result3.good_data, 
 133:  result3.bad_data, 
 134:  cast(
 135:    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
 136:  ) as percentage_good_data, 
 137:  cast(
 138:    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
 139:  ) as percentage_bad_data, 
 140:  "user_pseudo_id" field_name_checking 
 141:from 
 142:  (
 143:    select 
 144:      'Completeness' criteria, 
 145:      'field mandatory is null/blank user_pseudo_id' metrics, 
 146:      count(ga_session_id) total_data, 
 147:      count(
 148:        case when ifnull(user_pseudo_id, '')<> '' then 'pass' end
 149:      ) as good_data, 
 150:      count(
 151:        case when ifnull(user_pseudo_id, '')= '' then 'fail' end
 152:      ) as bad_data 
 153:    from 
 154:      x
 155:  ) result3
 156:  );
 157:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 07:37:56.953601 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1b74b3cd-150c-4e87-9e4d-7a3900d27607', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2122399af0>]}
2021-12-24 07:37:56.954386 (Thread-1): 14:37:56 | 1 of 3 ERROR creating table model dataset_dbt.dqa_completeness....... [ERROR in 3.44s]
2021-12-24 07:37:56.954793 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:37:56.955184 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:37:56.956148 (Thread-1): 14:37:56 | 2 of 3 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:37:56.956915 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:37:56.957330 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:37:56.962555 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:37:56.963270 (Thread-1): finished collecting timing info
2021-12-24 07:37:56.966390 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:37:56.975417 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:37:57.485044 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:37:57.485879 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:37:59.760447 (Thread-1): finished collecting timing info
2021-12-24 07:37:59.760732 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1b74b3cd-150c-4e87-9e4d-7a3900d27607', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2122336a90>]}
2021-12-24 07:37:59.760964 (Thread-1): 14:37:59 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 2.80s]
2021-12-24 07:37:59.761081 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:37:59.761707 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:37:59.761961 (Thread-1): 14:37:59 | 3 of 3 SKIP relation dataset_dbt.view_models_dqa..................... [SKIP]
2021-12-24 07:37:59.762112 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:37:59.762784 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:37:59.763067 (MainThread): 14:37:59 | 
2021-12-24 07:37:59.763243 (MainThread): 14:37:59 | Finished running 2 table models, 1 view model in 7.15s.
2021-12-24 07:37:59.763360 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:37:59.763445 (MainThread): Connection 'model.my_new_project.dqa_uniqueness' was properly closed.
2021-12-24 07:37:59.767474 (MainThread): 
2021-12-24 07:37:59.767759 (MainThread): Completed with 1 error and 0 warnings:
2021-12-24 07:37:59.767933 (MainThread): 
2021-12-24 07:37:59.768083 (MainThread): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
2021-12-24 07:37:59.768253 (MainThread):   No matching signature for function IFNULL for argument types: FLOAT64, STRING. Supported signature: IFNULL(ANY, ANY) at [148:19]
2021-12-24 07:37:59.768408 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 07:37:59.768635 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
2021-12-24 07:37:59.768933 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2125c23ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f212247b160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2122544a30>]}
2021-12-24 07:37:59.769248 (MainThread): Flushing usage events
2021-12-24 07:40:00.429521 (MainThread): Running with dbt=0.21.0
2021-12-24 07:40:00.581326 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:40:00.581996 (MainThread): Tracking: tracking
2021-12-24 07:40:00.587595 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb046a92880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb044ce6640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb044ce66a0>]}
2021-12-24 07:40:00.593788 (MainThread): Partial parsing not enabled
2021-12-24 07:40:00.597675 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:40:00.601652 (MainThread): Parsing macros/etc.sql
2021-12-24 07:40:00.602802 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:40:00.615873 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:40:00.617433 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:40:00.619016 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:40:00.620639 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:40:00.621720 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:40:00.627315 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:40:00.635902 (MainThread): Parsing macros/core.sql
2021-12-24 07:40:00.638065 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:40:00.639031 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:40:00.640398 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:40:00.641099 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:40:00.641889 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:40:00.646168 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:40:00.651681 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:40:00.664992 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:40:00.669124 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:40:00.671566 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:40:00.682893 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:40:00.683994 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:40:00.690185 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:40:00.698269 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:40:00.702583 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:40:00.712345 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:40:00.713339 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:40:00.731195 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:40:00.732176 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:40:00.733198 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:40:00.734063 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:40:00.734643 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:40:00.736146 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:40:00.737263 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:40:00.742240 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:40:00.889009 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:40:00.896621 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:40:00.898933 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:40:00.914347 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:40:00.915543 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:40:00.916580 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:40:00.917554 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:40:00.925994 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '062c11cb-8e68-4968-adff-4784529ea284', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb044bb70d0>]}
2021-12-24 07:40:00.928769 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:40:00.929024 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '062c11cb-8e68-4968-adff-4784529ea284', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb044bb7220>]}
2021-12-24 07:40:00.929204 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:40:00.930143 (MainThread): 
2021-12-24 07:40:00.930370 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:40:00.931004 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:40:00.931260 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:40:01.429118 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:40:01.429786 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:40:01.435924 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:40:01.781412 (MainThread): 14:40:01 | Concurrency: 1 threads (target='prod')
2021-12-24 07:40:01.781932 (MainThread): 14:40:01 | 
2021-12-24 07:40:01.786434 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:40:01.787203 (Thread-1): 14:40:01 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:40:01.788193 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:40:01.788670 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:40:01.795006 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:40:01.795716 (Thread-1): finished collecting timing info
2021-12-24 07:40:01.800251 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 53514), raddr=('142.251.12.95', 443)>
2021-12-24 07:40:01.800672 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57068), raddr=('34.101.5.42', 443)>
2021-12-24 07:40:01.800903 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 53518), raddr=('142.251.12.95', 443)>
2021-12-24 07:40:01.801106 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57072), raddr=('34.101.5.42', 443)>
2021-12-24 07:40:01.813515 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:40:01.816913 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:40:02.184963 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:40:02.185324 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3
  );
    
2021-12-24 07:40:05.565370 (Thread-1): finished collecting timing info
2021-12-24 07:40:05.566300 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '062c11cb-8e68-4968-adff-4784529ea284', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb044bb5160>]}
2021-12-24 07:40:05.567056 (Thread-1): 14:40:05 | 1 of 3 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (3.0 rows, 176.1 KB processed) in 3.78s]
2021-12-24 07:40:05.567462 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:40:05.567875 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:40:05.568627 (Thread-1): 14:40:05 | 2 of 3 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:40:05.569486 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:40:05.569869 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:40:05.573440 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:40:05.573827 (Thread-1): finished collecting timing info
2021-12-24 07:40:05.575461 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:40:05.579641 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:40:05.945648 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:40:05.946506 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:40:08.157097 (Thread-1): finished collecting timing info
2021-12-24 07:40:08.158079 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '062c11cb-8e68-4968-adff-4784529ea284', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb044245760>]}
2021-12-24 07:40:08.158915 (Thread-1): 14:40:08 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 2.59s]
2021-12-24 07:40:08.159360 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:40:08.160210 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:40:08.160419 (Thread-1): 14:40:08 | 3 of 3 START view model dataset_dbt.view_models_dqa.................. [RUN]
2021-12-24 07:40:08.160703 (Thread-1): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:40:08.160866 (Thread-1): Compiling model.my_new_project.view_models_dqa
2021-12-24 07:40:08.162855 (Thread-1): Writing injected SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 07:40:08.163170 (Thread-1): finished collecting timing info
2021-12-24 07:40:08.173752 (Thread-1): Writing runtime SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 07:40:08.174050 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:40:08.177450 (Thread-1): On model.my_new_project.view_models_dqa: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.view_models_dqa"} */


  create or replace view `playground-325606`.`dataset_dbt`.`view_models_dqa`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_completeness`;


2021-12-24 07:40:12.410531 (Thread-1): finished collecting timing info
2021-12-24 07:40:12.411730 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '062c11cb-8e68-4968-adff-4784529ea284', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb044bb5130>]}
2021-12-24 07:40:12.412831 (Thread-1): 14:40:12 | 3 of 3 OK created view model dataset_dbt.view_models_dqa............. [SCRIPT (404.0 Bytes processed) in 4.25s]
2021-12-24 07:40:12.413349 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:40:12.415788 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:40:12.416692 (MainThread): 14:40:12 | 
2021-12-24 07:40:12.417139 (MainThread): 14:40:12 | Finished running 2 table models, 1 view model in 11.49s.
2021-12-24 07:40:12.417610 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:40:12.418165 (MainThread): Connection 'model.my_new_project.view_models_dqa' was properly closed.
2021-12-24 07:40:12.420374 (MainThread): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 53530), raddr=('142.251.12.95', 443)>
2021-12-24 07:40:12.420680 (MainThread): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57084), raddr=('34.101.5.42', 443)>
2021-12-24 07:40:12.423425 (MainThread): 
2021-12-24 07:40:12.423672 (MainThread): Completed successfully
2021-12-24 07:40:12.423882 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2021-12-24 07:40:12.424132 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb044ce6940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb048430a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb044c27880>]}
2021-12-24 07:40:12.424407 (MainThread): Flushing usage events
2021-12-24 07:43:05.525751 (MainThread): Running with dbt=0.21.0
2021-12-24 07:43:05.703409 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:43:05.704149 (MainThread): Tracking: tracking
2021-12-24 07:43:05.710938 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f058f5a7280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f058d8406d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f058d840730>]}
2021-12-24 07:43:05.718482 (MainThread): Partial parsing not enabled
2021-12-24 07:43:05.723583 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:43:05.728627 (MainThread): Parsing macros/etc.sql
2021-12-24 07:43:05.730158 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:43:05.746196 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:43:05.748264 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:43:05.750314 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:43:05.752349 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:43:05.753653 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:43:05.760731 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:43:05.771632 (MainThread): Parsing macros/core.sql
2021-12-24 07:43:05.774441 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:43:05.775779 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:43:05.777438 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:43:05.778390 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:43:05.779455 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:43:05.784594 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:43:05.791581 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:43:05.807978 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:43:05.813185 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:43:05.816156 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:43:05.830215 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:43:05.831533 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:43:05.838895 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:43:05.848866 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:43:05.854107 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:43:05.866442 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:43:05.867808 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:43:05.889379 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:43:05.890567 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:43:05.891805 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:43:05.892923 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:43:05.893647 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:43:05.895360 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:43:05.896792 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:43:05.902987 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:43:06.088531 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:43:06.097990 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:43:06.101039 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:43:06.124084 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:43:06.125569 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:43:06.126837 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:43:06.128259 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:43:06.139536 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ea22813-6319-4810-8f2d-d34d835a23c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f058d6ca040>]}
2021-12-24 07:43:06.142910 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:43:06.143223 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ea22813-6319-4810-8f2d-d34d835a23c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f058d6ca220>]}
2021-12-24 07:43:06.143459 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:43:06.144586 (MainThread): 
2021-12-24 07:43:06.144877 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:43:06.145657 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:43:06.145866 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:43:11.835666 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:43:11.836462 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:43:11.842696 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:43:12.204891 (MainThread): 14:43:12 | Concurrency: 1 threads (target='prod')
2021-12-24 07:43:12.205389 (MainThread): 14:43:12 | 
2021-12-24 07:43:12.209359 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:43:12.210038 (Thread-1): 14:43:12 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:43:12.210792 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:43:12.211299 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:43:12.222893 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:43:12.223323 (Thread-1): finished collecting timing info
2021-12-24 07:43:12.225567 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 47872), raddr=('74.125.68.95', 443)>
2021-12-24 07:43:12.225832 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44544), raddr=('34.101.5.74', 443)>
2021-12-24 07:43:12.225951 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 47876), raddr=('74.125.68.95', 443)>
2021-12-24 07:43:12.226055 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44548), raddr=('34.101.5.74', 443)>
2021-12-24 07:43:12.236040 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:43:12.239796 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:43:12.663436 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:43:12.663751 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(time_gmt7, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(time_gmt7, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result21.criteria, 
  result21.metrics, 
  result21.total_data, 
  result21.good_data, 
  result21.bad_data, 
  cast(
    cast(
      result21.good_data * 100 as decimal
    )/ result21.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
  ) as percentage_bad_data, 
  "save_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank save_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'save_reflection'  
  ) result21 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result22.criteria, 
  result22.metrics, 
  result22.total_data, 
  result22.good_data, 
  result22.bad_data, 
  cast(
    cast(
      result22.good_data * 100 as decimal
    )/ result22.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
  ) as percentage_bad_data, 
  "post_test_result" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank post_test_result' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(post_test_result, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(post_test_result, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'finished_post_test'  
  ) result22 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result23.criteria, 
  result23.metrics, 
  result23.total_data, 
  result23.good_data, 
  result23.bad_data, 
  cast(
    cast(
      result23.good_data * 100 as decimal
    )/ result23.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_source" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_source' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_source, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_source, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result23 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result24.criteria, 
  result24.metrics, 
  result24.total_data, 
  result24.good_data, 
  result24.bad_data, 
  cast(
    cast(
      result24.good_data * 100 as decimal
    )/ result24.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_location" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_location' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_location, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result24 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result25.criteria, 
  result25.metrics, 
  result25.total_data, 
  result25.good_data, 
  result25.bad_data, 
  cast(
    cast(
      result25.good_data * 100 as decimal
    )/ result25.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_referrer" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_referrer' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result25 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result26.criteria, 
  result26.metrics, 
  result26.total_data, 
  result26.good_data, 
  result26.bad_data, 
  cast(
    cast(
      result26.good_data * 100 as decimal
    )/ result26.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_path" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_path' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_path, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result26 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result27.criteria, 
  result27.metrics, 
  result27.total_data, 
  result27.good_data, 
  result27.bad_data, 
  cast(
    cast(
      result27.good_data * 100 as decimal
    )/ result27.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_title" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_title' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_title, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result27 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result28.criteria, 
  result28.metrics, 
  result28.total_data, 
  result28.good_data, 
  result28.bad_data, 
  cast(
    cast(
      result28.good_data * 100 as decimal
    )/ result28.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_product" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_product' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_product, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_product, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result28
  );
    
2021-12-24 07:43:13.687676 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Could not cast literal "" to type TIME at [368:37]')
2021-12-24 07:43:15.247724 (Thread-1): finished collecting timing info
2021-12-24 07:43:15.248696 (Thread-1): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  Could not cast literal "" to type TIME at [368:37]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Could not cast literal "" to type TIME at [368:37]

(job ID: 8ed63536-3aa9-4ec1-b6c2-0f6e65e1db16)

                                                                               -----Query Job SQL Follows-----                                                                                

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    /*
  10:    Welcome to your first dbt model!
  11:    Did you know that you can also configure models directly within SQL files?
  12:    This will override configurations stated in dbt_project.yml
  13:
  14:    Try changing "table" to "view" below
  15:*/
  16:
  17:
  18:
  19:with x as (
  20:  select 
  21:    ga_session_id, 
  22:    user_id, 
  23:    user_pseudo_id, 
  24:    event_name, 
  25:    action_type_cleaned, 
  26:    user_first_timestamp_gmt7, 
  27:    timestamp_gmt7, 
  28:    date_gmt7, 
  29:    time_gmt7, 
  30:    hour_gmt7, 
  31:    device_category_cleaned, 
  32:    engagement_time_msec, 
  33:    material_id, 
  34:    module_id, 
  35:    video_id, 
  36:    video_inspirasi_id, 
  37:    video_inspirasi_playslist_id, 
  38:    quiz_id, 
  39:    text_id, 
  40:    open_reflection_id, 
  41:    save_reflection_id, 
  42:    post_test_result, 
  43:    event_params_source, 
  44:    event_params_page_location, 
  45:    event_params_page_referrer, 
  46:    event_params_page_path, 
  47:    event_params_page_title, 
  48:    event_params_product 
  49:  FROM 
  50:    `playground-325606.dataset_dbt.event_tracker_belajar`
  51:) 
  52:select 
  53:  datetime(
  54:    current_timestamp(), 
  55:    "Asia/Jakarta"
  56:  ) as time_execution, 
  57:  result1.criteria, 
  58:  result1.metrics, 
  59:  result1.total_data, 
  60:  result1.good_data, 
  61:  result1.bad_data, 
  62:  cast(
  63:    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  64:  ) as percentage_good_data, 
  65:  cast(
  66:    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  67:  ) as percentage_bad_data, 
  68:  "ga_session_id" field_name_checking 
  69:from 
  70:  (
  71:    select 
  72:      'Completeness' criteria, 
  73:      'field mandatory is null/blank ga_session_id' metrics, 
  74:      count(ga_session_id) total_data, 
  75:      count(
  76:        case when ifnull(
  77:          cast(ga_session_id as string), 
  78:          ''
  79:        )<> '' then 'pass' end
  80:      ) as good_data, 
  81:      count(
  82:        case when ifnull(
  83:          cast(ga_session_id as string), 
  84:          ''
  85:        )= '' then 'fail' end
  86:      ) as bad_data 
  87:    from 
  88:      x
  89:  ) result1 
  90:union all 
  91:select 
  92:  datetime(
  93:    current_timestamp(), 
  94:    "Asia/Jakarta"
  95:  ) as time_execution, 
  96:  result2.criteria, 
  97:  result2.metrics, 
  98:  result2.total_data, 
  99:  result2.good_data, 
 100:  result2.bad_data, 
 101:  cast(
 102:    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
 103:  ) as percentage_good_data, 
 104:  cast(
 105:    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
 106:  ) as percentage_bad_data, 
 107:  "user_id" field_name_checking 
 108:from 
 109:  (
 110:    select 
 111:      'Completeness' criteria, 
 112:      'field mandatory is null/blank user_id' metrics, 
 113:      count(ga_session_id) total_data, 
 114:      count(
 115:        case when ifnull(user_id, '')<> '' then 'pass' end
 116:      ) as good_data, 
 117:      count(
 118:        case when ifnull(user_id, '')= '' then 'fail' end
 119:      ) as bad_data 
 120:    from 
 121:      x
 122:  ) result2 
 123:union all 
 124:select 
 125:  datetime(
 126:    current_timestamp(), 
 127:    "Asia/Jakarta"
 128:  ) as time_execution, 
 129:  result3.criteria, 
 130:  result3.metrics, 
 131:  result3.total_data, 
 132:  result3.good_data, 
 133:  result3.bad_data, 
 134:  cast(
 135:    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
 136:  ) as percentage_good_data, 
 137:  cast(
 138:    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
 139:  ) as percentage_bad_data, 
 140:  "user_pseudo_id" field_name_checking 
 141:from 
 142:  (
 143:    select 
 144:      'Completeness' criteria, 
 145:      'field mandatory is null/blank user_pseudo_id' metrics, 
 146:      count(ga_session_id) total_data, 
 147:      count(
 148:        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
 149:      ) as good_data, 
 150:      count(
 151:        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
 152:      ) as bad_data 
 153:    from 
 154:      x
 155:  ) result3 
 156:union all 
 157:select 
 158:  datetime(
 159:    current_timestamp(), 
 160:    "Asia/Jakarta"
 161:  ) as time_execution, 
 162:  result4.criteria, 
 163:  result4.metrics, 
 164:  result4.total_data, 
 165:  result4.good_data, 
 166:  result4.bad_data, 
 167:  cast(
 168:    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
 169:  ) as percentage_good_data, 
 170:  cast(
 171:    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
 172:  ) as percentage_bad_data, 
 173:  "event_name" field_name_checking 
 174:from 
 175:  (
 176:    select 
 177:      'Completeness' criteria, 
 178:      'field mandatory is null/blank event_name' metrics, 
 179:      count(ga_session_id) total_data, 
 180:      count(
 181:        case when ifnull(event_name, '')<> '' then 'pass' end
 182:      ) as good_data, 
 183:      count(
 184:        case when ifnull(event_name, '')= '' then 'fail' end
 185:      ) as bad_data 
 186:    from 
 187:      x
 188:  ) result4 
 189:union all 
 190:select 
 191:  datetime(
 192:    current_timestamp(), 
 193:    "Asia/Jakarta"
 194:  ) as time_execution, 
 195:  result5.criteria, 
 196:  result5.metrics, 
 197:  result5.total_data, 
 198:  result5.good_data, 
 199:  result5.bad_data, 
 200:  cast(
 201:    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
 202:  ) as percentage_good_data, 
 203:  cast(
 204:    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
 205:  ) as percentage_bad_data, 
 206:  "action_type_cleaned" field_name_checking 
 207:from 
 208:  (
 209:    select 
 210:      'Completeness' criteria, 
 211:      'field mandatory is null/blank action_type_cleaned' metrics, 
 212:      count(ga_session_id) total_data, 
 213:      count(
 214:        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
 215:      ) as good_data, 
 216:      count(
 217:        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
 218:      ) as bad_data 
 219:    from 
 220:      x
 221:  ) result5 
 222:union all 
 223:select 
 224:  datetime(
 225:    current_timestamp(), 
 226:    "Asia/Jakarta"
 227:  ) as time_execution, 
 228:  result6.criteria, 
 229:  result6.metrics, 
 230:  result6.total_data, 
 231:  result6.good_data, 
 232:  result6.bad_data, 
 233:  cast(
 234:    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
 235:  ) as percentage_good_data, 
 236:  cast(
 237:    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
 238:  ) as percentage_bad_data, 
 239:  "user_first_timestamp_gmt7" field_name_checking 
 240:from 
 241:  (
 242:    select 
 243:      'Completeness' criteria, 
 244:      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
 245:      count(ga_session_id) total_data, 
 246:      count(
 247:        case when ifnull(
 248:          cast(
 249:            user_first_timestamp_gmt7 as string
 250:          ), 
 251:          ''
 252:        )<> '' then 'pass' end
 253:      ) as good_data, 
 254:      count(
 255:        case when ifnull(
 256:          cast(
 257:            user_first_timestamp_gmt7 as string
 258:          ), 
 259:          ''
 260:        )= '' then 'fail' end
 261:      ) as bad_data 
 262:    from 
 263:      x
 264:  ) result6 
 265:union all 
 266:select 
 267:  datetime(
 268:    current_timestamp(), 
 269:    "Asia/Jakarta"
 270:  ) as time_execution, 
 271:  result7.criteria, 
 272:  result7.metrics, 
 273:  result7.total_data, 
 274:  result7.good_data, 
 275:  result7.bad_data, 
 276:  cast(
 277:    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
 278:  ) as percentage_good_data, 
 279:  cast(
 280:    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
 281:  ) as percentage_bad_data, 
 282:  "timestamp_gmt7" field_name_checking 
 283:from 
 284:  (
 285:    select 
 286:      'Completeness' criteria, 
 287:      'field mandatory is null/blank timestamp_gmt7' metrics, 
 288:      count(ga_session_id) total_data, 
 289:      count(
 290:        case when ifnull(
 291:          cast(timestamp_gmt7 as string), 
 292:          ''
 293:        )<> '' then 'pass' end
 294:      ) as good_data, 
 295:      count(
 296:        case when ifnull(
 297:          cast(timestamp_gmt7 as string), 
 298:          ''
 299:        )= '' then 'fail' end
 300:      ) as bad_data 
 301:    from 
 302:      x
 303:  ) result7 
 304:union all 
 305:select 
 306:  datetime(
 307:    current_timestamp(), 
 308:    "Asia/Jakarta"
 309:  ) as time_execution, 
 310:  result8.criteria, 
 311:  result8.metrics, 
 312:  result8.total_data, 
 313:  result8.good_data, 
 314:  result8.bad_data, 
 315:  cast(
 316:    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
 317:  ) as percentage_good_data, 
 318:  cast(
 319:    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
 320:  ) as percentage_bad_data, 
 321:  "date_gmt7" field_name_checking 
 322:from 
 323:  (
 324:    select 
 325:      'Completeness' criteria, 
 326:      'field mandatory is null/blank date_gmt7' metrics, 
 327:      count(ga_session_id) total_data, 
 328:      count(
 329:        case when ifnull(
 330:          cast(date_gmt7 as string), 
 331:          ''
 332:        )<> '' then 'pass' end
 333:      ) as good_data, 
 334:      count(
 335:        case when ifnull(
 336:          cast(date_gmt7 as string), 
 337:          ''
 338:        )= '' then 'fail' end
 339:      ) as bad_data 
 340:    from 
 341:      x
 342:  ) result8 
 343:union all 
 344:select 
 345:  datetime(
 346:    current_timestamp(), 
 347:    "Asia/Jakarta"
 348:  ) as time_execution, 
 349:  result9.criteria, 
 350:  result9.metrics, 
 351:  result9.total_data, 
 352:  result9.good_data, 
 353:  result9.bad_data, 
 354:  cast(
 355:    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
 356:  ) as percentage_good_data, 
 357:  cast(
 358:    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
 359:  ) as percentage_bad_data, 
 360:  "time_gmt7" field_name_checking 
 361:from 
 362:  (
 363:    select 
 364:      'Completeness' criteria, 
 365:      'field mandatory is null/blank time_gmt7' metrics, 
 366:      count(ga_session_id) total_data, 
 367:      count(
 368:        case when ifnull(time_gmt7, '')<> '' then 'pass' end
 369:      ) as good_data, 
 370:      count(
 371:        case when ifnull(time_gmt7, '')= '' then 'fail' end
 372:      ) as bad_data 
 373:    from 
 374:      x
 375:  ) result9 
 376:union all 
 377:select 
 378:  datetime(
 379:    current_timestamp(), 
 380:    "Asia/Jakarta"
 381:  ) as time_execution, 
 382:  result10.criteria, 
 383:  result10.metrics, 
 384:  result10.total_data, 
 385:  result10.good_data, 
 386:  result10.bad_data, 
 387:  cast(
 388:    cast(
 389:      result10.good_data * 100 as decimal
 390:    )/ result10.total_data as numeric
 391:  ) as percentage_good_data, 
 392:  cast(
 393:    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
 394:  ) as percentage_bad_data, 
 395:  "hour_gmt7" field_name_checking 
 396:from 
 397:  (
 398:    select 
 399:      'Completeness' criteria, 
 400:      'field mandatory is null/blank hour_gmt7' metrics, 
 401:      count(ga_session_id) total_data, 
 402:      count(
 403:        case when ifnull(
 404:          cast(hour_gmt7 as string), 
 405:          ''
 406:        )<> '' then 'pass' end
 407:      ) as good_data, 
 408:      count(
 409:        case when ifnull(
 410:          cast(hour_gmt7 as string), 
 411:          ''
 412:        )= '' then 'fail' end
 413:      ) as bad_data 
 414:    from 
 415:      x
 416:  ) result10 
 417:union all 
 418:select 
 419:  datetime(
 420:    current_timestamp(), 
 421:    "Asia/Jakarta"
 422:  ) as time_execution, 
 423:  result11.criteria, 
 424:  result11.metrics, 
 425:  result11.total_data, 
 426:  result11.good_data, 
 427:  result11.bad_data, 
 428:  cast(
 429:    cast(
 430:      result11.good_data * 100 as decimal
 431:    )/ result11.total_data as numeric
 432:  ) as percentage_good_data, 
 433:  cast(
 434:    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
 435:  ) as percentage_bad_data, 
 436:  "device_category_cleaned" field_name_checking 
 437:from 
 438:  (
 439:    select 
 440:      'Completeness' criteria, 
 441:      'field mandatory is null/blank device_category_cleaned' metrics, 
 442:      count(ga_session_id) total_data, 
 443:      count(
 444:        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
 445:      ) as good_data, 
 446:      count(
 447:        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
 448:      ) as bad_data 
 449:    from 
 450:      x
 451:  ) result11 
 452:union all 
 453:select 
 454:  datetime(
 455:    current_timestamp(), 
 456:    "Asia/Jakarta"
 457:  ) as time_execution, 
 458:  result12.criteria, 
 459:  result12.metrics, 
 460:  result12.total_data, 
 461:  result12.good_data, 
 462:  result12.bad_data, 
 463:  cast(
 464:    cast(
 465:      result12.good_data * 100 as decimal
 466:    )/ result12.total_data as numeric
 467:  ) as percentage_good_data, 
 468:  cast(
 469:    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
 470:  ) as percentage_bad_data, 
 471:  "engagement_time_msec" field_name_checking 
 472:from 
 473:  (
 474:    select 
 475:      'Completeness' criteria, 
 476:      'field mandatory is null/blank engagement_time_msec' metrics, 
 477:      count(ga_session_id) total_data, 
 478:      count(
 479:        case when ifnull(
 480:          cast(engagement_time_msec as string), 
 481:          ''
 482:        )<> '' then 'pass' end
 483:      ) as good_data, 
 484:      count(
 485:        case when ifnull(
 486:          cast(engagement_time_msec as string), 
 487:          ''
 488:        )= '' then 'fail' end
 489:      ) as bad_data 
 490:    from 
 491:      x
 492:  ) result12 
 493:union all 
 494:select 
 495:  datetime(
 496:    current_timestamp(), 
 497:    "Asia/Jakarta"
 498:  ) as time_execution, 
 499:  result13.criteria, 
 500:  result13.metrics, 
 501:  result13.total_data, 
 502:  result13.good_data, 
 503:  result13.bad_data, 
 504:  cast(
 505:    cast(
 506:      result13.good_data * 100 as decimal
 507:    )/ result13.total_data as numeric
 508:  ) as percentage_good_data, 
 509:  cast(
 510:    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
 511:  ) as percentage_bad_data, 
 512:  "material_id" field_name_checking 
 513:from 
 514:  (
 515:    select 
 516:      'Completeness' criteria, 
 517:      'field mandatory is null/blank material_id' metrics, 
 518:      count(ga_session_id) total_data, 
 519:      count(
 520:        case when ifnull(
 521:          cast(material_id as string), 
 522:          ''
 523:        )<> '' then 'pass' end
 524:      ) as good_data, 
 525:      count(
 526:        case when ifnull(
 527:          cast(material_id as string), 
 528:          ''
 529:        )= '' then 'fail' end
 530:      ) as bad_data 
 531:    from 
 532:      x
 533:    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
 534:  ) result13 
 535:union all 
 536:select 
 537:  datetime(
 538:    current_timestamp(), 
 539:    "Asia/Jakarta"
 540:  ) as time_execution, 
 541:  result14.criteria, 
 542:  result14.metrics, 
 543:  result14.total_data, 
 544:  result14.good_data, 
 545:  result14.bad_data, 
 546:  cast(
 547:    cast(
 548:      result14.good_data * 100 as decimal
 549:    )/ result14.total_data as numeric
 550:  ) as percentage_good_data, 
 551:  cast(
 552:    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
 553:  ) as percentage_bad_data, 
 554:  "module_id" field_name_checking 
 555:from 
 556:  (
 557:    select 
 558:      'Completeness' criteria, 
 559:      'field mandatory is null/blank module_id' metrics, 
 560:      count(ga_session_id) total_data, 
 561:      count(
 562:        case when ifnull(
 563:          cast(module_id as string), 
 564:          ''
 565:        )<> '' then 'pass' end
 566:      ) as good_data, 
 567:      count(
 568:        case when ifnull(
 569:          cast(module_id as string), 
 570:          ''
 571:        )= '' then 'fail' end
 572:      ) as bad_data 
 573:    from 
 574:      x
 575:    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
 576:  ) result14 
 577:union all 
 578:select 
 579:  datetime(
 580:    current_timestamp(), 
 581:    "Asia/Jakarta"
 582:  ) as time_execution, 
 583:  result15.criteria, 
 584:  result15.metrics, 
 585:  result15.total_data, 
 586:  result15.good_data, 
 587:  result15.bad_data, 
 588:  cast(
 589:    cast(
 590:      result15.good_data * 100 as decimal
 591:    )/ result15.total_data as numeric
 592:  ) as percentage_good_data, 
 593:  cast(
 594:    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
 595:  ) as percentage_bad_data, 
 596:  "video_id" field_name_checking 
 597:from 
 598:  (
 599:    select 
 600:      'Completeness' criteria, 
 601:      'field mandatory is null/blank video_id' metrics, 
 602:      count(ga_session_id) total_data, 
 603:      count(
 604:        case when ifnull(
 605:          cast(video_id as string), 
 606:          ''
 607:        )<> '' then 'pass' end
 608:      ) as good_data, 
 609:      count(
 610:        case when ifnull(
 611:          cast(video_id as string), 
 612:          ''
 613:        )= '' then 'fail' end
 614:      ) as bad_data 
 615:    from 
 616:      x
 617:    where action_type_cleaned = "play_video"  
 618:  ) result15 
 619:union all 
 620:select 
 621:  datetime(
 622:    current_timestamp(), 
 623:    "Asia/Jakarta"
 624:  ) as time_execution, 
 625:  result16.criteria, 
 626:  result16.metrics, 
 627:  result16.total_data, 
 628:  result16.good_data, 
 629:  result16.bad_data, 
 630:  cast(
 631:    cast(
 632:      result16.good_data * 100 as decimal
 633:    )/ result16.total_data as numeric
 634:  ) as percentage_good_data, 
 635:  cast(
 636:    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
 637:  ) as percentage_bad_data, 
 638:  "video_inspirasi_id" field_name_checking 
 639:from 
 640:  (
 641:    select 
 642:      'Completeness' criteria, 
 643:      'field mandatory is null/blank video_inspirasi_id' metrics, 
 644:      count(ga_session_id) total_data, 
 645:      count(
 646:        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
 647:      ) as good_data, 
 648:      count(
 649:        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
 650:      ) as bad_data 
 651:    from 
 652:      x
 653:    where action_type_cleaned = "play_video_inspirasi"  
 654:  ) result16 
 655:union all 
 656:select 
 657:  datetime(
 658:    current_timestamp(), 
 659:    "Asia/Jakarta"
 660:  ) as time_execution, 
 661:  result17.criteria, 
 662:  result17.metrics, 
 663:  result17.total_data, 
 664:  result17.good_data, 
 665:  result17.bad_data, 
 666:  cast(
 667:    cast(
 668:      result17.good_data * 100 as decimal
 669:    )/ result17.total_data as numeric
 670:  ) as percentage_good_data, 
 671:  cast(
 672:    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
 673:  ) as percentage_bad_data, 
 674:  "video_inspirasi_playslist_id" field_name_checking 
 675:from 
 676:  (
 677:    select 
 678:      'Completeness' criteria, 
 679:      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
 680:      count(ga_session_id) total_data, 
 681:      count(
 682:        case when ifnull(
 683:          cast(
 684:            video_inspirasi_playslist_id as string
 685:          ), 
 686:          ''
 687:        )<> '' then 'pass' end
 688:      ) as good_data, 
 689:      count(
 690:        case when ifnull(
 691:          cast(
 692:            video_inspirasi_playslist_id as string
 693:          ), 
 694:          ''
 695:        )= '' then 'fail' end
 696:      ) as bad_data 
 697:    from 
 698:      x
 699:    where action_type_cleaned = "play_video_inspirasi"  
 700:  ) result17 
 701:union all 
 702:select 
 703:  datetime(
 704:    current_timestamp(), 
 705:    "Asia/Jakarta"
 706:  ) as time_execution, 
 707:  result18.criteria, 
 708:  result18.metrics, 
 709:  result18.total_data, 
 710:  result18.good_data, 
 711:  result18.bad_data, 
 712:  cast(
 713:    cast(
 714:      result18.good_data * 100 as decimal
 715:    )/ result18.total_data as numeric
 716:  ) as percentage_good_data, 
 717:  cast(
 718:    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
 719:  ) as percentage_bad_data, 
 720:  "quiz_id" field_name_checking 
 721:from 
 722:  (
 723:    select 
 724:      'Completeness' criteria, 
 725:      'field mandatory is null/blank quiz_id' metrics, 
 726:      count(ga_session_id) total_data, 
 727:      count(
 728:        case when ifnull(
 729:          cast(quiz_id as string), 
 730:          ''
 731:        )<> '' then 'pass' end
 732:      ) as good_data, 
 733:      count(
 734:        case when ifnull(
 735:          cast(quiz_id as string), 
 736:          ''
 737:        )= '' then 'fail' end
 738:      ) as bad_data 
 739:    from 
 740:      x
 741:    where action_type_cleaned = 'open_quiz'  
 742:  ) result18 
 743:union all 
 744:select 
 745:  datetime(
 746:    current_timestamp(), 
 747:    "Asia/Jakarta"
 748:  ) as time_execution, 
 749:  result19.criteria, 
 750:  result19.metrics, 
 751:  result19.total_data, 
 752:  result19.good_data, 
 753:  result19.bad_data, 
 754:  cast(
 755:    cast(
 756:      result19.good_data * 100 as decimal
 757:    )/ result19.total_data as numeric
 758:  ) as percentage_good_data, 
 759:  cast(
 760:    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
 761:  ) as percentage_bad_data, 
 762:  "text_id" field_name_checking 
 763:from 
 764:  (
 765:    select 
 766:      'Completeness' criteria, 
 767:      'field mandatory is null/blank text_id' metrics, 
 768:      count(ga_session_id) total_data, 
 769:      count(
 770:        case when ifnull(
 771:          cast(text_id as string), 
 772:          ''
 773:        )<> '' then 'pass' end
 774:      ) as good_data, 
 775:      count(
 776:        case when ifnull(
 777:          cast(text_id as string), 
 778:          ''
 779:        )= '' then 'fail' end
 780:      ) as bad_data 
 781:    from 
 782:      x
 783:    where action_type_cleaned = 'open_text'  
 784:  ) result19 
 785:union all 
 786:select 
 787:  datetime(
 788:    current_timestamp(), 
 789:    "Asia/Jakarta"
 790:  ) as time_execution, 
 791:  result20.criteria, 
 792:  result20.metrics, 
 793:  result20.total_data, 
 794:  result20.good_data, 
 795:  result20.bad_data, 
 796:  cast(
 797:    cast(
 798:      result20.good_data * 100 as decimal
 799:    )/ result20.total_data as numeric
 800:  ) as percentage_good_data, 
 801:  cast(
 802:    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
 803:  ) as percentage_bad_data, 
 804:  "open_reflection_id" field_name_checking 
 805:from 
 806:  (
 807:    select 
 808:      'Completeness' criteria, 
 809:      'field mandatory is null/blank open_reflection_id' metrics, 
 810:      count(ga_session_id) total_data, 
 811:      count(
 812:        case when ifnull(
 813:          cast(open_reflection_id as string), 
 814:          ''
 815:        )<> '' then 'pass' end
 816:      ) as good_data, 
 817:      count(
 818:        case when ifnull(
 819:          cast(open_reflection_id as string), 
 820:          ''
 821:        )= '' then 'fail' end
 822:      ) as bad_data 
 823:    from 
 824:      x
 825:    where action_type_cleaned = "open_reflection"  
 826:  ) result20 
 827:union all 
 828:select 
 829:  datetime(
 830:    current_timestamp(), 
 831:    "Asia/Jakarta"
 832:  ) as time_execution, 
 833:  result21.criteria, 
 834:  result21.metrics, 
 835:  result21.total_data, 
 836:  result21.good_data, 
 837:  result21.bad_data, 
 838:  cast(
 839:    cast(
 840:      result21.good_data * 100 as decimal
 841:    )/ result21.total_data as numeric
 842:  ) as percentage_good_data, 
 843:  cast(
 844:    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
 845:  ) as percentage_bad_data, 
 846:  "save_reflection_id" field_name_checking 
 847:from 
 848:  (
 849:    select 
 850:      'Completeness' criteria, 
 851:      'field mandatory is null/blank save_reflection_id' metrics, 
 852:      count(ga_session_id) total_data, 
 853:      count(
 854:        case when ifnull(
 855:          cast(save_reflection_id as string), 
 856:          ''
 857:        )<> '' then 'pass' end
 858:      ) as good_data, 
 859:      count(
 860:        case when ifnull(
 861:          cast(save_reflection_id as string), 
 862:          ''
 863:        )= '' then 'fail' end
 864:      ) as bad_data 
 865:    from 
 866:      x
 867:    where action_type_cleaned = 'save_reflection'  
 868:  ) result21 
 869:union all 
 870:select 
 871:  datetime(
 872:    current_timestamp(), 
 873:    "Asia/Jakarta"
 874:  ) as time_execution, 
 875:  result22.criteria, 
 876:  result22.metrics, 
 877:  result22.total_data, 
 878:  result22.good_data, 
 879:  result22.bad_data, 
 880:  cast(
 881:    cast(
 882:      result22.good_data * 100 as decimal
 883:    )/ result22.total_data as numeric
 884:  ) as percentage_good_data, 
 885:  cast(
 886:    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
 887:  ) as percentage_bad_data, 
 888:  "post_test_result" field_name_checking 
 889:from 
 890:  (
 891:    select 
 892:      'Completeness' criteria, 
 893:      'field mandatory is null/blank post_test_result' metrics, 
 894:      count(ga_session_id) total_data, 
 895:      count(
 896:        case when ifnull(post_test_result, '')<> '' then 'pass' end
 897:      ) as good_data, 
 898:      count(
 899:        case when ifnull(post_test_result, '')= '' then 'fail' end
 900:      ) as bad_data 
 901:    from 
 902:      x
 903:    where action_type_cleaned = 'finished_post_test'  
 904:  ) result22 
 905:union all 
 906:select 
 907:  datetime(
 908:    current_timestamp(), 
 909:    "Asia/Jakarta"
 910:  ) as time_execution, 
 911:  result23.criteria, 
 912:  result23.metrics, 
 913:  result23.total_data, 
 914:  result23.good_data, 
 915:  result23.bad_data, 
 916:  cast(
 917:    cast(
 918:      result23.good_data * 100 as decimal
 919:    )/ result23.total_data as numeric
 920:  ) as percentage_good_data, 
 921:  cast(
 922:    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
 923:  ) as percentage_bad_data, 
 924:  "event_params_source" field_name_checking 
 925:from 
 926:  (
 927:    select 
 928:      'Completeness' criteria, 
 929:      'field mandatory is null/blank event_params_source' metrics, 
 930:      count(ga_session_id) total_data, 
 931:      count(
 932:        case when ifnull(event_params_source, '')<> '' then 'pass' end
 933:      ) as good_data, 
 934:      count(
 935:        case when ifnull(event_params_source, '')= '' then 'fail' end
 936:      ) as bad_data 
 937:    from 
 938:      x
 939:  ) result23 
 940:union all 
 941:select 
 942:  datetime(
 943:    current_timestamp(), 
 944:    "Asia/Jakarta"
 945:  ) as time_execution, 
 946:  result24.criteria, 
 947:  result24.metrics, 
 948:  result24.total_data, 
 949:  result24.good_data, 
 950:  result24.bad_data, 
 951:  cast(
 952:    cast(
 953:      result24.good_data * 100 as decimal
 954:    )/ result24.total_data as numeric
 955:  ) as percentage_good_data, 
 956:  cast(
 957:    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
 958:  ) as percentage_bad_data, 
 959:  "event_params_page_location" field_name_checking 
 960:from 
 961:  (
 962:    select 
 963:      'Completeness' criteria, 
 964:      'field mandatory is null/blank event_params_page_location' metrics, 
 965:      count(ga_session_id) total_data, 
 966:      count(
 967:        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
 968:      ) as good_data, 
 969:      count(
 970:        case when ifnull(event_params_page_location, '')= '' then 'fail' end
 971:      ) as bad_data 
 972:    from 
 973:      x
 974:  ) result24 
 975:union all 
 976:select 
 977:  datetime(
 978:    current_timestamp(), 
 979:    "Asia/Jakarta"
 980:  ) as time_execution, 
 981:  result25.criteria, 
 982:  result25.metrics, 
 983:  result25.total_data, 
 984:  result25.good_data, 
 985:  result25.bad_data, 
 986:  cast(
 987:    cast(
 988:      result25.good_data * 100 as decimal
 989:    )/ result25.total_data as numeric
 990:  ) as percentage_good_data, 
 991:  cast(
 992:    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
 993:  ) as percentage_bad_data, 
 994:  "event_params_page_referrer" field_name_checking 
 995:from 
 996:  (
 997:    select 
 998:      'Completeness' criteria, 
 999:      'field mandatory is null/blank event_params_page_referrer' metrics, 
1000:      count(ga_session_id) total_data, 
1001:      count(
1002:        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
1003:      ) as good_data, 
1004:      count(
1005:        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
1006:      ) as bad_data 
1007:    from 
1008:      x
1009:  ) result25 
1010:union all 
1011:select 
1012:  datetime(
1013:    current_timestamp(), 
1014:    "Asia/Jakarta"
1015:  ) as time_execution, 
1016:  result26.criteria, 
1017:  result26.metrics, 
1018:  result26.total_data, 
1019:  result26.good_data, 
1020:  result26.bad_data, 
1021:  cast(
1022:    cast(
1023:      result26.good_data * 100 as decimal
1024:    )/ result26.total_data as numeric
1025:  ) as percentage_good_data, 
1026:  cast(
1027:    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
1028:  ) as percentage_bad_data, 
1029:  "event_params_page_path" field_name_checking 
1030:from 
1031:  (
1032:    select 
1033:      'Completeness' criteria, 
1034:      'field mandatory is null/blank event_params_page_path' metrics, 
1035:      count(ga_session_id) total_data, 
1036:      count(
1037:        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
1038:      ) as good_data, 
1039:      count(
1040:        case when ifnull(event_params_page_path, '')= '' then 'fail' end
1041:      ) as bad_data 
1042:    from 
1043:      x
1044:  ) result26 
1045:union all 
1046:select 
1047:  datetime(
1048:    current_timestamp(), 
1049:    "Asia/Jakarta"
1050:  ) as time_execution, 
1051:  result27.criteria, 
1052:  result27.metrics, 
1053:  result27.total_data, 
1054:  result27.good_data, 
1055:  result27.bad_data, 
1056:  cast(
1057:    cast(
1058:      result27.good_data * 100 as decimal
1059:    )/ result27.total_data as numeric
1060:  ) as percentage_good_data, 
1061:  cast(
1062:    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
1063:  ) as percentage_bad_data, 
1064:  "event_params_page_title" field_name_checking 
1065:from 
1066:  (
1067:    select 
1068:      'Completeness' criteria, 
1069:      'field mandatory is null/blank event_params_page_title' metrics, 
1070:      count(ga_session_id) total_data, 
1071:      count(
1072:        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
1073:      ) as good_data, 
1074:      count(
1075:        case when ifnull(event_params_page_title, '')= '' then 'fail' end
1076:      ) as bad_data 
1077:    from 
1078:      x
1079:  ) result27 
1080:union all 
1081:select 
1082:  datetime(
1083:    current_timestamp(), 
1084:    "Asia/Jakarta"
1085:  ) as time_execution, 
1086:  result28.criteria, 
1087:  result28.metrics, 
1088:  result28.total_data, 
1089:  result28.good_data, 
1090:  result28.bad_data, 
1091:  cast(
1092:    cast(
1093:      result28.good_data * 100 as decimal
1094:    )/ result28.total_data as numeric
1095:  ) as percentage_good_data, 
1096:  cast(
1097:    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
1098:  ) as percentage_bad_data, 
1099:  "event_params_product" field_name_checking 
1100:from 
1101:  (
1102:    select 
1103:      'Completeness' criteria, 
1104:      'field mandatory is null/blank event_params_product' metrics, 
1105:      count(ga_session_id) total_data, 
1106:      count(
1107:        case when ifnull(event_params_product, '')<> '' then 'pass' end
1108:      ) as good_data, 
1109:      count(
1110:        case when ifnull(event_params_product, '')= '' then 'fail' end
1111:      ) as bad_data 
1112:    from 
1113:      x
1114:  ) result28
1115:  );
1116:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
  Could not cast literal "" to type TIME at [368:37]
  compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 07:43:15.254084 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ea22813-6319-4810-8f2d-d34d835a23c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f058c590250>]}
2021-12-24 07:43:15.254962 (Thread-1): 14:43:15 | 1 of 3 ERROR creating table model dataset_dbt.dqa_completeness....... [ERROR in 3.04s]
2021-12-24 07:43:15.255433 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:43:15.255965 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:43:15.256914 (Thread-1): 14:43:15 | 2 of 3 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:43:15.257586 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:43:15.257925 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:43:15.263010 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:43:15.263771 (Thread-1): finished collecting timing info
2021-12-24 07:43:15.266896 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:43:15.273062 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:43:15.628516 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:43:15.629359 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:43:17.949311 (Thread-1): finished collecting timing info
2021-12-24 07:43:17.950341 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ea22813-6319-4810-8f2d-d34d835a23c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f058d6b3cd0>]}
2021-12-24 07:43:17.951112 (Thread-1): 14:43:17 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 2.69s]
2021-12-24 07:43:17.951531 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:43:17.952793 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:43:17.953524 (Thread-1): 14:43:17 | 3 of 3 SKIP relation dataset_dbt.view_models_dqa..................... [SKIP]
2021-12-24 07:43:17.954000 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:43:17.955459 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:43:17.956071 (MainThread): 14:43:17 | 
2021-12-24 07:43:17.956381 (MainThread): 14:43:17 | Finished running 2 table models, 1 view model in 11.81s.
2021-12-24 07:43:17.956649 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:43:17.956902 (MainThread): Connection 'model.my_new_project.dqa_uniqueness' was properly closed.
2021-12-24 07:43:17.962046 (MainThread): 
2021-12-24 07:43:17.962238 (MainThread): Completed with 1 error and 0 warnings:
2021-12-24 07:43:17.962341 (MainThread): 
2021-12-24 07:43:17.962436 (MainThread): Database Error in model dqa_completeness (models/example/dqa_completeness.sql)
2021-12-24 07:43:17.962537 (MainThread):   Could not cast literal "" to type TIME at [368:37]
2021-12-24 07:43:17.962660 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_completeness.sql
2021-12-24 07:43:17.962781 (MainThread): 
Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
2021-12-24 07:43:17.962959 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0590f3db50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f058d85fa30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f058d7d27f0>]}
2021-12-24 07:43:17.963146 (MainThread): Flushing usage events
2021-12-24 07:44:32.752338 (MainThread): Running with dbt=0.21.0
2021-12-24 07:44:32.921988 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:44:32.922570 (MainThread): Tracking: tracking
2021-12-24 07:44:32.928025 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6337859280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6335aa86d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6335aa8730>]}
2021-12-24 07:44:32.934570 (MainThread): Partial parsing not enabled
2021-12-24 07:44:32.938820 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:44:32.942752 (MainThread): Parsing macros/etc.sql
2021-12-24 07:44:32.944061 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:44:32.956834 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:44:32.958388 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:44:32.959964 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:44:32.961634 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:44:32.962855 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:44:32.968808 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:44:32.976966 (MainThread): Parsing macros/core.sql
2021-12-24 07:44:32.979219 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:44:32.980247 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:44:32.981556 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:44:32.982256 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:44:32.983050 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:44:32.987243 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:44:32.992665 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:44:33.005913 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:44:33.010233 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:44:33.012862 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:44:33.024341 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:44:33.025385 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:44:33.031246 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:44:33.039191 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:44:33.043361 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:44:33.053261 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:44:33.054261 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:44:33.071994 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:44:33.072906 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:44:33.073831 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:44:33.074710 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:44:33.075322 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:44:33.076746 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:44:33.077847 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:44:33.082724 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:44:33.227181 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:44:33.234776 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:44:33.237149 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:44:33.252816 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:44:33.253941 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:44:33.255011 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:44:33.256074 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:44:33.264499 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2439375c-57ce-4d02-b7ce-b2bf5d8fb319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633597d190>]}
2021-12-24 07:44:33.267692 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:44:33.268002 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2439375c-57ce-4d02-b7ce-b2bf5d8fb319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633597d1f0>]}
2021-12-24 07:44:33.268282 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:44:33.269229 (MainThread): 
2021-12-24 07:44:33.269477 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:44:33.270133 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:44:33.270294 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:44:33.746363 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:44:33.746548 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:44:33.752229 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:44:34.209543 (MainThread): 14:44:34 | Concurrency: 1 threads (target='prod')
2021-12-24 07:44:34.210099 (MainThread): 14:44:34 | 
2021-12-24 07:44:34.214421 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:44:34.215554 (Thread-1): 14:44:34 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:44:34.216727 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:44:34.217372 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:44:34.225136 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:44:34.226014 (Thread-1): finished collecting timing info
2021-12-24 07:44:34.230610 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 47894), raddr=('74.125.68.95', 443)>
2021-12-24 07:44:34.231072 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44566), raddr=('34.101.5.74', 443)>
2021-12-24 07:44:34.231316 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 47898), raddr=('74.125.68.95', 443)>
2021-12-24 07:44:34.231569 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44570), raddr=('34.101.5.74', 443)>
2021-12-24 07:44:34.243028 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:44:34.246517 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:44:34.598940 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:44:34.599258 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9
  );
    
2021-12-24 07:44:37.280158 (Thread-1): finished collecting timing info
2021-12-24 07:44:37.280655 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2439375c-57ce-4d02-b7ce-b2bf5d8fb319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633590c940>]}
2021-12-24 07:44:37.281050 (Thread-1): 14:44:37 | 1 of 3 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (9.0 rows, 687.6 KB processed) in 3.06s]
2021-12-24 07:44:37.281244 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:44:37.281500 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:44:37.281907 (Thread-1): 14:44:37 | 2 of 3 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:44:37.282260 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:44:37.282431 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:44:37.284452 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:44:37.284784 (Thread-1): finished collecting timing info
2021-12-24 07:44:37.286501 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:44:37.290485 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:44:37.619477 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:44:37.620332 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:44:39.696540 (Thread-1): finished collecting timing info
2021-12-24 07:44:39.697539 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2439375c-57ce-4d02-b7ce-b2bf5d8fb319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633401cb80>]}
2021-12-24 07:44:39.698369 (Thread-1): 14:44:39 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 2.42s]
2021-12-24 07:44:39.698811 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:44:39.700308 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:44:39.701028 (Thread-1): 14:44:39 | 3 of 3 START view model dataset_dbt.view_models_dqa.................. [RUN]
2021-12-24 07:44:39.701802 (Thread-1): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:44:39.702206 (Thread-1): Compiling model.my_new_project.view_models_dqa
2021-12-24 07:44:39.708557 (Thread-1): Writing injected SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 07:44:39.709459 (Thread-1): finished collecting timing info
2021-12-24 07:44:39.723487 (Thread-1): Writing runtime SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 07:44:39.723818 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:44:39.727303 (Thread-1): On model.my_new_project.view_models_dqa: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.view_models_dqa"} */


  create or replace view `playground-325606`.`dataset_dbt`.`view_models_dqa`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_completeness`;


2021-12-24 07:44:44.645363 (Thread-1): finished collecting timing info
2021-12-24 07:44:44.645768 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2439375c-57ce-4d02-b7ce-b2bf5d8fb319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633402b190>]}
2021-12-24 07:44:44.646032 (Thread-1): 14:44:44 | 3 of 3 OK created view model dataset_dbt.view_models_dqa............. [SCRIPT (1.2 KB processed) in 4.94s]
2021-12-24 07:44:44.646171 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:44:44.646903 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:44:44.647164 (MainThread): 14:44:44 | 
2021-12-24 07:44:44.647289 (MainThread): 14:44:44 | Finished running 2 table models, 1 view model in 11.38s.
2021-12-24 07:44:44.647392 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:44:44.647484 (MainThread): Connection 'model.my_new_project.view_models_dqa' was properly closed.
2021-12-24 07:44:44.649249 (MainThread): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 47910), raddr=('74.125.68.95', 443)>
2021-12-24 07:44:44.649754 (MainThread): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44582), raddr=('34.101.5.74', 443)>
2021-12-24 07:44:44.655889 (MainThread): 
2021-12-24 07:44:44.656180 (MainThread): Completed successfully
2021-12-24 07:44:44.656435 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2021-12-24 07:44:44.656674 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63359524f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6335ad2a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6335a9d5e0>]}
2021-12-24 07:44:44.656995 (MainThread): Flushing usage events
2021-12-24 07:47:58.812337 (MainThread): Running with dbt=0.21.0
2021-12-24 07:47:58.966835 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:47:58.967476 (MainThread): Tracking: tracking
2021-12-24 07:47:58.973060 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f335053ca00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f334e7d56a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f334e7d5700>]}
2021-12-24 07:47:58.979187 (MainThread): Partial parsing not enabled
2021-12-24 07:47:58.983234 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:47:58.987526 (MainThread): Parsing macros/etc.sql
2021-12-24 07:47:58.988735 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:47:59.001839 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:47:59.003380 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:47:59.005004 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:47:59.006512 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:47:59.007567 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:47:59.013367 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:47:59.022264 (MainThread): Parsing macros/core.sql
2021-12-24 07:47:59.024544 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:47:59.025603 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:47:59.027000 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:47:59.027753 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:47:59.028620 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:47:59.032980 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:47:59.038406 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:47:59.051818 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:47:59.055998 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:47:59.058280 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:47:59.069864 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:47:59.070903 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:47:59.077207 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:47:59.085358 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:47:59.089705 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:47:59.099683 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:47:59.100727 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:47:59.118685 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:47:59.119651 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:47:59.120609 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:47:59.121481 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:47:59.122104 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:47:59.123512 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:47:59.124577 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:47:59.129724 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:47:59.277291 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:47:59.285052 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:47:59.287450 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:47:59.304335 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:47:59.305533 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:47:59.306548 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:47:59.307694 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:47:59.316305 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '621c5928-b209-4ffc-a79f-cf90ceca66d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f334e661190>]}
2021-12-24 07:47:59.319184 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:47:59.319422 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '621c5928-b209-4ffc-a79f-cf90ceca66d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f334e6612b0>]}
2021-12-24 07:47:59.319599 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:47:59.320570 (MainThread): 
2021-12-24 07:47:59.320822 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:47:59.321521 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:47:59.321734 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:47:59.870039 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:47:59.870783 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:47:59.877520 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:48:00.220680 (MainThread): 14:48:00 | Concurrency: 1 threads (target='prod')
2021-12-24 07:48:00.221201 (MainThread): 14:48:00 | 
2021-12-24 07:48:00.225995 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:48:00.226760 (Thread-1): 14:48:00 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:48:00.227514 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:48:00.228042 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:48:00.237977 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:48:00.238739 (Thread-1): finished collecting timing info
2021-12-24 07:48:00.242601 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41768), raddr=('74.125.24.95', 443)>
2021-12-24 07:48:00.242949 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44612), raddr=('34.101.5.74', 443)>
2021-12-24 07:48:00.243173 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41772), raddr=('74.125.24.95', 443)>
2021-12-24 07:48:00.243399 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44616), raddr=('34.101.5.74', 443)>
2021-12-24 07:48:00.255143 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:48:00.258309 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:48:00.548776 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:48:00.549089 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 

union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20
  );
    
2021-12-24 07:48:05.032610 (Thread-1): finished collecting timing info
2021-12-24 07:48:05.033466 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '621c5928-b209-4ffc-a79f-cf90ceca66d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f334e64a580>]}
2021-12-24 07:48:05.034168 (Thread-1): 14:48:05 | 1 of 3 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (20.0 rows, 934.5 KB processed) in 4.81s]
2021-12-24 07:48:05.034526 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:48:05.034872 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:48:05.035557 (Thread-1): 14:48:05 | 2 of 3 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:48:05.036027 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:48:05.036276 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:48:05.039326 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:48:05.039912 (Thread-1): finished collecting timing info
2021-12-24 07:48:05.041512 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:48:05.046545 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:48:05.504624 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:48:05.505421 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:48:07.852038 (Thread-1): finished collecting timing info
2021-12-24 07:48:07.853041 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '621c5928-b209-4ffc-a79f-cf90ceca66d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f334e64a8b0>]}
2021-12-24 07:48:07.853889 (Thread-1): 14:48:07 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 2.82s]
2021-12-24 07:48:07.854339 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:48:07.855897 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:48:07.856577 (Thread-1): 14:48:07 | 3 of 3 START view model dataset_dbt.view_models_dqa.................. [RUN]
2021-12-24 07:48:07.857432 (Thread-1): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:48:07.857846 (Thread-1): Compiling model.my_new_project.view_models_dqa
2021-12-24 07:48:07.863700 (Thread-1): Writing injected SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 07:48:07.864496 (Thread-1): finished collecting timing info
2021-12-24 07:48:07.881478 (Thread-1): Writing runtime SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 07:48:07.881831 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:48:07.885053 (Thread-1): On model.my_new_project.view_models_dqa: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.view_models_dqa"} */


  create or replace view `playground-325606`.`dataset_dbt`.`view_models_dqa`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_completeness`;


2021-12-24 07:48:11.663743 (Thread-1): finished collecting timing info
2021-12-24 07:48:11.664732 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '621c5928-b209-4ffc-a79f-cf90ceca66d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f334e64aa60>]}
2021-12-24 07:48:11.665543 (Thread-1): 14:48:11 | 3 of 3 OK created view model dataset_dbt.view_models_dqa............. [SCRIPT (2.7 KB processed) in 3.81s]
2021-12-24 07:48:11.665987 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:48:11.668157 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:48:11.668979 (MainThread): 14:48:11 | 
2021-12-24 07:48:11.669409 (MainThread): 14:48:11 | Finished running 2 table models, 1 view model in 12.35s.
2021-12-24 07:48:11.669825 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:48:11.670335 (MainThread): Connection 'model.my_new_project.view_models_dqa' was properly closed.
2021-12-24 07:48:11.675492 (MainThread): 
2021-12-24 07:48:11.675652 (MainThread): Completed successfully
2021-12-24 07:48:11.675761 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2021-12-24 07:48:11.675994 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f334e7788e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f334e6a0520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f334e60adc0>]}
2021-12-24 07:48:11.676219 (MainThread): Flushing usage events
2021-12-24 07:50:21.080920 (MainThread): Running with dbt=0.21.0
2021-12-24 07:50:21.232054 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:50:21.232638 (MainThread): Tracking: tracking
2021-12-24 07:50:21.238166 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40a198d910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409fbdd6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409fbdd730>]}
2021-12-24 07:50:21.244465 (MainThread): Partial parsing not enabled
2021-12-24 07:50:21.248905 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:50:21.252713 (MainThread): Parsing macros/etc.sql
2021-12-24 07:50:21.253897 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:50:21.267466 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:50:21.269051 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:50:21.270741 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:50:21.272318 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:50:21.273361 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:50:21.279063 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:50:21.287627 (MainThread): Parsing macros/core.sql
2021-12-24 07:50:21.290060 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:50:21.291126 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:50:21.292524 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:50:21.293244 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:50:21.294056 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:50:21.298274 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:50:21.303828 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:50:21.317131 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:50:21.321326 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:50:21.323829 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:50:21.335420 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:50:21.336479 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:50:21.342416 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:50:21.351056 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:50:21.356245 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:50:21.366127 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:50:21.367140 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:50:21.385450 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:50:21.386397 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:50:21.387361 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:50:21.388303 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:50:21.388885 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:50:21.390318 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:50:21.391418 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:50:21.396604 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:50:21.544401 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:50:21.552272 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:50:21.554728 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:50:21.573348 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:50:21.574478 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:50:21.575707 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:50:21.577445 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:50:21.578435 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:50:21.579465 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:50:21.580450 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:50:21.581499 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:50:21.582496 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:50:21.583566 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:50:21.583843 (MainThread): [WARNING]: Test 'test.my_new_project.unique_dqa_validity_id.8220938621' (models/example/schema.yml) depends on a node named 'dqa_validity' which was not found
2021-12-24 07:50:21.584002 (MainThread): [WARNING]: Test 'test.my_new_project.not_null_dqa_validity_id.85c8fb43e7' (models/example/schema.yml) depends on a node named 'dqa_validity' which was not found
2021-12-24 07:50:21.584128 (MainThread): [WARNING]: Test 'test.my_new_project.unique_dqa_timeliness_id.3748db8974' (models/example/schema.yml) depends on a node named 'dqa_timeliness' which was not found
2021-12-24 07:50:21.584228 (MainThread): [WARNING]: Test 'test.my_new_project.not_null_dqa_timeliness_id.a6c444adbc' (models/example/schema.yml) depends on a node named 'dqa_timeliness' which was not found
2021-12-24 07:50:21.584323 (MainThread): [WARNING]: Test 'test.my_new_project.unique_dqa_integrity_id.9da0cb4b5f' (models/example/schema.yml) depends on a node named 'dqa_integrity' which was not found
2021-12-24 07:50:21.584424 (MainThread): [WARNING]: Test 'test.my_new_project.not_null_dqa_integrity_id.805e303cc9' (models/example/schema.yml) depends on a node named 'dqa_integrity' which was not found
2021-12-24 07:50:21.593683 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ad6ff0c9-1126-42ac-9230-0c491990a158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409faa6b20>]}
2021-12-24 07:50:21.597037 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:50:21.597338 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ad6ff0c9-1126-42ac-9230-0c491990a158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409faa6cd0>]}
2021-12-24 07:50:21.597541 (MainThread): Found 3 models, 4 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:50:21.598601 (MainThread): 
2021-12-24 07:50:21.598848 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:50:21.599582 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:50:21.599796 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:50:22.637779 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:50:22.638527 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:50:22.650039 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:50:22.987699 (MainThread): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41800), raddr=('74.125.24.95', 443)>
2021-12-24 07:50:22.988305 (MainThread): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44644), raddr=('34.101.5.74', 443)>
2021-12-24 07:50:22.989071 (MainThread): 14:50:22 | Concurrency: 1 threads (target='prod')
2021-12-24 07:50:22.989533 (MainThread): 14:50:22 | 
2021-12-24 07:50:22.993386 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:50:22.994489 (Thread-1): 14:50:22 | 1 of 3 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:50:22.995607 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:50:22.996269 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:50:23.007309 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:50:23.007915 (Thread-1): finished collecting timing info
2021-12-24 07:50:23.021185 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:50:23.024579 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:50:23.414076 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:50:23.414442 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 

union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result21.criteria, 
  result21.metrics, 
  result21.total_data, 
  result21.good_data, 
  result21.bad_data, 
  cast(
    cast(
      result21.good_data * 100 as decimal
    )/ result21.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
  ) as percentage_bad_data, 
  "save_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank save_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'save_reflection'  
  ) result21 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result22.criteria, 
  result22.metrics, 
  result22.total_data, 
  result22.good_data, 
  result22.bad_data, 
  cast(
    cast(
      result22.good_data * 100 as decimal
    )/ result22.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
  ) as percentage_bad_data, 
  "post_test_result" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank post_test_result' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(post_test_result, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(post_test_result, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'finished_post_test'  
  ) result22 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result23.criteria, 
  result23.metrics, 
  result23.total_data, 
  result23.good_data, 
  result23.bad_data, 
  cast(
    cast(
      result23.good_data * 100 as decimal
    )/ result23.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_source" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_source' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_source, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_source, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result23 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result24.criteria, 
  result24.metrics, 
  result24.total_data, 
  result24.good_data, 
  result24.bad_data, 
  cast(
    cast(
      result24.good_data * 100 as decimal
    )/ result24.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_location" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_location' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_location, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result24 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result25.criteria, 
  result25.metrics, 
  result25.total_data, 
  result25.good_data, 
  result25.bad_data, 
  cast(
    cast(
      result25.good_data * 100 as decimal
    )/ result25.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_referrer" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_referrer' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result25 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result26.criteria, 
  result26.metrics, 
  result26.total_data, 
  result26.good_data, 
  result26.bad_data, 
  cast(
    cast(
      result26.good_data * 100 as decimal
    )/ result26.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_path" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_path' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_path, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result26 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result27.criteria, 
  result27.metrics, 
  result27.total_data, 
  result27.good_data, 
  result27.bad_data, 
  cast(
    cast(
      result27.good_data * 100 as decimal
    )/ result27.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_title" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_title' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_title, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result27 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result28.criteria, 
  result28.metrics, 
  result28.total_data, 
  result28.good_data, 
  result28.bad_data, 
  cast(
    cast(
      result28.good_data * 100 as decimal
    )/ result28.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_product" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_product' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_product, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_product, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result28
  );
    
2021-12-24 07:50:26.373767 (Thread-1): finished collecting timing info
2021-12-24 07:50:26.374883 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad6ff0c9-1126-42ac-9230-0c491990a158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409d991ee0>]}
2021-12-24 07:50:26.375737 (Thread-1): 14:50:26 | 1 of 3 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (28.0 rows, 2.6 MB processed) in 3.38s]
2021-12-24 07:50:26.376359 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:50:26.376750 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:50:26.377439 (Thread-1): 14:50:26 | 2 of 3 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:50:26.378432 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:50:26.378775 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:50:26.382154 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:50:26.382589 (Thread-1): finished collecting timing info
2021-12-24 07:50:26.384474 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:50:26.388677 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:50:26.759036 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:50:26.759942 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:50:28.950985 (Thread-1): finished collecting timing info
2021-12-24 07:50:28.952011 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad6ff0c9-1126-42ac-9230-0c491990a158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409d92c550>]}
2021-12-24 07:50:28.952859 (Thread-1): 14:50:28 | 2 of 3 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 2.57s]
2021-12-24 07:50:28.953304 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:50:28.954291 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:50:28.954508 (Thread-1): 14:50:28 | 3 of 3 START view model dataset_dbt.view_models_dqa.................. [RUN]
2021-12-24 07:50:28.954809 (Thread-1): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:50:28.954943 (Thread-1): Compiling model.my_new_project.view_models_dqa
2021-12-24 07:50:28.957176 (Thread-1): Writing injected SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 07:50:28.957578 (Thread-1): finished collecting timing info
2021-12-24 07:50:28.967501 (Thread-1): Writing runtime SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 07:50:28.967859 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:50:28.971349 (Thread-1): On model.my_new_project.view_models_dqa: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.view_models_dqa"} */


  create or replace view `playground-325606`.`dataset_dbt`.`view_models_dqa`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_completeness`;


2021-12-24 07:50:32.034792 (Thread-1): finished collecting timing info
2021-12-24 07:50:32.035821 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad6ff0c9-1126-42ac-9230-0c491990a158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409d987700>]}
2021-12-24 07:50:32.036672 (Thread-1): 14:50:32 | 3 of 3 OK created view model dataset_dbt.view_models_dqa............. [SCRIPT (3.9 KB processed) in 3.08s]
2021-12-24 07:50:32.037105 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:50:32.039062 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:50:32.039889 (MainThread): 14:50:32 | 
2021-12-24 07:50:32.040416 (MainThread): 14:50:32 | Finished running 2 table models, 1 view model in 10.44s.
2021-12-24 07:50:32.040803 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:50:32.041271 (MainThread): Connection 'model.my_new_project.view_models_dqa' was properly closed.
2021-12-24 07:50:32.045799 (MainThread): unclosed <ssl.SSLSocket fd=9, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41808), raddr=('74.125.24.95', 443)>
2021-12-24 07:50:32.046324 (MainThread): unclosed <ssl.SSLSocket fd=10, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44652), raddr=('34.101.5.74', 443)>
2021-12-24 07:50:32.046784 (MainThread): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44656), raddr=('34.101.5.74', 443)>
2021-12-24 07:50:32.047270 (MainThread): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41812), raddr=('74.125.24.95', 443)>
2021-12-24 07:50:32.052078 (MainThread): 
2021-12-24 07:50:32.052360 (MainThread): Completed successfully
2021-12-24 07:50:32.052593 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2021-12-24 07:50:32.052959 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409d8c6ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409d91abb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409faeba60>]}
2021-12-24 07:50:32.053309 (MainThread): Flushing usage events
2021-12-24 07:54:45.124733 (MainThread): Running with dbt=0.21.0
2021-12-24 07:54:45.278673 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:54:45.279278 (MainThread): Tracking: tracking
2021-12-24 07:54:45.285031 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c7c257a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c7a4ee670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c7a4ee6d0>]}
2021-12-24 07:54:45.291514 (MainThread): Partial parsing not enabled
2021-12-24 07:54:45.295814 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:54:45.299893 (MainThread): Parsing macros/etc.sql
2021-12-24 07:54:45.301124 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:54:45.314447 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:54:45.316078 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:54:45.317695 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:54:45.319236 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:54:45.320309 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:54:45.325928 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:54:45.335532 (MainThread): Parsing macros/core.sql
2021-12-24 07:54:45.337697 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:54:45.338666 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:54:45.340118 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:54:45.340841 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:54:45.341635 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:54:45.345670 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:54:45.351450 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:54:45.364717 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:54:45.369026 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:54:45.371294 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:54:45.382864 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:54:45.384011 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:54:45.390017 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:54:45.398089 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:54:45.402450 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:54:45.412414 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:54:45.413440 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:54:45.431574 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:54:45.432571 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:54:45.433596 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:54:45.434477 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:54:45.435066 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:54:45.436572 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:54:45.437620 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:54:45.442596 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:54:45.591189 (MainThread): 1699: statically parsed example/dqa_timeliness.sql
2021-12-24 07:54:45.591461 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-24 07:54:45.598836 (MainThread): Sending event: {'category': 'dbt', 'action': 'experimental_parser', 'label': '56ca0d0b-b037-4607-9d52-20a2887d0fd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c7a434100>]}
2021-12-24 07:54:45.599624 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:54:45.601907 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:54:45.604028 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-24 07:54:45.606195 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:54:45.609983 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_integrity".
2021-12-24 07:54:45.627416 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:54:45.628699 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:54:45.629784 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:54:45.630947 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:54:45.631998 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:54:45.632905 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:54:45.633904 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:54:45.635050 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:54:45.636050 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:54:45.636981 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:54:45.645660 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '56ca0d0b-b037-4607-9d52-20a2887d0fd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c795a7a00>]}
2021-12-24 07:54:45.649415 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:54:45.649825 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '56ca0d0b-b037-4607-9d52-20a2887d0fd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c795c81c0>]}
2021-12-24 07:54:45.650039 (MainThread): Found 6 models, 10 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:54:45.651306 (MainThread): 
2021-12-24 07:54:45.651560 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:54:45.652351 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:54:45.652577 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:54:46.528085 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:54:46.528694 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:54:46.540384 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:54:46.889824 (MainThread): 14:54:46 | Concurrency: 1 threads (target='prod')
2021-12-24 07:54:46.890358 (MainThread): 14:54:46 | 
2021-12-24 07:54:46.894296 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:54:46.895100 (Thread-1): 14:54:46 | 1 of 5 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:54:46.895961 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:54:46.896435 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:54:46.907917 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:54:46.908705 (Thread-1): finished collecting timing info
2021-12-24 07:54:46.921996 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:54:46.925408 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:54:47.295232 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:54:47.295548 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 

union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result21.criteria, 
  result21.metrics, 
  result21.total_data, 
  result21.good_data, 
  result21.bad_data, 
  cast(
    cast(
      result21.good_data * 100 as decimal
    )/ result21.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
  ) as percentage_bad_data, 
  "save_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank save_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'save_reflection'  
  ) result21 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result22.criteria, 
  result22.metrics, 
  result22.total_data, 
  result22.good_data, 
  result22.bad_data, 
  cast(
    cast(
      result22.good_data * 100 as decimal
    )/ result22.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
  ) as percentage_bad_data, 
  "post_test_result" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank post_test_result' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(post_test_result, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(post_test_result, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'finished_post_test'  
  ) result22 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result23.criteria, 
  result23.metrics, 
  result23.total_data, 
  result23.good_data, 
  result23.bad_data, 
  cast(
    cast(
      result23.good_data * 100 as decimal
    )/ result23.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_source" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_source' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_source, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_source, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result23 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result24.criteria, 
  result24.metrics, 
  result24.total_data, 
  result24.good_data, 
  result24.bad_data, 
  cast(
    cast(
      result24.good_data * 100 as decimal
    )/ result24.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_location" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_location' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_location, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result24 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result25.criteria, 
  result25.metrics, 
  result25.total_data, 
  result25.good_data, 
  result25.bad_data, 
  cast(
    cast(
      result25.good_data * 100 as decimal
    )/ result25.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_referrer" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_referrer' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result25 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result26.criteria, 
  result26.metrics, 
  result26.total_data, 
  result26.good_data, 
  result26.bad_data, 
  cast(
    cast(
      result26.good_data * 100 as decimal
    )/ result26.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_path" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_path' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_path, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result26 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result27.criteria, 
  result27.metrics, 
  result27.total_data, 
  result27.good_data, 
  result27.bad_data, 
  cast(
    cast(
      result27.good_data * 100 as decimal
    )/ result27.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_title" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_title' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_title, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result27 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result28.criteria, 
  result28.metrics, 
  result28.total_data, 
  result28.good_data, 
  result28.bad_data, 
  cast(
    cast(
      result28.good_data * 100 as decimal
    )/ result28.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_product" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_product' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_product, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_product, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result28
  );
    
2021-12-24 07:54:49.928445 (Thread-1): finished collecting timing info
2021-12-24 07:54:49.929475 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '56ca0d0b-b037-4607-9d52-20a2887d0fd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c795a7d90>]}
2021-12-24 07:54:49.930260 (Thread-1): 14:54:49 | 1 of 5 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (28.0 rows, 2.6 MB processed) in 3.03s]
2021-12-24 07:54:49.930672 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:54:49.931268 (Thread-1): Began running node model.my_new_project.dqa_timeliness
2021-12-24 07:54:49.932071 (Thread-1): 14:54:49 | 2 of 5 START table model dataset_dbt.dqa_timeliness.................. [RUN]
2021-12-24 07:54:49.932806 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-24 07:54:49.933112 (Thread-1): Compiling model.my_new_project.dqa_timeliness
2021-12-24 07:54:49.936405 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_timeliness"
2021-12-24 07:54:49.936793 (Thread-1): finished collecting timing info
2021-12-24 07:54:49.938905 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_timeliness"
2021-12-24 07:54:49.939214 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:54:49.943276 (Thread-1): On model.my_new_project.dqa_timeliness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_timeliness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_timeliness`
  
  
  OPTIONS()
  as (
    

select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result.criteria, 
  result.metrics, 
  result.total_data, 
  result.good_data, 
  result.bad_data, 
  cast(
    cast(result.good_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result.bad_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_bad_data, 
  "updated_at" field_name_checking 
from 
  (
    select 
      'Timeliness' criteria, 
      'freshness data Day - 1 on staging table' metrics, 
      count(updated_at) total_data, 
      count(
        case when updated_at in (
          select 
            updated_at 
          from 
            `satu-data-staging.fact_merdeka_mengajar.event_tracker_belajar` 
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'true' end
      ) as good_data, 
      count(
        case when updated_at not in (
          select 
            updated_at 
          from 
            `satu-data-staging.fact_merdeka_mengajar.event_tracker_belajar` 
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'false' end
      ) as bad_data, 
    from 
      `satu-data-staging.fact_merdeka_mengajar.event_tracker_belajar`
  ) result;
  );
    
2021-12-24 07:54:50.505319 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected ")" but got ";" at [66:11]')
2021-12-24 07:54:51.333249 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41852), raddr=('74.125.24.95', 443)>
2021-12-24 07:54:51.333978 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44696), raddr=('34.101.5.74', 443)>
2021-12-24 07:54:51.334654 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41856), raddr=('74.125.24.95', 443)>
2021-12-24 07:54:51.335210 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44700), raddr=('34.101.5.74', 443)>
2021-12-24 07:54:51.335705 (Thread-1): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41860), raddr=('74.125.24.95', 443)>
2021-12-24 07:54:51.336105 (Thread-1): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44704), raddr=('34.101.5.74', 443)>
2021-12-24 07:54:51.336977 (Thread-1): finished collecting timing info
2021-12-24 07:54:51.337828 (Thread-1): Database Error in model dqa_timeliness (models/example/dqa_timeliness.sql)
  Syntax error: Expected ")" but got ";" at [66:11]
  compiled SQL at target/run/my_new_project/models/example/dqa_timeliness.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Expected ")" but got ";" at [66:11]

(job ID: 5cb388dc-4b60-47d0-adf9-038ca4f83fef)

                                                           -----Query Job SQL Follows-----                                                            

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_timeliness"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`dataset_dbt`.`dqa_timeliness`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    
  10:
  11:select 
  12:  datetime(
  13:    current_timestamp(), 
  14:    "Asia/Jakarta"
  15:  ) as time_execution, 
  16:  result.criteria, 
  17:  result.metrics, 
  18:  result.total_data, 
  19:  result.good_data, 
  20:  result.bad_data, 
  21:  cast(
  22:    cast(result.good_data * 100 as decimal)/ result.total_data as numeric
  23:  ) as percentage_good_data, 
  24:  cast(
  25:    cast(result.bad_data * 100 as decimal)/ result.total_data as numeric
  26:  ) as percentage_bad_data, 
  27:  "updated_at" field_name_checking 
  28:from 
  29:  (
  30:    select 
  31:      'Timeliness' criteria, 
  32:      'freshness data Day - 1 on staging table' metrics, 
  33:      count(updated_at) total_data, 
  34:      count(
  35:        case when updated_at in (
  36:          select 
  37:            updated_at 
  38:          from 
  39:            `satu-data-staging.fact_merdeka_mengajar.event_tracker_belajar` 
  40:          where 
  41:            date(
  42:              current_date()
  43:            ) < DATE_SUB(
  44:              current_date(), 
  45:              interval 1 day
  46:            )
  47:        ) then 'true' end
  48:      ) as good_data, 
  49:      count(
  50:        case when updated_at not in (
  51:          select 
  52:            updated_at 
  53:          from 
  54:            `satu-data-staging.fact_merdeka_mengajar.event_tracker_belajar` 
  55:          where 
  56:            date(
  57:              current_date()
  58:            ) < DATE_SUB(
  59:              current_date(), 
  60:              interval 1 day
  61:            )
  62:        ) then 'false' end
  63:      ) as bad_data, 
  64:    from 
  65:      `satu-data-staging.fact_merdeka_mengajar.event_tracker_belajar`
  66:  ) result;
  67:  );
  68:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_timeliness (models/example/dqa_timeliness.sql)
  Syntax error: Expected ")" but got ";" at [66:11]
  compiled SQL at target/run/my_new_project/models/example/dqa_timeliness.sql
2021-12-24 07:54:51.342062 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '56ca0d0b-b037-4607-9d52-20a2887d0fd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c7957f730>]}
2021-12-24 07:54:51.342700 (Thread-1): 14:54:51 | 2 of 5 ERROR creating table model dataset_dbt.dqa_timeliness......... [ERROR in 1.41s]
2021-12-24 07:54:51.343049 (Thread-1): Finished running node model.my_new_project.dqa_timeliness
2021-12-24 07:54:51.343381 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:54:51.344121 (Thread-1): 14:54:51 | 3 of 5 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:54:51.344682 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:54:51.344942 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:54:51.348419 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:54:51.348876 (Thread-1): finished collecting timing info
2021-12-24 07:54:51.350366 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:54:51.354205 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:54:51.683554 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:54:51.684455 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:54:53.884231 (Thread-1): finished collecting timing info
2021-12-24 07:54:53.885210 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '56ca0d0b-b037-4607-9d52-20a2887d0fd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c7957f550>]}
2021-12-24 07:54:53.886013 (Thread-1): 14:54:53 | 3 of 5 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 2.54s]
2021-12-24 07:54:53.886485 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:54:53.886909 (Thread-1): Began running node model.my_new_project.dqa_validity
2021-12-24 07:54:53.887859 (Thread-1): 14:54:53 | 4 of 5 START table model dataset_dbt.dqa_validity.................... [RUN]
2021-12-24 07:54:53.888849 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-24 07:54:53.889470 (Thread-1): Compiling model.my_new_project.dqa_validity
2021-12-24 07:54:53.897079 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_validity"
2021-12-24 07:54:53.897828 (Thread-1): finished collecting timing info
2021-12-24 07:54:53.901361 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_validity"
2021-12-24 07:54:53.901988 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:54:53.907488 (Thread-1): On model.my_new_project.dqa_validity: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_validity"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_validity`
  
  
  OPTIONS()
  as (
    

with x as (
  select 
    user_id,
    ga_session_id,
    video_inspirasi_id  
  FROM 
    `satu-data-staging.fact_merdeka_mengajar.event_tracker_belajar`
)
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length user_id' metrics, 
      count(x.user_id) total_data, 
      count(
        case when length(x.user_id) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.user_id) < 10 
        or length(x.user_id) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length ga_session_id' metrics, 
      count(x.ga_session_id) total_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        ) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        )< 10 
        or length(
          cast(x.ga_session_id as string)
        ) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length video_inspirasi_id' metrics, 
      count(x.video_inspirasi_id) total_data, 
      count(
        case when length(x.video_inspirasi_id) = 11 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.video_inspirasi_id) < 11 
        or length(x.video_inspirasi_id) > 11 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result3
  );
    
2021-12-24 07:54:54.729070 (Thread-1): finished collecting timing info
2021-12-24 07:54:54.730013 (Thread-1): Database Error in model dqa_validity (models/example/dqa_validity.sql)
  Access Denied: Table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar: User does not have permission to query table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar.
  compiled SQL at target/run/my_new_project/models/example/dqa_validity.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.Forbidden: 403 Access Denied: Table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar: User does not have permission to query table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar.

(job ID: c5d7397c-7952-484e-a7d8-0523067642e9)

                                                          -----Query Job SQL Follows-----                                                           

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_validity"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`dataset_dbt`.`dqa_validity`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    
  10:
  11:with x as (
  12:  select 
  13:    user_id,
  14:    ga_session_id,
  15:    video_inspirasi_id  
  16:  FROM 
  17:    `satu-data-staging.fact_merdeka_mengajar.event_tracker_belajar`
  18:)
  19:select 
  20:  datetime(
  21:    current_timestamp(), 
  22:    "Asia/Jakarta"
  23:  ) as time_execution, 
  24:  result1.criteria, 
  25:  result1.metrics, 
  26:  result1.total_data, 
  27:  result1.good_data, 
  28:  result1.bad_data, 
  29:  cast(
  30:    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  31:  ) as percentage_good_data, 
  32:  cast(
  33:    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  34:  ) as percentage_bad_data, 
  35:  "user_id" field_name_checking 
  36:from 
  37:  (
  38:    select 
  39:      'Validity' criteria, 
  40:      'not valid format, syntax or length user_id' metrics, 
  41:      count(x.user_id) total_data, 
  42:      count(
  43:        case when length(x.user_id) = 10 then 'pass' end
  44:      ) as good_data, 
  45:      count(
  46:        case when length(x.user_id) < 10 
  47:        or length(x.user_id) > 10 then 'fail' end
  48:      ) as bad_data, 
  49:    from 
  50:      x
  51:  ) result1 
  52:union all 
  53:select 
  54:  datetime(
  55:    current_timestamp(), 
  56:    "Asia/Jakarta"
  57:  ) as time_execution, 
  58:  result2.criteria, 
  59:  result2.metrics, 
  60:  result2.total_data, 
  61:  result2.good_data, 
  62:  result2.bad_data, 
  63:  cast(
  64:    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  65:  ) as percentage_good_data, 
  66:  cast(
  67:    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  68:  ) as percentage_bad_data, 
  69:  "ga_session_id" field_name_checking 
  70:from 
  71:  (
  72:    select 
  73:      'Validity' criteria, 
  74:      'not valid format, syntax or length ga_session_id' metrics, 
  75:      count(x.ga_session_id) total_data, 
  76:      count(
  77:        case when length(
  78:          cast(x.ga_session_id as string)
  79:        ) = 10 then 'pass' end
  80:      ) as good_data, 
  81:      count(
  82:        case when length(
  83:          cast(x.ga_session_id as string)
  84:        )< 10 
  85:        or length(
  86:          cast(x.ga_session_id as string)
  87:        ) > 10 then 'fail' end
  88:      ) as bad_data, 
  89:    from 
  90:      x
  91:  ) result2 
  92:union all 
  93:select 
  94:  datetime(
  95:    current_timestamp(), 
  96:    "Asia/Jakarta"
  97:  ) as time_execution, 
  98:  result3.criteria, 
  99:  result3.metrics, 
 100:  result3.total_data, 
 101:  result3.good_data, 
 102:  result3.bad_data, 
 103:  cast(
 104:    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
 105:  ) as percentage_good_data, 
 106:  cast(
 107:    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
 108:  ) as percentage_bad_data, 
 109:  "video_inspirasi_id" field_name_checking 
 110:from 
 111:  (
 112:    select 
 113:      'Validity' criteria, 
 114:      'not valid format, syntax or length video_inspirasi_id' metrics, 
 115:      count(x.video_inspirasi_id) total_data, 
 116:      count(
 117:        case when length(x.video_inspirasi_id) = 11 then 'pass' end
 118:      ) as good_data, 
 119:      count(
 120:        case when length(x.video_inspirasi_id) < 11 
 121:        or length(x.video_inspirasi_id) > 11 then 'fail' end
 122:      ) as bad_data, 
 123:    from 
 124:      x
 125:  ) result3
 126:  );
 127:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 171, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_validity (models/example/dqa_validity.sql)
  Access Denied: Table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar: User does not have permission to query table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar.
  compiled SQL at target/run/my_new_project/models/example/dqa_validity.sql
2021-12-24 07:54:54.731351 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '56ca0d0b-b037-4607-9d52-20a2887d0fd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c795a7610>]}
2021-12-24 07:54:54.732431 (Thread-1): 14:54:54 | 4 of 5 ERROR creating table model dataset_dbt.dqa_validity........... [ERROR in 0.84s]
2021-12-24 07:54:54.732880 (Thread-1): Finished running node model.my_new_project.dqa_validity
2021-12-24 07:54:54.733305 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:54:54.734064 (Thread-1): 14:54:54 | 5 of 5 SKIP relation dataset_dbt.view_models_dqa..................... [SKIP]
2021-12-24 07:54:54.734581 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:54:54.736889 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:54:54.737896 (MainThread): 14:54:54 | 
2021-12-24 07:54:54.738392 (MainThread): 14:54:54 | Finished running 4 table models, 1 view model in 9.09s.
2021-12-24 07:54:54.738965 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:54:54.739465 (MainThread): Connection 'model.my_new_project.dqa_validity' was properly closed.
2021-12-24 07:54:54.750587 (MainThread): 
2021-12-24 07:54:54.751155 (MainThread): Completed with 2 errors and 0 warnings:
2021-12-24 07:54:54.751641 (MainThread): 
2021-12-24 07:54:54.752071 (MainThread): Database Error in model dqa_timeliness (models/example/dqa_timeliness.sql)
2021-12-24 07:54:54.752387 (MainThread):   Syntax error: Expected ")" but got ";" at [66:11]
2021-12-24 07:54:54.752643 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_timeliness.sql
2021-12-24 07:54:54.752905 (MainThread): 
2021-12-24 07:54:54.753176 (MainThread): Database Error in model dqa_validity (models/example/dqa_validity.sql)
2021-12-24 07:54:54.753417 (MainThread):   Access Denied: Table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar: User does not have permission to query table satu-data-staging:fact_merdeka_mengajar.event_tracker_belajar.
2021-12-24 07:54:54.753674 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_validity.sql
2021-12-24 07:54:54.753951 (MainThread): 
Done. PASS=2 WARN=0 ERROR=2 SKIP=1 TOTAL=5
2021-12-24 07:54:54.754341 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c7a511a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c7dbefaf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7c7a4ee670>]}
2021-12-24 07:54:54.754662 (MainThread): Flushing usage events
2021-12-24 07:56:30.817863 (MainThread): Running with dbt=0.21.0
2021-12-24 07:56:30.996402 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:56:30.996992 (MainThread): Tracking: tracking
2021-12-24 07:56:31.004171 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe60e69ea30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe60c8f06a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe60c8f0700>]}
2021-12-24 07:56:31.012163 (MainThread): Partial parsing not enabled
2021-12-24 07:56:31.017436 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:56:31.022046 (MainThread): Parsing macros/etc.sql
2021-12-24 07:56:31.023454 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:56:31.039448 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:56:31.041449 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:56:31.043408 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:56:31.045302 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:56:31.046490 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:56:31.053593 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:56:31.064858 (MainThread): Parsing macros/core.sql
2021-12-24 07:56:31.067620 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:56:31.068865 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:56:31.070493 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:56:31.071440 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:56:31.072453 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:56:31.077360 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:56:31.084249 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:56:31.100410 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:56:31.105666 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:56:31.108652 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:56:31.122363 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:56:31.123641 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:56:31.130937 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:56:31.140825 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:56:31.145995 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:56:31.158440 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:56:31.159750 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:56:31.180999 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:56:31.182141 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:56:31.183330 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:56:31.184500 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:56:31.185276 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:56:31.187105 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:56:31.188390 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:56:31.194326 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:56:31.378207 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-24 07:56:31.387801 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:56:31.390793 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:56:31.393659 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-24 07:56:31.396442 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:56:31.401958 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_integrity".
2021-12-24 07:56:31.424180 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:56:31.425592 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:56:31.426839 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:56:31.428235 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:56:31.429611 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:56:31.430893 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:56:31.432254 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:56:31.433722 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:56:31.435089 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:56:31.436364 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:56:31.448861 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '92b5285a-bfe5-4cc6-869a-a7a2e6adf4bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe60c7f4b20>]}
2021-12-24 07:56:31.453120 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:56:31.453468 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '92b5285a-bfe5-4cc6-869a-a7a2e6adf4bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe60c8163d0>]}
2021-12-24 07:56:31.453696 (MainThread): Found 6 models, 10 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:56:31.455120 (MainThread): 
2021-12-24 07:56:31.455427 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:56:31.456301 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:56:31.456483 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:56:32.009247 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:56:32.010009 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:56:32.021449 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:56:32.371399 (MainThread): 14:56:32 | Concurrency: 1 threads (target='prod')
2021-12-24 07:56:32.371951 (MainThread): 14:56:32 | 
2021-12-24 07:56:32.375679 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:56:32.376534 (Thread-1): 14:56:32 | 1 of 5 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:56:32.377621 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:56:32.378119 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:56:32.388567 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:56:32.389075 (Thread-1): finished collecting timing info
2021-12-24 07:56:32.400633 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:56:32.404039 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:56:32.771546 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:56:32.772010 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 

union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result21.criteria, 
  result21.metrics, 
  result21.total_data, 
  result21.good_data, 
  result21.bad_data, 
  cast(
    cast(
      result21.good_data * 100 as decimal
    )/ result21.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
  ) as percentage_bad_data, 
  "save_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank save_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'save_reflection'  
  ) result21 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result22.criteria, 
  result22.metrics, 
  result22.total_data, 
  result22.good_data, 
  result22.bad_data, 
  cast(
    cast(
      result22.good_data * 100 as decimal
    )/ result22.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
  ) as percentage_bad_data, 
  "post_test_result" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank post_test_result' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(post_test_result, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(post_test_result, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'finished_post_test'  
  ) result22 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result23.criteria, 
  result23.metrics, 
  result23.total_data, 
  result23.good_data, 
  result23.bad_data, 
  cast(
    cast(
      result23.good_data * 100 as decimal
    )/ result23.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_source" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_source' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_source, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_source, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result23 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result24.criteria, 
  result24.metrics, 
  result24.total_data, 
  result24.good_data, 
  result24.bad_data, 
  cast(
    cast(
      result24.good_data * 100 as decimal
    )/ result24.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_location" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_location' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_location, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result24 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result25.criteria, 
  result25.metrics, 
  result25.total_data, 
  result25.good_data, 
  result25.bad_data, 
  cast(
    cast(
      result25.good_data * 100 as decimal
    )/ result25.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_referrer" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_referrer' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result25 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result26.criteria, 
  result26.metrics, 
  result26.total_data, 
  result26.good_data, 
  result26.bad_data, 
  cast(
    cast(
      result26.good_data * 100 as decimal
    )/ result26.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_path" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_path' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_path, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result26 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result27.criteria, 
  result27.metrics, 
  result27.total_data, 
  result27.good_data, 
  result27.bad_data, 
  cast(
    cast(
      result27.good_data * 100 as decimal
    )/ result27.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_title" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_title' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_title, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result27 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result28.criteria, 
  result28.metrics, 
  result28.total_data, 
  result28.good_data, 
  result28.bad_data, 
  cast(
    cast(
      result28.good_data * 100 as decimal
    )/ result28.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_product" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_product' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_product, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_product, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result28
  );
    
2021-12-24 07:56:35.070211 (Thread-1): finished collecting timing info
2021-12-24 07:56:35.071364 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92b5285a-bfe5-4cc6-869a-a7a2e6adf4bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe60c7f4fa0>]}
2021-12-24 07:56:35.072246 (Thread-1): 14:56:35 | 1 of 5 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (28.0 rows, 2.6 MB processed) in 2.69s]
2021-12-24 07:56:35.072639 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:56:35.073054 (Thread-1): Began running node model.my_new_project.dqa_timeliness
2021-12-24 07:56:35.073707 (Thread-1): 14:56:35 | 2 of 5 START table model dataset_dbt.dqa_timeliness.................. [RUN]
2021-12-24 07:56:35.074247 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-24 07:56:35.074556 (Thread-1): Compiling model.my_new_project.dqa_timeliness
2021-12-24 07:56:35.077490 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_timeliness"
2021-12-24 07:56:35.077948 (Thread-1): finished collecting timing info
2021-12-24 07:56:35.079854 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_timeliness"
2021-12-24 07:56:35.080211 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:56:35.084105 (Thread-1): On model.my_new_project.dqa_timeliness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_timeliness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_timeliness`
  
  
  OPTIONS()
  as (
    

select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result.criteria, 
  result.metrics, 
  result.total_data, 
  result.good_data, 
  result.bad_data, 
  cast(
    cast(result.good_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result.bad_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_bad_data, 
  "updated_at" field_name_checking 
from 
  (
    select 
      'Timeliness' criteria, 
      'freshness data Day - 1 on staging table' metrics, 
      count(updated_at) total_data, 
      count(
        case when updated_at in (
          select 
            updated_at 
          from 
            `playground-325606.dataset_dbt.event_tracker_belajar`
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'true' end
      ) as good_data, 
      count(
        case when updated_at not in (
          select 
            updated_at 
          from 
            `playground-325606.dataset_dbt.event_tracker_belajar`
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'false' end
      ) as bad_data, 
    from 
      `playground-325606.dataset_dbt.event_tracker_belajar`
  ) result;
  );
    
2021-12-24 07:56:35.638469 (Thread-1): Retry attempt 1 of 1 after error: BadRequest('Syntax error: Expected ")" but got ";" at [66:11]')
2021-12-24 07:56:36.936253 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41910), raddr=('74.125.24.95', 443)>
2021-12-24 07:56:36.936778 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57334), raddr=('34.101.5.42', 443)>
2021-12-24 07:56:36.937160 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41914), raddr=('74.125.24.95', 443)>
2021-12-24 07:56:36.937611 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57338), raddr=('34.101.5.42', 443)>
2021-12-24 07:56:36.938300 (Thread-1): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41918), raddr=('74.125.24.95', 443)>
2021-12-24 07:56:36.938743 (Thread-1): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57342), raddr=('34.101.5.42', 443)>
2021-12-24 07:56:36.939787 (Thread-1): finished collecting timing info
2021-12-24 07:56:36.940064 (Thread-1): Database Error in model dqa_timeliness (models/example/dqa_timeliness.sql)
  Syntax error: Expected ")" but got ";" at [66:11]
  compiled SQL at target/run/my_new_project/models/example/dqa_timeliness.sql
Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 163, in exception_handler
    yield
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 345, in fn
    return self._query_and_results(client, sql, conn, job_params)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 552, in _query_and_results
    iterator = query_job.result(timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1450, in result
    do_get_result()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    return retry_target(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/retry.py", line 189, in retry_target
    return target()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1440, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 727, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/google/api_core/future/polling.py", line 135, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Syntax error: Expected ")" but got ";" at [66:11]

(job ID: 8576abe7-2071-4342-83ba-ba677c774aa9)

                                                           -----Query Job SQL Follows-----                                                            

    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |
   1:/* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_timeliness"} */
   2:
   3:
   4:  create or replace table `playground-325606`.`dataset_dbt`.`dqa_timeliness`
   5:  
   6:  
   7:  OPTIONS()
   8:  as (
   9:    
  10:
  11:select 
  12:  datetime(
  13:    current_timestamp(), 
  14:    "Asia/Jakarta"
  15:  ) as time_execution, 
  16:  result.criteria, 
  17:  result.metrics, 
  18:  result.total_data, 
  19:  result.good_data, 
  20:  result.bad_data, 
  21:  cast(
  22:    cast(result.good_data * 100 as decimal)/ result.total_data as numeric
  23:  ) as percentage_good_data, 
  24:  cast(
  25:    cast(result.bad_data * 100 as decimal)/ result.total_data as numeric
  26:  ) as percentage_bad_data, 
  27:  "updated_at" field_name_checking 
  28:from 
  29:  (
  30:    select 
  31:      'Timeliness' criteria, 
  32:      'freshness data Day - 1 on staging table' metrics, 
  33:      count(updated_at) total_data, 
  34:      count(
  35:        case when updated_at in (
  36:          select 
  37:            updated_at 
  38:          from 
  39:            `playground-325606.dataset_dbt.event_tracker_belajar`
  40:          where 
  41:            date(
  42:              current_date()
  43:            ) < DATE_SUB(
  44:              current_date(), 
  45:              interval 1 day
  46:            )
  47:        ) then 'true' end
  48:      ) as good_data, 
  49:      count(
  50:        case when updated_at not in (
  51:          select 
  52:            updated_at 
  53:          from 
  54:            `playground-325606.dataset_dbt.event_tracker_belajar`
  55:          where 
  56:            date(
  57:              current_date()
  58:            ) < DATE_SUB(
  59:              current_date(), 
  60:              interval 1 day
  61:            )
  62:        ) then 'false' end
  63:      ) as bad_data, 
  64:    from 
  65:      `playground-325606.dataset_dbt.event_tracker_belajar`
  66:  ) result;
  67:  );
  68:    
    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 128, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 226, in execute
    return self.connections.execute(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 356, in execute
    query_job, iterator = self.raw_execute(sql, fetch=fetch)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 347, in raw_execute
    query_job, iterator = self._retry_and_handle(msg=sql, conn=conn, fn=fn)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 566, in _retry_and_handle
    return retry.retry_target(
  File "/usr/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 167, in exception_handler
    self.handle_error(e, message)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/adapters/bigquery/connections.py", line 155, in handle_error
    raise DatabaseException(error_msg)
dbt.exceptions.DatabaseException: Database Error in model dqa_timeliness (models/example/dqa_timeliness.sql)
  Syntax error: Expected ")" but got ";" at [66:11]
  compiled SQL at target/run/my_new_project/models/example/dqa_timeliness.sql
2021-12-24 07:56:36.941396 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92b5285a-bfe5-4cc6-869a-a7a2e6adf4bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe60a620ac0>]}
2021-12-24 07:56:36.941657 (Thread-1): 14:56:36 | 2 of 5 ERROR creating table model dataset_dbt.dqa_timeliness......... [ERROR in 1.87s]
2021-12-24 07:56:36.941786 (Thread-1): Finished running node model.my_new_project.dqa_timeliness
2021-12-24 07:56:36.941911 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:56:36.942217 (Thread-1): 14:56:36 | 3 of 5 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:56:36.942448 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:56:36.942602 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:56:36.944276 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:56:36.944515 (Thread-1): finished collecting timing info
2021-12-24 07:56:36.945559 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:56:36.949018 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:56:37.281035 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:56:37.281893 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:56:40.748657 (Thread-1): finished collecting timing info
2021-12-24 07:56:40.749662 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92b5285a-bfe5-4cc6-869a-a7a2e6adf4bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe60c7f4b20>]}
2021-12-24 07:56:40.750464 (Thread-1): 14:56:40 | 3 of 5 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 3.81s]
2021-12-24 07:56:40.750927 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:56:40.751572 (Thread-1): Began running node model.my_new_project.dqa_validity
2021-12-24 07:56:40.752514 (Thread-1): 14:56:40 | 4 of 5 START table model dataset_dbt.dqa_validity.................... [RUN]
2021-12-24 07:56:40.753530 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-24 07:56:40.754035 (Thread-1): Compiling model.my_new_project.dqa_validity
2021-12-24 07:56:40.759856 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_validity"
2021-12-24 07:56:40.760441 (Thread-1): finished collecting timing info
2021-12-24 07:56:40.763677 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_validity"
2021-12-24 07:56:40.764114 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:56:40.769036 (Thread-1): On model.my_new_project.dqa_validity: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_validity"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_validity`
  
  
  OPTIONS()
  as (
    

with x as (
  select 
    user_id,
    ga_session_id,
    video_inspirasi_id  
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
)
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length user_id' metrics, 
      count(x.user_id) total_data, 
      count(
        case when length(x.user_id) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.user_id) < 10 
        or length(x.user_id) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length ga_session_id' metrics, 
      count(x.ga_session_id) total_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        ) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        )< 10 
        or length(
          cast(x.ga_session_id as string)
        ) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length video_inspirasi_id' metrics, 
      count(x.video_inspirasi_id) total_data, 
      count(
        case when length(x.video_inspirasi_id) = 11 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.video_inspirasi_id) < 11 
        or length(x.video_inspirasi_id) > 11 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result3
  );
    
2021-12-24 07:56:42.911126 (Thread-1): finished collecting timing info
2021-12-24 07:56:42.911551 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92b5285a-bfe5-4cc6-869a-a7a2e6adf4bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe60c7f4b20>]}
2021-12-24 07:56:42.911803 (Thread-1): 14:56:42 | 4 of 5 OK created table model dataset_dbt.dqa_validity............... [CREATE TABLE (3.0 rows, 113.1 KB processed) in 2.16s]
2021-12-24 07:56:42.911955 (Thread-1): Finished running node model.my_new_project.dqa_validity
2021-12-24 07:56:42.912101 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:56:42.912251 (Thread-1): 14:56:42 | 5 of 5 SKIP relation dataset_dbt.view_models_dqa..................... [SKIP]
2021-12-24 07:56:42.912532 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:56:42.913326 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:56:42.913590 (MainThread): 14:56:42 | 
2021-12-24 07:56:42.913715 (MainThread): 14:56:42 | Finished running 4 table models, 1 view model in 11.46s.
2021-12-24 07:56:42.913879 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:56:42.913980 (MainThread): Connection 'model.my_new_project.dqa_validity' was properly closed.
2021-12-24 07:56:42.924455 (MainThread): 
2021-12-24 07:56:42.924834 (MainThread): Completed with 1 error and 0 warnings:
2021-12-24 07:56:42.925197 (MainThread): 
2021-12-24 07:56:42.925467 (MainThread): Database Error in model dqa_timeliness (models/example/dqa_timeliness.sql)
2021-12-24 07:56:42.925832 (MainThread):   Syntax error: Expected ")" but got ";" at [66:11]
2021-12-24 07:56:42.926087 (MainThread):   compiled SQL at target/run/my_new_project/models/example/dqa_timeliness.sql
2021-12-24 07:56:42.926481 (MainThread): 
Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
2021-12-24 07:56:42.926958 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe60c915a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe610036b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe60c8f06a0>]}
2021-12-24 07:56:42.927276 (MainThread): Flushing usage events
2021-12-24 07:58:04.588788 (MainThread): Running with dbt=0.21.0
2021-12-24 07:58:04.740363 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 07:58:04.740949 (MainThread): Tracking: tracking
2021-12-24 07:58:04.746361 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe98d1aa00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe96f6b670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe96f6b6d0>]}
2021-12-24 07:58:04.752946 (MainThread): Partial parsing not enabled
2021-12-24 07:58:04.757486 (MainThread): Parsing macros/catalog.sql
2021-12-24 07:58:04.761485 (MainThread): Parsing macros/etc.sql
2021-12-24 07:58:04.762650 (MainThread): Parsing macros/adapters.sql
2021-12-24 07:58:04.775895 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 07:58:04.777475 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 07:58:04.779051 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 07:58:04.780611 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 07:58:04.781589 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 07:58:04.787299 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 07:58:04.796226 (MainThread): Parsing macros/core.sql
2021-12-24 07:58:04.798415 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 07:58:04.799394 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 07:58:04.800758 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 07:58:04.801498 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 07:58:04.802329 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 07:58:04.806332 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 07:58:04.812014 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 07:58:04.825152 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 07:58:04.829449 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 07:58:04.831721 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 07:58:04.843269 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 07:58:04.844353 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 07:58:04.850330 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 07:58:04.858557 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 07:58:04.862790 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 07:58:04.872790 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 07:58:04.873792 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 07:58:04.891834 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 07:58:04.892771 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 07:58:04.893769 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 07:58:04.894642 (MainThread): Parsing macros/etc/query.sql
2021-12-24 07:58:04.895231 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 07:58:04.896673 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 07:58:04.897707 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 07:58:04.902684 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 07:58:05.048210 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-24 07:58:05.056326 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:58:05.058952 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:58:05.061104 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-24 07:58:05.063418 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:58:05.067518 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_integrity".
2021-12-24 07:58:05.084565 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:58:05.085735 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:58:05.086887 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:58:05.088112 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:58:05.089096 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:58:05.090043 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:58:05.091124 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:58:05.092264 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:58:05.093249 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:58:05.094203 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 07:58:05.102838 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ffa78e4e-392f-4020-a58d-9fad17edd77d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe96e71b20>]}
2021-12-24 07:58:05.106418 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 07:58:05.106710 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ffa78e4e-392f-4020-a58d-9fad17edd77d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe96e92a90>]}
2021-12-24 07:58:05.106903 (MainThread): Found 6 models, 10 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 07:58:05.108116 (MainThread): 
2021-12-24 07:58:05.108347 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:58:05.109139 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 07:58:05.109356 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 07:58:05.602957 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 07:58:05.603712 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 07:58:05.615110 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:58:05.976727 (MainThread): 14:58:05 | Concurrency: 1 threads (target='prod')
2021-12-24 07:58:05.977279 (MainThread): 14:58:05 | 
2021-12-24 07:58:05.981223 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 07:58:05.982014 (Thread-1): 14:58:05 | 1 of 5 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 07:58:05.982809 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 07:58:05.983369 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 07:58:05.994570 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:58:05.995216 (Thread-1): finished collecting timing info
2021-12-24 07:58:06.009140 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:58:06.012567 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:58:06.651768 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 07:58:06.652098 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 

union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result21.criteria, 
  result21.metrics, 
  result21.total_data, 
  result21.good_data, 
  result21.bad_data, 
  cast(
    cast(
      result21.good_data * 100 as decimal
    )/ result21.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
  ) as percentage_bad_data, 
  "save_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank save_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'save_reflection'  
  ) result21 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result22.criteria, 
  result22.metrics, 
  result22.total_data, 
  result22.good_data, 
  result22.bad_data, 
  cast(
    cast(
      result22.good_data * 100 as decimal
    )/ result22.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
  ) as percentage_bad_data, 
  "post_test_result" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank post_test_result' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(post_test_result, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(post_test_result, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'finished_post_test'  
  ) result22 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result23.criteria, 
  result23.metrics, 
  result23.total_data, 
  result23.good_data, 
  result23.bad_data, 
  cast(
    cast(
      result23.good_data * 100 as decimal
    )/ result23.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_source" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_source' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_source, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_source, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result23 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result24.criteria, 
  result24.metrics, 
  result24.total_data, 
  result24.good_data, 
  result24.bad_data, 
  cast(
    cast(
      result24.good_data * 100 as decimal
    )/ result24.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_location" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_location' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_location, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result24 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result25.criteria, 
  result25.metrics, 
  result25.total_data, 
  result25.good_data, 
  result25.bad_data, 
  cast(
    cast(
      result25.good_data * 100 as decimal
    )/ result25.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_referrer" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_referrer' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result25 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result26.criteria, 
  result26.metrics, 
  result26.total_data, 
  result26.good_data, 
  result26.bad_data, 
  cast(
    cast(
      result26.good_data * 100 as decimal
    )/ result26.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_path" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_path' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_path, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result26 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result27.criteria, 
  result27.metrics, 
  result27.total_data, 
  result27.good_data, 
  result27.bad_data, 
  cast(
    cast(
      result27.good_data * 100 as decimal
    )/ result27.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_title" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_title' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_title, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result27 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result28.criteria, 
  result28.metrics, 
  result28.total_data, 
  result28.good_data, 
  result28.bad_data, 
  cast(
    cast(
      result28.good_data * 100 as decimal
    )/ result28.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_product" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_product' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_product, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_product, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result28
  );
    
2021-12-24 07:58:09.533635 (Thread-1): finished collecting timing info
2021-12-24 07:58:09.534833 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffa78e4e-392f-4020-a58d-9fad17edd77d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe96e710a0>]}
2021-12-24 07:58:09.535793 (Thread-1): 14:58:09 | 1 of 5 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (28.0 rows, 2.6 MB processed) in 3.55s]
2021-12-24 07:58:09.536362 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 07:58:09.536901 (Thread-1): Began running node model.my_new_project.dqa_timeliness
2021-12-24 07:58:09.537799 (Thread-1): 14:58:09 | 2 of 5 START table model dataset_dbt.dqa_timeliness.................. [RUN]
2021-12-24 07:58:09.538389 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-24 07:58:09.538812 (Thread-1): Compiling model.my_new_project.dqa_timeliness
2021-12-24 07:58:09.542149 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_timeliness"
2021-12-24 07:58:09.542759 (Thread-1): finished collecting timing info
2021-12-24 07:58:09.544651 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_timeliness"
2021-12-24 07:58:09.545007 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:58:09.548662 (Thread-1): On model.my_new_project.dqa_timeliness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_timeliness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_timeliness`
  
  
  OPTIONS()
  as (
    

select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result.criteria, 
  result.metrics, 
  result.total_data, 
  result.good_data, 
  result.bad_data, 
  cast(
    cast(result.good_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result.bad_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_bad_data, 
  "updated_at" field_name_checking 
from 
  (
    select 
      'Timeliness' criteria, 
      'freshness data Day - 1 on staging table' metrics, 
      count(updated_at) total_data, 
      count(
        case when updated_at in (
          select 
            updated_at 
          from 
            `playground-325606.dataset_dbt.event_tracker_belajar`
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'true' end
      ) as good_data, 
      count(
        case when updated_at not in (
          select 
            updated_at 
          from 
            `playground-325606.dataset_dbt.event_tracker_belajar`
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'false' end
      ) as bad_data, 
    from 
      `playground-325606.dataset_dbt.event_tracker_belajar`
  ) result
  );
    
2021-12-24 07:58:11.720097 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41964), raddr=('74.125.24.95', 443)>
2021-12-24 07:58:11.720700 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57388), raddr=('34.101.5.42', 443)>
2021-12-24 07:58:11.721274 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41968), raddr=('74.125.24.95', 443)>
2021-12-24 07:58:11.721659 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57392), raddr=('34.101.5.42', 443)>
2021-12-24 07:58:11.722119 (Thread-1): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41972), raddr=('74.125.24.95', 443)>
2021-12-24 07:58:11.722493 (Thread-1): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57396), raddr=('34.101.5.42', 443)>
2021-12-24 07:58:12.143137 (Thread-1): finished collecting timing info
2021-12-24 07:58:12.144193 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffa78e4e-392f-4020-a58d-9fad17edd77d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe96e710a0>]}
2021-12-24 07:58:12.145088 (Thread-1): 14:58:12 | 2 of 5 OK created table model dataset_dbt.dqa_timeliness............. [CREATE TABLE (1.0 rows, 63.7 KB processed) in 2.61s]
2021-12-24 07:58:12.145538 (Thread-1): Finished running node model.my_new_project.dqa_timeliness
2021-12-24 07:58:12.146079 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 07:58:12.147012 (Thread-1): 14:58:12 | 3 of 5 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 07:58:12.147785 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 07:58:12.148287 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 07:58:12.153823 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:58:12.154617 (Thread-1): finished collecting timing info
2021-12-24 07:58:12.157388 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:58:12.163886 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:58:12.500930 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 07:58:12.501716 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 07:58:14.602814 (Thread-1): finished collecting timing info
2021-12-24 07:58:14.603795 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffa78e4e-392f-4020-a58d-9fad17edd77d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe96e71d30>]}
2021-12-24 07:58:14.604631 (Thread-1): 14:58:14 | 3 of 5 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 2.46s]
2021-12-24 07:58:14.605049 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 07:58:14.605432 (Thread-1): Began running node model.my_new_project.dqa_validity
2021-12-24 07:58:14.606135 (Thread-1): 14:58:14 | 4 of 5 START table model dataset_dbt.dqa_validity.................... [RUN]
2021-12-24 07:58:14.607447 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-24 07:58:14.607920 (Thread-1): Compiling model.my_new_project.dqa_validity
2021-12-24 07:58:14.613425 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_validity"
2021-12-24 07:58:14.614257 (Thread-1): finished collecting timing info
2021-12-24 07:58:14.616472 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:58:14.621333 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 07:58:14.950057 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_validity"
2021-12-24 07:58:14.950896 (Thread-1): On model.my_new_project.dqa_validity: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_validity"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_validity`
  
  
  OPTIONS()
  as (
    

with x as (
  select 
    user_id,
    ga_session_id,
    video_inspirasi_id  
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
)
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length user_id' metrics, 
      count(x.user_id) total_data, 
      count(
        case when length(x.user_id) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.user_id) < 10 
        or length(x.user_id) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length ga_session_id' metrics, 
      count(x.ga_session_id) total_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        ) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        )< 10 
        or length(
          cast(x.ga_session_id as string)
        ) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length video_inspirasi_id' metrics, 
      count(x.video_inspirasi_id) total_data, 
      count(
        case when length(x.video_inspirasi_id) = 11 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.video_inspirasi_id) < 11 
        or length(x.video_inspirasi_id) > 11 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result3
  );
    
2021-12-24 07:58:16.922251 (Thread-1): finished collecting timing info
2021-12-24 07:58:16.923221 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffa78e4e-392f-4020-a58d-9fad17edd77d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe96e71cd0>]}
2021-12-24 07:58:16.924023 (Thread-1): 14:58:16 | 4 of 5 OK created table model dataset_dbt.dqa_validity............... [CREATE TABLE (3.0 rows, 113.1 KB processed) in 2.32s]
2021-12-24 07:58:16.924464 (Thread-1): Finished running node model.my_new_project.dqa_validity
2021-12-24 07:58:16.924870 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 07:58:16.925656 (Thread-1): 14:58:16 | 5 of 5 START view model dataset_dbt.view_models_dqa.................. [RUN]
2021-12-24 07:58:16.926422 (Thread-1): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 07:58:16.926887 (Thread-1): Compiling model.my_new_project.view_models_dqa
2021-12-24 07:58:16.927943 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 41984), raddr=('74.125.24.95', 443)>
2021-12-24 07:58:16.928428 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 57408), raddr=('34.101.5.42', 443)>
2021-12-24 07:58:16.934865 (Thread-1): Writing injected SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 07:58:16.935654 (Thread-1): finished collecting timing info
2021-12-24 07:58:16.949552 (Thread-1): Writing runtime SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 07:58:16.949854 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 07:58:16.953645 (Thread-1): On model.my_new_project.view_models_dqa: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.view_models_dqa"} */


  create or replace view `playground-325606`.`dataset_dbt`.`view_models_dqa`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_completeness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_timeliness`;


2021-12-24 07:58:19.966307 (Thread-1): finished collecting timing info
2021-12-24 07:58:19.966657 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffa78e4e-392f-4020-a58d-9fad17edd77d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe94c3daf0>]}
2021-12-24 07:58:19.966924 (Thread-1): 14:58:19 | 5 of 5 OK created view model dataset_dbt.view_models_dqa............. [SCRIPT (4.1 KB processed) in 3.04s]
2021-12-24 07:58:19.967070 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 07:58:19.967863 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 07:58:19.968129 (MainThread): 14:58:19 | 
2021-12-24 07:58:19.968253 (MainThread): 14:58:19 | Finished running 4 table models, 1 view model in 14.86s.
2021-12-24 07:58:19.968357 (MainThread): Connection 'master' was properly closed.
2021-12-24 07:58:19.968473 (MainThread): Connection 'model.my_new_project.view_models_dqa' was properly closed.
2021-12-24 07:58:19.974130 (MainThread): 
2021-12-24 07:58:19.974628 (MainThread): Completed successfully
2021-12-24 07:58:19.975031 (MainThread): 
Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
2021-12-24 07:58:19.975524 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe96f93a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe9a6b1af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe96f6b670>]}
2021-12-24 07:58:19.976006 (MainThread): Flushing usage events
2021-12-24 08:03:12.392243 (MainThread): Running with dbt=0.21.0
2021-12-24 08:03:12.541193 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 08:03:12.541816 (MainThread): Tracking: tracking
2021-12-24 08:03:12.547143 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60b254ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60b07e47c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60b07e4820>]}
2021-12-24 08:03:12.553585 (MainThread): Partial parsing not enabled
2021-12-24 08:03:12.555341 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60b0749490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60b0749460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60b0749340>]}
2021-12-24 08:03:12.555523 (MainThread): Flushing usage events
2021-12-24 08:03:14.469105 (MainThread): Encountered an error:
2021-12-24 08:03:14.470055 (MainThread): Compilation Error
  Error reading my_new_project: example/schema.yml - Runtime Error
    Syntax error near line 49
    ------------------------------
    46 |           - unique
    47 |           - not_null
    48 |   
    49 |   name: dqa_all_criteria
    50 |     description: "A starter dbt model"
    51 |     columns:
    52 |       - name: id
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 3
    did not find expected '-' indicator
      in "<unicode string>", line 49, column 3
2021-12-24 08:03:14.481019 (MainThread): Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/yaml_helper.py", line 65, in load_yaml_text
    return safe_load(contents)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/yaml_helper.py", line 60, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 3
did not find expected '-' indicator
  in "<unicode string>", line 49, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/schemas.py", line 108, in yaml_from_file
    return load_yaml_text(source_file.contents)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/yaml_helper.py", line 72, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 49
  ------------------------------
  46 |           - unique
  47 |           - not_null
  48 |   
  49 |   name: dqa_all_criteria
  50 |     description: "A starter dbt model"
  51 |     columns:
  52 |       - name: id
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 3
  did not find expected '-' indicator
    in "<unicode string>", line 49, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/main.py", line 127, in main
    results, succeeded = handle_and_check(args)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/main.py", line 205, in handle_and_check
    task, res = run_from_args(parsed)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/main.py", line 258, in run_from_args
    results = task.run()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 433, in run
    self._runtime_initialize()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 149, in _runtime_initialize
    super()._runtime_initialize()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 87, in _runtime_initialize
    self.load_manifest()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 74, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/manifest.py", line 182, in get_full_manifest
    manifest = loader.load()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/manifest.py", line 210, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 151, in read_files
    project_files['SchemaParser'] = read_files_for_parser(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 106, in read_files_for_parser
    source_files = get_source_files(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 97, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 37, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/schemas.py", line 111, in yaml_from_file
    raise CompilationException(
dbt.exceptions.CompilationException: Compilation Error
  Error reading my_new_project: example/schema.yml - Runtime Error
    Syntax error near line 49
    ------------------------------
    46 |           - unique
    47 |           - not_null
    48 |   
    49 |   name: dqa_all_criteria
    50 |     description: "A starter dbt model"
    51 |     columns:
    52 |       - name: id
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 3
    did not find expected '-' indicator
      in "<unicode string>", line 49, column 3

2021-12-24 08:04:09.374916 (MainThread): Running with dbt=0.21.0
2021-12-24 08:04:09.547845 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 08:04:09.548522 (MainThread): Tracking: tracking
2021-12-24 08:04:09.555309 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b92c3c1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b90e8e700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b90e8e760>]}
2021-12-24 08:04:09.562976 (MainThread): Partial parsing not enabled
2021-12-24 08:04:09.565117 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b90e3a490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b90e3a460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b90e3a340>]}
2021-12-24 08:04:09.565361 (MainThread): Flushing usage events
2021-12-24 08:04:11.390861 (MainThread): Encountered an error:
2021-12-24 08:04:11.391931 (MainThread): Compilation Error
  Error reading my_new_project: example/schema.yml - Runtime Error
    Syntax error near line 49
    ------------------------------
    46 |           - unique
    47 |           - not_null
    48 |   
    49 |   name: dqa_all_criteria
    50 |     description: "A starter dbt model"
    51 |     columns:
    52 |       - name: id
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 3
    did not find expected '-' indicator
      in "<unicode string>", line 49, column 3
2021-12-24 08:04:11.395243 (MainThread): Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/yaml_helper.py", line 65, in load_yaml_text
    return safe_load(contents)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/yaml_helper.py", line 60, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 3
did not find expected '-' indicator
  in "<unicode string>", line 49, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/schemas.py", line 108, in yaml_from_file
    return load_yaml_text(source_file.contents)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/yaml_helper.py", line 72, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 49
  ------------------------------
  46 |           - unique
  47 |           - not_null
  48 |   
  49 |   name: dqa_all_criteria
  50 |     description: "A starter dbt model"
  51 |     columns:
  52 |       - name: id
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 3
  did not find expected '-' indicator
    in "<unicode string>", line 49, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/main.py", line 127, in main
    results, succeeded = handle_and_check(args)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/main.py", line 205, in handle_and_check
    task, res = run_from_args(parsed)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/main.py", line 258, in run_from_args
    results = task.run()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 433, in run
    self._runtime_initialize()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 149, in _runtime_initialize
    super()._runtime_initialize()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 87, in _runtime_initialize
    self.load_manifest()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 74, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/manifest.py", line 182, in get_full_manifest
    manifest = loader.load()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/manifest.py", line 210, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 151, in read_files
    project_files['SchemaParser'] = read_files_for_parser(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 106, in read_files_for_parser
    source_files = get_source_files(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 97, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 37, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/schemas.py", line 111, in yaml_from_file
    raise CompilationException(
dbt.exceptions.CompilationException: Compilation Error
  Error reading my_new_project: example/schema.yml - Runtime Error
    Syntax error near line 49
    ------------------------------
    46 |           - unique
    47 |           - not_null
    48 |   
    49 |   name: dqa_all_criteria
    50 |     description: "A starter dbt model"
    51 |     columns:
    52 |       - name: id
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 3
    did not find expected '-' indicator
      in "<unicode string>", line 49, column 3

2021-12-24 08:05:45.780990 (MainThread): Running with dbt=0.21.0
2021-12-24 08:05:45.934710 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 08:05:45.935247 (MainThread): Tracking: tracking
2021-12-24 08:05:45.940901 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e52bc87c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e50e61610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e50e61670>]}
2021-12-24 08:05:45.947157 (MainThread): Partial parsing not enabled
2021-12-24 08:05:45.948882 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e50dc63a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e50dc6370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e50dc6250>]}
2021-12-24 08:05:45.949067 (MainThread): Flushing usage events
2021-12-24 08:05:47.614749 (MainThread): Encountered an error:
2021-12-24 08:05:47.615829 (MainThread): Compilation Error
  Error reading my_new_project: example/schema.yml - Runtime Error
    Syntax error near line 49
    ------------------------------
    46 |           - unique
    47 |           - not_null
    48 |   
    49 |   name: dqa_all_criteria
    50 |     description: "A starter dbt model"
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 3
    did not find expected '-' indicator
      in "<unicode string>", line 49, column 3
2021-12-24 08:05:47.618905 (MainThread): Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/yaml_helper.py", line 65, in load_yaml_text
    return safe_load(contents)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/yaml_helper.py", line 60, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 3
did not find expected '-' indicator
  in "<unicode string>", line 49, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/schemas.py", line 108, in yaml_from_file
    return load_yaml_text(source_file.contents)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/clients/yaml_helper.py", line 72, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 49
  ------------------------------
  46 |           - unique
  47 |           - not_null
  48 |   
  49 |   name: dqa_all_criteria
  50 |     description: "A starter dbt model"
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 3
  did not find expected '-' indicator
    in "<unicode string>", line 49, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/main.py", line 127, in main
    results, succeeded = handle_and_check(args)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/main.py", line 205, in handle_and_check
    task, res = run_from_args(parsed)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/main.py", line 258, in run_from_args
    results = task.run()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 433, in run
    self._runtime_initialize()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 149, in _runtime_initialize
    super()._runtime_initialize()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 87, in _runtime_initialize
    self.load_manifest()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/task/runnable.py", line 74, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/manifest.py", line 182, in get_full_manifest
    manifest = loader.load()
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/manifest.py", line 210, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 151, in read_files
    project_files['SchemaParser'] = read_files_for_parser(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 106, in read_files_for_parser
    source_files = get_source_files(
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 97, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/read_files.py", line 37, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/home/leerizza/dbt-env/lib/python3.8/site-packages/dbt/parser/schemas.py", line 111, in yaml_from_file
    raise CompilationException(
dbt.exceptions.CompilationException: Compilation Error
  Error reading my_new_project: example/schema.yml - Runtime Error
    Syntax error near line 49
    ------------------------------
    46 |           - unique
    47 |           - not_null
    48 |   
    49 |   name: dqa_all_criteria
    50 |     description: "A starter dbt model"
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 3
    did not find expected '-' indicator
      in "<unicode string>", line 49, column 3

2021-12-24 08:06:06.238992 (MainThread): Running with dbt=0.21.0
2021-12-24 08:06:06.414065 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-24 08:06:06.414636 (MainThread): Tracking: tracking
2021-12-24 08:06:06.421313 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25d8d7820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25bb6f6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25bb6f700>]}
2021-12-24 08:06:06.428809 (MainThread): Partial parsing not enabled
2021-12-24 08:06:06.434077 (MainThread): Parsing macros/catalog.sql
2021-12-24 08:06:06.438751 (MainThread): Parsing macros/etc.sql
2021-12-24 08:06:06.440243 (MainThread): Parsing macros/adapters.sql
2021-12-24 08:06:06.456128 (MainThread): Parsing macros/materializations/seed.sql
2021-12-24 08:06:06.458054 (MainThread): Parsing macros/materializations/copy.sql
2021-12-24 08:06:06.460220 (MainThread): Parsing macros/materializations/view.sql
2021-12-24 08:06:06.462143 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-24 08:06:06.463434 (MainThread): Parsing macros/materializations/table.sql
2021-12-24 08:06:06.470413 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-24 08:06:06.480408 (MainThread): Parsing macros/core.sql
2021-12-24 08:06:06.482571 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-24 08:06:06.483602 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-24 08:06:06.484927 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-24 08:06:06.485627 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-24 08:06:06.486445 (MainThread): Parsing macros/materializations/test.sql
2021-12-24 08:06:06.490371 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-24 08:06:06.496079 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-24 08:06:06.509144 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-24 08:06:06.513417 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-24 08:06:06.515638 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-24 08:06:06.527171 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-24 08:06:06.528372 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-24 08:06:06.534192 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-24 08:06:06.542166 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-24 08:06:06.546470 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-24 08:06:06.556238 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-24 08:06:06.557230 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-24 08:06:06.575237 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-24 08:06:06.576211 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-24 08:06:06.577156 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-24 08:06:06.578387 (MainThread): Parsing macros/etc/query.sql
2021-12-24 08:06:06.579037 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-24 08:06:06.580494 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-24 08:06:06.581582 (MainThread): Parsing macros/etc/datetime.sql
2021-12-24 08:06:06.586577 (MainThread): Parsing macros/adapters/common.sql
2021-12-24 08:06:06.732591 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_all_criteria".
2021-12-24 08:06:06.740301 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-24 08:06:06.742422 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 08:06:06.745674 (MainThread): 1699: statically parsed example/dqa_uniqueness.sql
2021-12-24 08:06:06.745834 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 08:06:06.747664 (MainThread): Sending event: {'category': 'dbt', 'action': 'experimental_parser', 'label': 'c364f79b-d19c-4200-a41b-73b33acd9ea2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25ba35370>]}
2021-12-24 08:06:06.748332 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-24 08:06:06.750452 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 08:06:06.754195 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_integrity".
2021-12-24 08:06:06.770874 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 08:06:06.772026 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 08:06:06.773154 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 08:06:06.774186 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 08:06:06.775224 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 08:06:06.776334 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 08:06:06.777494 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 08:06:06.778485 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 08:06:06.779543 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 08:06:06.780531 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-24 08:06:06.789468 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c364f79b-d19c-4200-a41b-73b33acd9ea2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25ba236a0>]}
2021-12-24 08:06:06.792990 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-24 08:06:06.793323 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c364f79b-d19c-4200-a41b-73b33acd9ea2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25ba23310>]}
2021-12-24 08:06:06.793522 (MainThread): Found 7 models, 10 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-24 08:06:06.794879 (MainThread): 
2021-12-24 08:06:06.795181 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 08:06:06.796166 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-24 08:06:06.796490 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-24 08:06:07.778629 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-24 08:06:07.779371 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-24 08:06:07.791748 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 08:06:08.175242 (MainThread): 15:06:08 | Concurrency: 1 threads (target='prod')
2021-12-24 08:06:08.175788 (MainThread): 15:06:08 | 
2021-12-24 08:06:08.180005 (Thread-1): Began running node model.my_new_project.dqa_all_criteria
2021-12-24 08:06:08.180778 (Thread-1): 15:06:08 | 1 of 6 START table model dataset_dbt.dqa_all_criteria................ [RUN]
2021-12-24 08:06:08.181569 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_all_criteria".
2021-12-24 08:06:08.182180 (Thread-1): Compiling model.my_new_project.dqa_all_criteria
2021-12-24 08:06:08.189262 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_all_criteria"
2021-12-24 08:06:08.190165 (Thread-1): finished collecting timing info
2021-12-24 08:06:08.217988 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_all_criteria"
2021-12-24 08:06:08.218292 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 08:06:08.221629 (Thread-1): On model.my_new_project.dqa_all_criteria: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_all_criteria"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_all_criteria`
  
  
  OPTIONS()
  as (
    

select * from `playground-325606.dataset_dbt.dqa_uniqueness`
union all
select * from `playground-325606.dataset_dbt.dqa_completeness`
union all
select * from `playground-325606.dataset_dbt.dqa_validity`
union all
select * from `playground-325606.dataset_dbt.dqa_timeliness`
  );
    
2021-12-24 08:06:10.479402 (Thread-1): finished collecting timing info
2021-12-24 08:06:10.480516 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c364f79b-d19c-4200-a41b-73b33acd9ea2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc258ab6f10>]}
2021-12-24 08:06:10.481367 (Thread-1): 15:06:10 | 1 of 6 OK created table model dataset_dbt.dqa_all_criteria........... [CREATE TABLE (33.0 rows, 4.7 KB processed) in 2.30s]
2021-12-24 08:06:10.481828 (Thread-1): Finished running node model.my_new_project.dqa_all_criteria
2021-12-24 08:06:10.482324 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-24 08:06:10.483239 (Thread-1): 15:06:10 | 2 of 6 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-24 08:06:10.484061 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-24 08:06:10.484376 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-24 08:06:10.485415 (Thread-1): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55666), raddr=('74.125.130.95', 443)>
2021-12-24 08:06:10.485765 (Thread-1): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44918), raddr=('34.101.5.74', 443)>
2021-12-24 08:06:10.491527 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 08:06:10.491895 (Thread-1): finished collecting timing info
2021-12-24 08:06:10.493307 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 08:06:10.496902 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 08:06:11.928053 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-24 08:06:11.929030 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 

union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result21.criteria, 
  result21.metrics, 
  result21.total_data, 
  result21.good_data, 
  result21.bad_data, 
  cast(
    cast(
      result21.good_data * 100 as decimal
    )/ result21.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
  ) as percentage_bad_data, 
  "save_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank save_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'save_reflection'  
  ) result21 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result22.criteria, 
  result22.metrics, 
  result22.total_data, 
  result22.good_data, 
  result22.bad_data, 
  cast(
    cast(
      result22.good_data * 100 as decimal
    )/ result22.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
  ) as percentage_bad_data, 
  "post_test_result" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank post_test_result' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(post_test_result, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(post_test_result, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'finished_post_test'  
  ) result22 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result23.criteria, 
  result23.metrics, 
  result23.total_data, 
  result23.good_data, 
  result23.bad_data, 
  cast(
    cast(
      result23.good_data * 100 as decimal
    )/ result23.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_source" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_source' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_source, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_source, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result23 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result24.criteria, 
  result24.metrics, 
  result24.total_data, 
  result24.good_data, 
  result24.bad_data, 
  cast(
    cast(
      result24.good_data * 100 as decimal
    )/ result24.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_location" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_location' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_location, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result24 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result25.criteria, 
  result25.metrics, 
  result25.total_data, 
  result25.good_data, 
  result25.bad_data, 
  cast(
    cast(
      result25.good_data * 100 as decimal
    )/ result25.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_referrer" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_referrer' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result25 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result26.criteria, 
  result26.metrics, 
  result26.total_data, 
  result26.good_data, 
  result26.bad_data, 
  cast(
    cast(
      result26.good_data * 100 as decimal
    )/ result26.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_path" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_path' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_path, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result26 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result27.criteria, 
  result27.metrics, 
  result27.total_data, 
  result27.good_data, 
  result27.bad_data, 
  cast(
    cast(
      result27.good_data * 100 as decimal
    )/ result27.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_title" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_title' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_title, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result27 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result28.criteria, 
  result28.metrics, 
  result28.total_data, 
  result28.good_data, 
  result28.bad_data, 
  cast(
    cast(
      result28.good_data * 100 as decimal
    )/ result28.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_product" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_product' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_product, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_product, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result28
  );
    
2021-12-24 08:06:11.931336 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55658), raddr=('74.125.130.95', 443)>
2021-12-24 08:06:11.931485 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44910), raddr=('34.101.5.74', 443)>
2021-12-24 08:06:11.931592 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55662), raddr=('74.125.130.95', 443)>
2021-12-24 08:06:11.931694 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44914), raddr=('34.101.5.74', 443)>
2021-12-24 08:06:14.799702 (Thread-1): finished collecting timing info
2021-12-24 08:06:14.800877 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c364f79b-d19c-4200-a41b-73b33acd9ea2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25abfe670>]}
2021-12-24 08:06:14.801712 (Thread-1): 15:06:14 | 2 of 6 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (28.0 rows, 2.6 MB processed) in 4.32s]
2021-12-24 08:06:14.802186 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-24 08:06:14.802632 (Thread-1): Began running node model.my_new_project.dqa_timeliness
2021-12-24 08:06:14.803611 (Thread-1): 15:06:14 | 3 of 6 START table model dataset_dbt.dqa_timeliness.................. [RUN]
2021-12-24 08:06:14.804375 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-24 08:06:14.804770 (Thread-1): Compiling model.my_new_project.dqa_timeliness
2021-12-24 08:06:14.810478 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_timeliness"
2021-12-24 08:06:14.811171 (Thread-1): finished collecting timing info
2021-12-24 08:06:14.813427 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 08:06:14.818821 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 08:06:15.447514 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_timeliness"
2021-12-24 08:06:15.448390 (Thread-1): On model.my_new_project.dqa_timeliness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_timeliness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_timeliness`
  
  
  OPTIONS()
  as (
    

select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result.criteria, 
  result.metrics, 
  result.total_data, 
  result.good_data, 
  result.bad_data, 
  cast(
    cast(result.good_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result.bad_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_bad_data, 
  "updated_at" field_name_checking 
from 
  (
    select 
      'Timeliness' criteria, 
      'freshness data Day - 1 on staging table' metrics, 
      count(updated_at) total_data, 
      count(
        case when updated_at in (
          select 
            updated_at 
          from 
            `playground-325606.dataset_dbt.event_tracker_belajar`
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'true' end
      ) as good_data, 
      count(
        case when updated_at not in (
          select 
            updated_at 
          from 
            `playground-325606.dataset_dbt.event_tracker_belajar`
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'false' end
      ) as bad_data, 
    from 
      `playground-325606.dataset_dbt.event_tracker_belajar`
  ) result
  );
    
2021-12-24 08:06:17.839649 (Thread-1): finished collecting timing info
2021-12-24 08:06:17.840746 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c364f79b-d19c-4200-a41b-73b33acd9ea2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25abfea30>]}
2021-12-24 08:06:17.841672 (Thread-1): 15:06:17 | 3 of 6 OK created table model dataset_dbt.dqa_timeliness............. [CREATE TABLE (1.0 rows, 63.7 KB processed) in 3.04s]
2021-12-24 08:06:17.842171 (Thread-1): Finished running node model.my_new_project.dqa_timeliness
2021-12-24 08:06:17.842825 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-24 08:06:17.843600 (Thread-1): 15:06:17 | 4 of 6 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-24 08:06:17.844401 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-24 08:06:17.844824 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-24 08:06:17.852001 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 08:06:17.852710 (Thread-1): finished collecting timing info
2021-12-24 08:06:17.855686 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 08:06:17.861357 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 08:06:18.770208 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-24 08:06:18.771098 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-24 08:06:21.050580 (Thread-1): finished collecting timing info
2021-12-24 08:06:21.051482 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c364f79b-d19c-4200-a41b-73b33acd9ea2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25abfe970>]}
2021-12-24 08:06:21.052303 (Thread-1): 15:06:21 | 4 of 6 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 3.21s]
2021-12-24 08:06:21.052716 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-24 08:06:21.053078 (Thread-1): Began running node model.my_new_project.dqa_validity
2021-12-24 08:06:21.053845 (Thread-1): 15:06:21 | 5 of 6 START table model dataset_dbt.dqa_validity.................... [RUN]
2021-12-24 08:06:21.054674 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-24 08:06:21.055203 (Thread-1): Compiling model.my_new_project.dqa_validity
2021-12-24 08:06:21.056711 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55680), raddr=('74.125.130.95', 443)>
2021-12-24 08:06:21.057214 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44932), raddr=('34.101.5.74', 443)>
2021-12-24 08:06:21.061184 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_validity"
2021-12-24 08:06:21.061741 (Thread-1): finished collecting timing info
2021-12-24 08:06:21.063996 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 08:06:21.068325 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-24 08:06:21.469608 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_validity"
2021-12-24 08:06:21.470528 (Thread-1): On model.my_new_project.dqa_validity: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_validity"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_validity`
  
  
  OPTIONS()
  as (
    

with x as (
  select 
    user_id,
    ga_session_id,
    video_inspirasi_id  
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
)
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length user_id' metrics, 
      count(x.user_id) total_data, 
      count(
        case when length(x.user_id) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.user_id) < 10 
        or length(x.user_id) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length ga_session_id' metrics, 
      count(x.ga_session_id) total_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        ) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        )< 10 
        or length(
          cast(x.ga_session_id as string)
        ) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length video_inspirasi_id' metrics, 
      count(x.video_inspirasi_id) total_data, 
      count(
        case when length(x.video_inspirasi_id) = 11 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.video_inspirasi_id) < 11 
        or length(x.video_inspirasi_id) > 11 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result3
  );
    
2021-12-24 08:06:23.929114 (Thread-1): finished collecting timing info
2021-12-24 08:06:23.930317 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c364f79b-d19c-4200-a41b-73b33acd9ea2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2589e0520>]}
2021-12-24 08:06:23.931336 (Thread-1): 15:06:23 | 5 of 6 OK created table model dataset_dbt.dqa_validity............... [CREATE TABLE (3.0 rows, 113.1 KB processed) in 2.88s]
2021-12-24 08:06:23.931869 (Thread-1): Finished running node model.my_new_project.dqa_validity
2021-12-24 08:06:23.933085 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-24 08:06:23.934101 (Thread-1): 15:06:23 | 6 of 6 START view model dataset_dbt.view_models_dqa.................. [RUN]
2021-12-24 08:06:23.934918 (Thread-1): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-24 08:06:23.935354 (Thread-1): Compiling model.my_new_project.view_models_dqa
2021-12-24 08:06:23.940640 (Thread-1): Writing injected SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 08:06:23.940993 (Thread-1): finished collecting timing info
2021-12-24 08:06:23.951413 (Thread-1): Writing runtime SQL for node "model.my_new_project.view_models_dqa"
2021-12-24 08:06:23.951783 (Thread-1): Opening a new connection, currently in state closed
2021-12-24 08:06:23.955461 (Thread-1): On model.my_new_project.view_models_dqa: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.view_models_dqa"} */


  create or replace view `playground-325606`.`dataset_dbt`.`view_models_dqa`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_completeness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_timeliness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_validity`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_all_criteria`;


2021-12-24 08:06:30.883387 (Thread-1): finished collecting timing info
2021-12-24 08:06:30.884584 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c364f79b-d19c-4200-a41b-73b33acd9ea2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2589ef640>]}
2021-12-24 08:06:30.885656 (Thread-1): 15:06:30 | 6 of 6 OK created view model dataset_dbt.view_models_dqa............. [SCRIPT (9.2 KB processed) in 6.95s]
2021-12-24 08:06:30.886210 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-24 08:06:30.888431 (MainThread): Acquiring new bigquery connection "master".
2021-12-24 08:06:30.889358 (MainThread): 15:06:30 | 
2021-12-24 08:06:30.889803 (MainThread): 15:06:30 | Finished running 5 table models, 1 view model in 24.09s.
2021-12-24 08:06:30.890250 (MainThread): Connection 'master' was properly closed.
2021-12-24 08:06:30.890585 (MainThread): Connection 'model.my_new_project.view_models_dqa' was properly closed.
2021-12-24 08:06:30.894721 (MainThread): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55676), raddr=('74.125.130.95', 443)>
2021-12-24 08:06:30.895529 (MainThread): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44928), raddr=('34.101.5.74', 443)>
2021-12-24 08:06:30.896173 (MainThread): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 55686), raddr=('74.125.130.95', 443)>
2021-12-24 08:06:30.896594 (MainThread): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 44938), raddr=('34.101.5.74', 443)>
2021-12-24 08:06:30.905379 (MainThread): 
2021-12-24 08:06:30.905668 (MainThread): Completed successfully
2021-12-24 08:06:30.905897 (MainThread): 
Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
2021-12-24 08:06:30.906219 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25bb6ff70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25f26eaf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc25bb90940>]}
2021-12-24 08:06:30.906689 (MainThread): Flushing usage events
2021-12-27 11:34:46.754175 (MainThread): Running with dbt=0.21.0
2021-12-27 11:34:46.937325 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-27 11:34:46.937907 (MainThread): Tracking: tracking
2021-12-27 11:34:46.945080 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9538dbb910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9537055670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f95370556d0>]}
2021-12-27 11:34:46.953197 (MainThread): Partial parsing not enabled
2021-12-27 11:34:46.966521 (MainThread): Parsing macros/catalog.sql
2021-12-27 11:34:46.971628 (MainThread): Parsing macros/etc.sql
2021-12-27 11:34:46.973126 (MainThread): Parsing macros/adapters.sql
2021-12-27 11:34:46.990313 (MainThread): Parsing macros/materializations/seed.sql
2021-12-27 11:34:46.992450 (MainThread): Parsing macros/materializations/copy.sql
2021-12-27 11:34:46.994552 (MainThread): Parsing macros/materializations/view.sql
2021-12-27 11:34:46.996681 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-27 11:34:46.997992 (MainThread): Parsing macros/materializations/table.sql
2021-12-27 11:34:47.004002 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-27 11:34:47.014401 (MainThread): Parsing macros/core.sql
2021-12-27 11:34:47.017286 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-27 11:34:47.018536 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-27 11:34:47.020351 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-27 11:34:47.021283 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-27 11:34:47.022358 (MainThread): Parsing macros/materializations/test.sql
2021-12-27 11:34:47.027764 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-27 11:34:47.034869 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-27 11:34:47.052234 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-27 11:34:47.057773 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-27 11:34:47.060792 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-27 11:34:47.075927 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-27 11:34:47.077330 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-27 11:34:47.085146 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-27 11:34:47.095467 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-27 11:34:47.101112 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-27 11:34:47.114240 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-27 11:34:47.115609 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-27 11:34:47.138705 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-27 11:34:47.139981 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-27 11:34:47.141281 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-27 11:34:47.142459 (MainThread): Parsing macros/etc/query.sql
2021-12-27 11:34:47.143239 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-27 11:34:47.145064 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-27 11:34:47.146362 (MainThread): Parsing macros/etc/datetime.sql
2021-12-27 11:34:47.152954 (MainThread): Parsing macros/adapters/common.sql
2021-12-27 11:34:47.348049 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_all_criteria".
2021-12-27 11:34:47.357764 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-27 11:34:47.360608 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-27 11:34:47.364060 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-27 11:34:47.366823 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-27 11:34:47.369758 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-27 11:34:47.375480 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_integrity".
2021-12-27 11:34:47.398396 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 11:34:47.399937 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 11:34:47.401365 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 11:34:47.402651 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 11:34:47.403956 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 11:34:47.405376 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 11:34:47.406787 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 11:34:47.408113 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 11:34:47.409379 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 11:34:47.410642 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 11:34:47.422442 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '385aac02-05ca-4e75-a25f-eb4aa02e8eb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9536f078b0>]}
2021-12-27 11:34:47.427075 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-27 11:34:47.427455 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '385aac02-05ca-4e75-a25f-eb4aa02e8eb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9536f07eb0>]}
2021-12-27 11:34:47.427709 (MainThread): Found 7 models, 10 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-27 11:34:47.429229 (MainThread): 
2021-12-27 11:34:47.429527 (MainThread): Acquiring new bigquery connection "master".
2021-12-27 11:34:47.430391 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-27 11:34:47.430636 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-27 11:34:47.947995 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-27 11:34:47.948541 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-27 11:34:47.954887 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 11:34:48.351865 (MainThread): 18:34:48 | Concurrency: 1 threads (target='prod')
2021-12-27 11:34:48.352369 (MainThread): 18:34:48 | 
2021-12-27 11:34:48.361426 (Thread-1): Began running node model.my_new_project.dqa_all_criteria
2021-12-27 11:34:48.362170 (Thread-1): 18:34:48 | 1 of 6 START table model dataset_dbt.dqa_all_criteria................ [RUN]
2021-12-27 11:34:48.362897 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_all_criteria".
2021-12-27 11:34:48.363289 (Thread-1): Compiling model.my_new_project.dqa_all_criteria
2021-12-27 11:34:48.367838 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_all_criteria"
2021-12-27 11:34:48.368541 (Thread-1): finished collecting timing info
2021-12-27 11:34:48.386029 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 11:34:48.389562 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 11:34:48.764784 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_all_criteria"
2021-12-27 11:34:48.765135 (Thread-1): On model.my_new_project.dqa_all_criteria: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_all_criteria"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_all_criteria`
  
  
  OPTIONS()
  as (
    

select * from `playground-325606.dataset_dbt.dqa_uniqueness`
union all
select * from `playground-325606.dataset_dbt.dqa_completeness`
union all
select * from `playground-325606.dataset_dbt.dqa_validity`
union all
select * from `playground-325606.dataset_dbt.dqa_timeliness`
  );
    
2021-12-27 11:34:52.162778 (Thread-1): finished collecting timing info
2021-12-27 11:34:52.163545 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '385aac02-05ca-4e75-a25f-eb4aa02e8eb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9536ef5460>]}
2021-12-27 11:34:52.164212 (Thread-1): 18:34:52 | 1 of 6 OK created table model dataset_dbt.dqa_all_criteria........... [CREATE TABLE (33.0 rows, 4.7 KB processed) in 3.80s]
2021-12-27 11:34:52.164563 (Thread-1): Finished running node model.my_new_project.dqa_all_criteria
2021-12-27 11:34:52.164899 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-27 11:34:52.165601 (Thread-1): 18:34:52 | 2 of 6 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-27 11:34:52.166169 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-27 11:34:52.166422 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-27 11:34:52.172027 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-27 11:34:52.172448 (Thread-1): finished collecting timing info
2021-12-27 11:34:52.173725 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 11:34:52.177108 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 11:34:52.520943 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-27 11:34:52.522096 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 

union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result21.criteria, 
  result21.metrics, 
  result21.total_data, 
  result21.good_data, 
  result21.bad_data, 
  cast(
    cast(
      result21.good_data * 100 as decimal
    )/ result21.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
  ) as percentage_bad_data, 
  "save_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank save_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'save_reflection'  
  ) result21 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result22.criteria, 
  result22.metrics, 
  result22.total_data, 
  result22.good_data, 
  result22.bad_data, 
  cast(
    cast(
      result22.good_data * 100 as decimal
    )/ result22.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
  ) as percentage_bad_data, 
  "post_test_result" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank post_test_result' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(post_test_result, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(post_test_result, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'finished_post_test'  
  ) result22 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result23.criteria, 
  result23.metrics, 
  result23.total_data, 
  result23.good_data, 
  result23.bad_data, 
  cast(
    cast(
      result23.good_data * 100 as decimal
    )/ result23.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_source" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_source' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_source, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_source, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result23 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result24.criteria, 
  result24.metrics, 
  result24.total_data, 
  result24.good_data, 
  result24.bad_data, 
  cast(
    cast(
      result24.good_data * 100 as decimal
    )/ result24.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_location" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_location' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_location, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result24 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result25.criteria, 
  result25.metrics, 
  result25.total_data, 
  result25.good_data, 
  result25.bad_data, 
  cast(
    cast(
      result25.good_data * 100 as decimal
    )/ result25.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_referrer" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_referrer' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result25 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result26.criteria, 
  result26.metrics, 
  result26.total_data, 
  result26.good_data, 
  result26.bad_data, 
  cast(
    cast(
      result26.good_data * 100 as decimal
    )/ result26.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_path" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_path' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_path, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result26 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result27.criteria, 
  result27.metrics, 
  result27.total_data, 
  result27.good_data, 
  result27.bad_data, 
  cast(
    cast(
      result27.good_data * 100 as decimal
    )/ result27.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_title" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_title' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_title, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result27 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result28.criteria, 
  result28.metrics, 
  result28.total_data, 
  result28.good_data, 
  result28.bad_data, 
  cast(
    cast(
      result28.good_data * 100 as decimal
    )/ result28.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_product" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_product' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_product, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_product, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result28
  );
    
2021-12-27 11:34:53.109355 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 33362), raddr=('142.250.4.95', 443)>
2021-12-27 11:34:53.109934 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 43666), raddr=('34.101.5.42', 443)>
2021-12-27 11:34:53.110331 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 33366), raddr=('142.250.4.95', 443)>
2021-12-27 11:34:53.110701 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 43670), raddr=('34.101.5.42', 443)>
2021-12-27 11:34:53.111126 (Thread-1): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 33370), raddr=('142.250.4.95', 443)>
2021-12-27 11:34:53.111487 (Thread-1): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 43674), raddr=('34.101.5.42', 443)>
2021-12-27 11:34:56.690238 (Thread-1): finished collecting timing info
2021-12-27 11:34:56.691362 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '385aac02-05ca-4e75-a25f-eb4aa02e8eb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9536ef5040>]}
2021-12-27 11:34:56.692207 (Thread-1): 18:34:56 | 2 of 6 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (28.0 rows, 2.6 MB processed) in 4.53s]
2021-12-27 11:34:56.692625 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-27 11:34:56.693002 (Thread-1): Began running node model.my_new_project.dqa_timeliness
2021-12-27 11:34:56.693928 (Thread-1): 18:34:56 | 3 of 6 START table model dataset_dbt.dqa_timeliness.................. [RUN]
2021-12-27 11:34:56.694693 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-27 11:34:56.695204 (Thread-1): Compiling model.my_new_project.dqa_timeliness
2021-12-27 11:34:56.699734 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_timeliness"
2021-12-27 11:34:56.700427 (Thread-1): finished collecting timing info
2021-12-27 11:34:56.703573 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 11:34:56.710160 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 11:34:57.065407 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_timeliness"
2021-12-27 11:34:57.067196 (Thread-1): On model.my_new_project.dqa_timeliness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_timeliness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_timeliness`
  
  
  OPTIONS()
  as (
    

select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result.criteria, 
  result.metrics, 
  result.total_data, 
  result.good_data, 
  result.bad_data, 
  cast(
    cast(result.good_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result.bad_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_bad_data, 
  "updated_at" field_name_checking 
from 
  (
    select 
      'Timeliness' criteria, 
      'freshness data Day - 1 on staging table' metrics, 
      count(updated_at) total_data, 
      count(
        case when updated_at in (
          select 
            updated_at 
          from 
            `playground-325606.dataset_dbt.event_tracker_belajar`
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'true' end
      ) as good_data, 
      count(
        case when updated_at not in (
          select 
            updated_at 
          from 
            `playground-325606.dataset_dbt.event_tracker_belajar`
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'false' end
      ) as bad_data, 
    from 
      `playground-325606.dataset_dbt.event_tracker_belajar`
  ) result
  );
    
2021-12-27 11:35:00.751998 (Thread-1): finished collecting timing info
2021-12-27 11:35:00.752968 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '385aac02-05ca-4e75-a25f-eb4aa02e8eb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9536ef5670>]}
2021-12-27 11:35:00.753745 (Thread-1): 18:35:00 | 3 of 6 OK created table model dataset_dbt.dqa_timeliness............. [CREATE TABLE (1.0 rows, 63.7 KB processed) in 4.06s]
2021-12-27 11:35:00.754165 (Thread-1): Finished running node model.my_new_project.dqa_timeliness
2021-12-27 11:35:00.754626 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-27 11:35:00.755386 (Thread-1): 18:35:00 | 4 of 6 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-27 11:35:00.756099 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-27 11:35:00.756571 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-27 11:35:00.763353 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-27 11:35:00.763978 (Thread-1): finished collecting timing info
2021-12-27 11:35:00.766141 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 11:35:00.771275 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 11:35:01.197306 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-27 11:35:01.198138 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-27 11:35:04.499260 (Thread-1): finished collecting timing info
2021-12-27 11:35:04.500335 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '385aac02-05ca-4e75-a25f-eb4aa02e8eb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9534d92d60>]}
2021-12-27 11:35:04.501149 (Thread-1): 18:35:04 | 4 of 6 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 3.74s]
2021-12-27 11:35:04.501589 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-27 11:35:04.502154 (Thread-1): Began running node model.my_new_project.dqa_validity
2021-12-27 11:35:04.502939 (Thread-1): 18:35:04 | 5 of 6 START table model dataset_dbt.dqa_validity.................... [RUN]
2021-12-27 11:35:04.503733 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-27 11:35:04.504151 (Thread-1): Compiling model.my_new_project.dqa_validity
2021-12-27 11:35:04.509781 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_validity"
2021-12-27 11:35:04.511066 (Thread-1): finished collecting timing info
2021-12-27 11:35:04.513818 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 11:35:04.519184 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 11:35:09.911954 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_validity"
2021-12-27 11:35:09.917549 (Thread-1): On model.my_new_project.dqa_validity: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_validity"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_validity`
  
  
  OPTIONS()
  as (
    

with x as (
  select 
    user_id,
    ga_session_id,
    video_inspirasi_id  
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
)
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length user_id' metrics, 
      count(x.user_id) total_data, 
      count(
        case when length(x.user_id) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.user_id) < 10 
        or length(x.user_id) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length ga_session_id' metrics, 
      count(x.ga_session_id) total_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        ) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        )< 10 
        or length(
          cast(x.ga_session_id as string)
        ) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length video_inspirasi_id' metrics, 
      count(x.video_inspirasi_id) total_data, 
      count(
        case when length(x.video_inspirasi_id) = 11 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.video_inspirasi_id) < 11 
        or length(x.video_inspirasi_id) > 11 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result3
  );
    
2021-12-27 11:35:13.434632 (Thread-1): finished collecting timing info
2021-12-27 11:35:13.435716 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '385aac02-05ca-4e75-a25f-eb4aa02e8eb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9534d92e80>]}
2021-12-27 11:35:13.436600 (Thread-1): 18:35:13 | 5 of 6 OK created table model dataset_dbt.dqa_validity............... [CREATE TABLE (3.0 rows, 113.1 KB processed) in 8.93s]
2021-12-27 11:35:13.437034 (Thread-1): Finished running node model.my_new_project.dqa_validity
2021-12-27 11:35:13.438517 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-27 11:35:13.439148 (Thread-1): 18:35:13 | 6 of 6 START view model dataset_dbt.view_models_dqa.................. [RUN]
2021-12-27 11:35:13.440298 (Thread-1): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-27 11:35:13.440692 (Thread-1): Compiling model.my_new_project.view_models_dqa
2021-12-27 11:35:13.446610 (Thread-1): Writing injected SQL for node "model.my_new_project.view_models_dqa"
2021-12-27 11:35:13.447277 (Thread-1): finished collecting timing info
2021-12-27 11:35:13.463531 (Thread-1): Writing runtime SQL for node "model.my_new_project.view_models_dqa"
2021-12-27 11:35:13.463923 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 11:35:13.467468 (Thread-1): On model.my_new_project.view_models_dqa: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.view_models_dqa"} */


  create or replace view `playground-325606`.`dataset_dbt`.`view_models_dqa`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_completeness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_timeliness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_validity`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_all_criteria`;


2021-12-27 11:35:21.585218 (Thread-1): finished collecting timing info
2021-12-27 11:35:21.590627 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '385aac02-05ca-4e75-a25f-eb4aa02e8eb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9534caf370>]}
2021-12-27 11:35:21.591327 (Thread-1): 18:35:21 | 6 of 6 OK created view model dataset_dbt.view_models_dqa............. [SCRIPT (9.2 KB processed) in 8.15s]
2021-12-27 11:35:21.591719 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-27 11:35:21.593864 (MainThread): Acquiring new bigquery connection "master".
2021-12-27 11:35:21.594671 (MainThread): 18:35:21 | 
2021-12-27 11:35:21.595084 (MainThread): 18:35:21 | Finished running 5 table models, 1 view model in 34.17s.
2021-12-27 11:35:21.595510 (MainThread): Connection 'master' was properly closed.
2021-12-27 11:35:21.596004 (MainThread): Connection 'model.my_new_project.view_models_dqa' was properly closed.
2021-12-27 11:35:21.599842 (MainThread): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 33378), raddr=('142.250.4.95', 443)>
2021-12-27 11:35:21.600287 (MainThread): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 43682), raddr=('34.101.5.42', 443)>
2021-12-27 11:35:21.600585 (MainThread): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 33382), raddr=('142.250.4.95', 443)>
2021-12-27 11:35:21.600828 (MainThread): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 43686), raddr=('34.101.5.42', 443)>
2021-12-27 11:35:21.601075 (MainThread): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 36390), raddr=('172.217.194.95', 443)>
2021-12-27 11:35:21.601290 (MainThread): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 43690), raddr=('34.101.5.42', 443)>
2021-12-27 11:35:21.605855 (MainThread): 
2021-12-27 11:35:21.606141 (MainThread): Completed successfully
2021-12-27 11:35:21.606367 (MainThread): 
Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
2021-12-27 11:35:21.606667 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9534cbbe20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9534ca2b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9536f1af40>]}
2021-12-27 11:35:21.606995 (MainThread): Flushing usage events
2021-12-27 15:35:58.103093 (MainThread): Running with dbt=0.21.0
2021-12-27 15:35:58.256320 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='/home/leerizza/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-12-27 15:35:58.256856 (MainThread): Tracking: tracking
2021-12-27 15:35:58.262342 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe3b0c1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe1d5e700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe1d5e760>]}
2021-12-27 15:35:58.268438 (MainThread): Partial parsing not enabled
2021-12-27 15:35:58.272908 (MainThread): Parsing macros/catalog.sql
2021-12-27 15:35:58.276938 (MainThread): Parsing macros/etc.sql
2021-12-27 15:35:58.278125 (MainThread): Parsing macros/adapters.sql
2021-12-27 15:35:58.290680 (MainThread): Parsing macros/materializations/seed.sql
2021-12-27 15:35:58.292224 (MainThread): Parsing macros/materializations/copy.sql
2021-12-27 15:35:58.293717 (MainThread): Parsing macros/materializations/view.sql
2021-12-27 15:35:58.295405 (MainThread): Parsing macros/materializations/snapshot.sql
2021-12-27 15:35:58.296389 (MainThread): Parsing macros/materializations/table.sql
2021-12-27 15:35:58.301683 (MainThread): Parsing macros/materializations/incremental.sql
2021-12-27 15:35:58.310200 (MainThread): Parsing macros/core.sql
2021-12-27 15:35:58.312541 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-12-27 15:35:58.313554 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-12-27 15:35:58.314951 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-12-27 15:35:58.315768 (MainThread): Parsing macros/schema_tests/unique.sql
2021-12-27 15:35:58.316543 (MainThread): Parsing macros/materializations/test.sql
2021-12-27 15:35:58.320265 (MainThread): Parsing macros/materializations/helpers.sql
2021-12-27 15:35:58.325579 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-12-27 15:35:58.338339 (MainThread): Parsing macros/materializations/view/view.sql
2021-12-27 15:35:58.342381 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-12-27 15:35:58.344587 (MainThread): Parsing macros/materializations/incremental/on_schema_change.sql
2021-12-27 15:35:58.355608 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-12-27 15:35:58.356638 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-12-27 15:35:58.362387 (MainThread): Parsing macros/materializations/common/merge.sql
2021-12-27 15:35:58.370126 (MainThread): Parsing macros/materializations/table/table.sql
2021-12-27 15:35:58.374205 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-12-27 15:35:58.383715 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-12-27 15:35:58.384678 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-12-27 15:35:58.401882 (MainThread): Parsing macros/etc/is_incremental.sql
2021-12-27 15:35:58.402765 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-12-27 15:35:58.403728 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-12-27 15:35:58.404577 (MainThread): Parsing macros/etc/query.sql
2021-12-27 15:35:58.405151 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-12-27 15:35:58.406510 (MainThread): Parsing macros/etc/where_subquery.sql
2021-12-27 15:35:58.407518 (MainThread): Parsing macros/etc/datetime.sql
2021-12-27 15:35:58.412262 (MainThread): Parsing macros/adapters/common.sql
2021-12-27 15:35:58.553559 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_all_criteria".
2021-12-27 15:35:58.561004 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-27 15:35:58.563071 (MainThread): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-27 15:35:58.565492 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-27 15:35:58.567490 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-27 15:35:58.569621 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-27 15:35:58.573204 (MainThread): Acquiring new bigquery connection "model.my_new_project.dqa_integrity".
2021-12-27 15:35:58.589033 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 15:35:58.590056 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 15:35:58.591093 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 15:35:58.592090 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 15:35:58.593123 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 15:35:58.594024 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 15:35:58.595084 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 15:35:58.596098 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 15:35:58.597022 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 15:35:58.598015 (MainThread): 'soft_unicode' has been renamed to 'soft_str'. The old name will be removed in MarkupSafe 2.1.
2021-12-27 15:35:58.606167 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8a191896-ac6f-4435-abea-c14644ab8eb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe1cc6700>]}
2021-12-27 15:35:58.609411 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2021-12-27 15:35:58.609701 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8a191896-ac6f-4435-abea-c14644ab8eb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe1cc6fa0>]}
2021-12-27 15:35:58.609934 (MainThread): Found 7 models, 10 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-12-27 15:35:58.611251 (MainThread): 
2021-12-27 15:35:58.611477 (MainThread): Acquiring new bigquery connection "master".
2021-12-27 15:35:58.612308 (ThreadPoolExecutor-0_0): Acquiring new bigquery connection "list_playground-325606".
2021-12-27 15:35:58.612475 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-12-27 15:35:59.113755 (ThreadPoolExecutor-1_0): Acquiring new bigquery connection "list_playground-325606_dataset_dbt".
2021-12-27 15:35:59.113935 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state closed
2021-12-27 15:35:59.121112 (ThreadPoolExecutor-1_0): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 15:35:59.807219 (MainThread): 22:35:59 | Concurrency: 1 threads (target='prod')
2021-12-27 15:35:59.807739 (MainThread): 22:35:59 | 
2021-12-27 15:35:59.811679 (Thread-1): Began running node model.my_new_project.dqa_all_criteria
2021-12-27 15:35:59.812496 (Thread-1): 22:35:59 | 1 of 6 START table model dataset_dbt.dqa_all_criteria................ [RUN]
2021-12-27 15:35:59.813258 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_all_criteria".
2021-12-27 15:35:59.813762 (Thread-1): Compiling model.my_new_project.dqa_all_criteria
2021-12-27 15:35:59.818893 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_all_criteria"
2021-12-27 15:35:59.820101 (Thread-1): finished collecting timing info
2021-12-27 15:35:59.843168 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 15:35:59.846914 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 15:36:00.228412 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_all_criteria"
2021-12-27 15:36:00.228690 (Thread-1): On model.my_new_project.dqa_all_criteria: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_all_criteria"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_all_criteria`
  
  
  OPTIONS()
  as (
    

select * from `playground-325606.dataset_dbt.dqa_uniqueness`
union all
select * from `playground-325606.dataset_dbt.dqa_completeness`
union all
select * from `playground-325606.dataset_dbt.dqa_validity`
union all
select * from `playground-325606.dataset_dbt.dqa_timeliness`
  );
    
2021-12-27 15:36:02.407272 (Thread-1): finished collecting timing info
2021-12-27 15:36:02.408229 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a191896-ac6f-4435-abea-c14644ab8eb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe02f3fa0>]}
2021-12-27 15:36:02.409085 (Thread-1): 22:36:02 | 1 of 6 OK created table model dataset_dbt.dqa_all_criteria........... [CREATE TABLE (33.0 rows, 4.7 KB processed) in 2.60s]
2021-12-27 15:36:02.409495 (Thread-1): Finished running node model.my_new_project.dqa_all_criteria
2021-12-27 15:36:02.409999 (Thread-1): Began running node model.my_new_project.dqa_completeness
2021-12-27 15:36:02.410848 (Thread-1): 22:36:02 | 2 of 6 START table model dataset_dbt.dqa_completeness................ [RUN]
2021-12-27 15:36:02.411564 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_completeness".
2021-12-27 15:36:02.411994 (Thread-1): Compiling model.my_new_project.dqa_completeness
2021-12-27 15:36:02.420488 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_completeness"
2021-12-27 15:36:02.420963 (Thread-1): finished collecting timing info
2021-12-27 15:36:02.422517 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 15:36:02.427625 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 15:36:02.792737 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_completeness"
2021-12-27 15:36:02.793700 (Thread-1): On model.my_new_project.dqa_completeness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_completeness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_completeness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (
  select 
    ga_session_id, 
    user_id, 
    user_pseudo_id, 
    event_name, 
    action_type_cleaned, 
    user_first_timestamp_gmt7, 
    timestamp_gmt7, 
    date_gmt7, 
    time_gmt7, 
    hour_gmt7, 
    device_category_cleaned, 
    engagement_time_msec, 
    material_id, 
    module_id, 
    video_id, 
    video_inspirasi_id, 
    video_inspirasi_playslist_id, 
    quiz_id, 
    text_id, 
    open_reflection_id, 
    save_reflection_id, 
    post_test_result, 
    event_params_source, 
    event_params_page_location, 
    event_params_page_referrer, 
    event_params_page_path, 
    event_params_page_title, 
    event_params_product 
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
) 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank ga_session_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(ga_session_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(user_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(user_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "user_pseudo_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_pseudo_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull (cast(user_pseudo_id as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result3 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result4.criteria, 
  result4.metrics, 
  result4.total_data, 
  result4.good_data, 
  result4.bad_data, 
  cast(
    cast(result4.good_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result4.bad_data * 100 as decimal)/ result4.total_data as numeric
  ) as percentage_bad_data, 
  "event_name" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_name' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_name, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_name, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result4 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result5.criteria, 
  result5.metrics, 
  result5.total_data, 
  result5.good_data, 
  result5.bad_data, 
  cast(
    cast(result5.good_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result5.bad_data * 100 as decimal)/ result5.total_data as numeric
  ) as percentage_bad_data, 
  "action_type_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank action_type_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(action_type_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(action_type_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result5 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result6.criteria, 
  result6.metrics, 
  result6.total_data, 
  result6.good_data, 
  result6.bad_data, 
  cast(
    cast(result6.good_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result6.bad_data * 100 as decimal)/ result6.total_data as numeric
  ) as percentage_bad_data, 
  "user_first_timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank user_first_timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            user_first_timestamp_gmt7 as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result6 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result7.criteria, 
  result7.metrics, 
  result7.total_data, 
  result7.good_data, 
  result7.bad_data, 
  cast(
    cast(result7.good_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result7.bad_data * 100 as decimal)/ result7.total_data as numeric
  ) as percentage_bad_data, 
  "timestamp_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank timestamp_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(timestamp_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result7 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result8.criteria, 
  result8.metrics, 
  result8.total_data, 
  result8.good_data, 
  result8.bad_data, 
  cast(
    cast(result8.good_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result8.bad_data * 100 as decimal)/ result8.total_data as numeric
  ) as percentage_bad_data, 
  "date_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank date_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(date_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result8 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result9.criteria, 
  result9.metrics, 
  result9.total_data, 
  result9.good_data, 
  result9.bad_data, 
  cast(
    cast(result9.good_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result9.bad_data * 100 as decimal)/ result9.total_data as numeric
  ) as percentage_bad_data, 
  "time_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank time_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(cast(time_gmt7 as string), '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result9 

union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result10.criteria, 
  result10.metrics, 
  result10.total_data, 
  result10.good_data, 
  result10.bad_data, 
  cast(
    cast(
      result10.good_data * 100 as decimal
    )/ result10.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result10.bad_data * 100 as decimal)/ result10.total_data as numeric
  ) as percentage_bad_data, 
  "hour_gmt7" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank hour_gmt7' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(hour_gmt7 as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result10 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result11.criteria, 
  result11.metrics, 
  result11.total_data, 
  result11.good_data, 
  result11.bad_data, 
  cast(
    cast(
      result11.good_data * 100 as decimal
    )/ result11.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result11.bad_data * 100 as decimal)/ result11.total_data as numeric
  ) as percentage_bad_data, 
  "device_category_cleaned" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank device_category_cleaned' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(device_category_cleaned, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(device_category_cleaned, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result11 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result12.criteria, 
  result12.metrics, 
  result12.total_data, 
  result12.good_data, 
  result12.bad_data, 
  cast(
    cast(
      result12.good_data * 100 as decimal
    )/ result12.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result12.bad_data * 100 as decimal)/ result12.total_data as numeric
  ) as percentage_bad_data, 
  "engagement_time_msec" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank engagement_time_msec' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(engagement_time_msec as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result12 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result13.criteria, 
  result13.metrics, 
  result13.total_data, 
  result13.good_data, 
  result13.bad_data, 
  cast(
    cast(
      result13.good_data * 100 as decimal
    )/ result13.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result13.bad_data * 100 as decimal)/ result13.total_data as numeric
  ) as percentage_bad_data, 
  "material_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank material_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(material_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result13 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result14.criteria, 
  result14.metrics, 
  result14.total_data, 
  result14.good_data, 
  result14.bad_data, 
  cast(
    cast(
      result14.good_data * 100 as decimal
    )/ result14.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result14.bad_data * 100 as decimal)/ result14.total_data as numeric
  ) as percentage_bad_data, 
  "module_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank module_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(module_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned IN ('open_text', 'play_video_material', 'open_quiz', 'open_reflection', 'complete_material_quiz', 'save_reflection', 'play_material_video', 'play_video')  
  ) result14 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result15.criteria, 
  result15.metrics, 
  result15.total_data, 
  result15.good_data, 
  result15.bad_data, 
  cast(
    cast(
      result15.good_data * 100 as decimal
    )/ result15.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result15.bad_data * 100 as decimal)/ result15.total_data as numeric
  ) as percentage_bad_data, 
  "video_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(video_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video"  
  ) result15 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result16.criteria, 
  result16.metrics, 
  result16.total_data, 
  result16.good_data, 
  result16.bad_data, 
  cast(
    cast(
      result16.good_data * 100 as decimal
    )/ result16.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result16.bad_data * 100 as decimal)/ result16.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(video_inspirasi_id, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(video_inspirasi_id, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result16 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result17.criteria, 
  result17.metrics, 
  result17.total_data, 
  result17.good_data, 
  result17.bad_data, 
  cast(
    cast(
      result17.good_data * 100 as decimal
    )/ result17.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result17.bad_data * 100 as decimal)/ result17.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_playslist_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank video_inspirasi_playslist_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(
            video_inspirasi_playslist_id as string
          ), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "play_video_inspirasi"  
  ) result17 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result18.criteria, 
  result18.metrics, 
  result18.total_data, 
  result18.good_data, 
  result18.bad_data, 
  cast(
    cast(
      result18.good_data * 100 as decimal
    )/ result18.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result18.bad_data * 100 as decimal)/ result18.total_data as numeric
  ) as percentage_bad_data, 
  "quiz_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank quiz_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(quiz_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_quiz'  
  ) result18 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result19.criteria, 
  result19.metrics, 
  result19.total_data, 
  result19.good_data, 
  result19.bad_data, 
  cast(
    cast(
      result19.good_data * 100 as decimal
    )/ result19.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result19.bad_data * 100 as decimal)/ result19.total_data as numeric
  ) as percentage_bad_data, 
  "text_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank text_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(text_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'open_text'  
  ) result19 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result20.criteria, 
  result20.metrics, 
  result20.total_data, 
  result20.good_data, 
  result20.bad_data, 
  cast(
    cast(
      result20.good_data * 100 as decimal
    )/ result20.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result20.bad_data * 100 as decimal)/ result20.total_data as numeric
  ) as percentage_bad_data, 
  "open_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank open_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(open_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = "open_reflection"  
  ) result20 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result21.criteria, 
  result21.metrics, 
  result21.total_data, 
  result21.good_data, 
  result21.bad_data, 
  cast(
    cast(
      result21.good_data * 100 as decimal
    )/ result21.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result21.bad_data * 100 as decimal)/ result21.total_data as numeric
  ) as percentage_bad_data, 
  "save_reflection_id" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank save_reflection_id' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(
          cast(save_reflection_id as string), 
          ''
        )= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'save_reflection'  
  ) result21 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result22.criteria, 
  result22.metrics, 
  result22.total_data, 
  result22.good_data, 
  result22.bad_data, 
  cast(
    cast(
      result22.good_data * 100 as decimal
    )/ result22.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result22.bad_data * 100 as decimal)/ result22.total_data as numeric
  ) as percentage_bad_data, 
  "post_test_result" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank post_test_result' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(post_test_result, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(post_test_result, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
    where action_type_cleaned = 'finished_post_test'  
  ) result22 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result23.criteria, 
  result23.metrics, 
  result23.total_data, 
  result23.good_data, 
  result23.bad_data, 
  cast(
    cast(
      result23.good_data * 100 as decimal
    )/ result23.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result23.bad_data * 100 as decimal)/ result23.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_source" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_source' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_source, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_source, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result23 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result24.criteria, 
  result24.metrics, 
  result24.total_data, 
  result24.good_data, 
  result24.bad_data, 
  cast(
    cast(
      result24.good_data * 100 as decimal
    )/ result24.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result24.bad_data * 100 as decimal)/ result24.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_location" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_location' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_location, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_location, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result24 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result25.criteria, 
  result25.metrics, 
  result25.total_data, 
  result25.good_data, 
  result25.bad_data, 
  cast(
    cast(
      result25.good_data * 100 as decimal
    )/ result25.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result25.bad_data * 100 as decimal)/ result25.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_referrer" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_referrer' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_referrer, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_referrer, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result25 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result26.criteria, 
  result26.metrics, 
  result26.total_data, 
  result26.good_data, 
  result26.bad_data, 
  cast(
    cast(
      result26.good_data * 100 as decimal
    )/ result26.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result26.bad_data * 100 as decimal)/ result26.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_path" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_path' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_path, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_path, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result26 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result27.criteria, 
  result27.metrics, 
  result27.total_data, 
  result27.good_data, 
  result27.bad_data, 
  cast(
    cast(
      result27.good_data * 100 as decimal
    )/ result27.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result27.bad_data * 100 as decimal)/ result27.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_page_title" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_page_title' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_page_title, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_page_title, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result27 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result28.criteria, 
  result28.metrics, 
  result28.total_data, 
  result28.good_data, 
  result28.bad_data, 
  cast(
    cast(
      result28.good_data * 100 as decimal
    )/ result28.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result28.bad_data * 100 as decimal)/ result28.total_data as numeric
  ) as percentage_bad_data, 
  "event_params_product" field_name_checking 
from 
  (
    select 
      'Completeness' criteria, 
      'field mandatory is null/blank event_params_product' metrics, 
      count(ga_session_id) total_data, 
      count(
        case when ifnull(event_params_product, '')<> '' then 'pass' end
      ) as good_data, 
      count(
        case when ifnull(event_params_product, '')= '' then 'fail' end
      ) as bad_data 
    from 
      x
  ) result28
  );
    
2021-12-27 15:36:03.098806 (Thread-1): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 60602), raddr=('74.125.130.95', 443)>
2021-12-27 15:36:03.099469 (Thread-1): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 45374), raddr=('34.101.5.74', 443)>
2021-12-27 15:36:03.100001 (Thread-1): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 60606), raddr=('74.125.130.95', 443)>
2021-12-27 15:36:03.100468 (Thread-1): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 45378), raddr=('34.101.5.74', 443)>
2021-12-27 15:36:03.100983 (Thread-1): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 60610), raddr=('74.125.130.95', 443)>
2021-12-27 15:36:03.101436 (Thread-1): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 45382), raddr=('34.101.5.74', 443)>
2021-12-27 15:36:06.873130 (Thread-1): finished collecting timing info
2021-12-27 15:36:06.874386 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a191896-ac6f-4435-abea-c14644ab8eb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe1c39d00>]}
2021-12-27 15:36:06.875309 (Thread-1): 22:36:06 | 2 of 6 OK created table model dataset_dbt.dqa_completeness........... [CREATE TABLE (28.0 rows, 2.6 MB processed) in 4.46s]
2021-12-27 15:36:06.875870 (Thread-1): Finished running node model.my_new_project.dqa_completeness
2021-12-27 15:36:06.876394 (Thread-1): Began running node model.my_new_project.dqa_timeliness
2021-12-27 15:36:06.877108 (Thread-1): 22:36:06 | 3 of 6 START table model dataset_dbt.dqa_timeliness.................. [RUN]
2021-12-27 15:36:06.878216 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_timeliness".
2021-12-27 15:36:06.878785 (Thread-1): Compiling model.my_new_project.dqa_timeliness
2021-12-27 15:36:06.884977 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_timeliness"
2021-12-27 15:36:06.885893 (Thread-1): finished collecting timing info
2021-12-27 15:36:06.889665 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 15:36:06.896422 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 15:36:07.278620 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_timeliness"
2021-12-27 15:36:07.279506 (Thread-1): On model.my_new_project.dqa_timeliness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_timeliness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_timeliness`
  
  
  OPTIONS()
  as (
    

select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result.criteria, 
  result.metrics, 
  result.total_data, 
  result.good_data, 
  result.bad_data, 
  cast(
    cast(result.good_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result.bad_data * 100 as decimal)/ result.total_data as numeric
  ) as percentage_bad_data, 
  "updated_at" field_name_checking 
from 
  (
    select 
      'Timeliness' criteria, 
      'freshness data Day - 1 on staging table' metrics, 
      count(updated_at) total_data, 
      count(
        case when updated_at in (
          select 
            updated_at 
          from 
            `playground-325606.dataset_dbt.event_tracker_belajar`
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'true' end
      ) as good_data, 
      count(
        case when updated_at not in (
          select 
            updated_at 
          from 
            `playground-325606.dataset_dbt.event_tracker_belajar`
          where 
            date(
              current_date()
            ) < DATE_SUB(
              current_date(), 
              interval 1 day
            )
        ) then 'false' end
      ) as bad_data, 
    from 
      `playground-325606.dataset_dbt.event_tracker_belajar`
  ) result
  );
    
2021-12-27 15:36:10.829427 (Thread-1): finished collecting timing info
2021-12-27 15:36:10.830442 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a191896-ac6f-4435-abea-c14644ab8eb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe1c39e50>]}
2021-12-27 15:36:10.831317 (Thread-1): 22:36:10 | 3 of 6 OK created table model dataset_dbt.dqa_timeliness............. [CREATE TABLE (1.0 rows, 63.7 KB processed) in 3.95s]
2021-12-27 15:36:10.831787 (Thread-1): Finished running node model.my_new_project.dqa_timeliness
2021-12-27 15:36:10.832242 (Thread-1): Began running node model.my_new_project.dqa_uniqueness
2021-12-27 15:36:10.833073 (Thread-1): 22:36:10 | 4 of 6 START table model dataset_dbt.dqa_uniqueness.................. [RUN]
2021-12-27 15:36:10.833873 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_uniqueness".
2021-12-27 15:36:10.834332 (Thread-1): Compiling model.my_new_project.dqa_uniqueness
2021-12-27 15:36:10.839471 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-27 15:36:10.839933 (Thread-1): finished collecting timing info
2021-12-27 15:36:10.841030 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 15:36:10.846407 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 15:36:11.175374 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_uniqueness"
2021-12-27 15:36:11.176158 (Thread-1): On model.my_new_project.dqa_uniqueness: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_uniqueness"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with x as (

    select distinct 
date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned, count(1) total
FROM `playground-325606.dataset_dbt.event_tracker_belajar`
group by date_gmt7,
time_gmt7,
hour_gmt7, 
timestamp_gmt7,
event_name,
ga_session_id,
user_id,
event_params_page_title,
event_params_product,
action_type_cleaned

)

select datetime(current_timestamp(), "Asia/Jakarta") as time_execution, result.criteria, result.metrics, result.total_data, result.good_data, result.bad_data, cast(cast(result.good_data * 100 as decimal)/result.total_data as numeric) as percentage_good_data, cast(cast(result.bad_data * 100 as decimal)/result.total_data as numeric) as percentage_bad_data, "date_gmt7, time_gmt7, hour_gmt7, timestamp_gmt7, event_name, ga_session_id, user_id, event_params_page_title, event_params_product, action_type_cleaned" field_name_checking
from (
select 'Uniqueness' criteria,
'duplicate in rows' metrics,
count(x.total ) as total_data,
count(
    case 
    when (x.total = 1) then 'pass' end ) as good_data,
count(
    case
    when (x.total > 1) then 'fail' end ) as bad_data        
from x
)result


/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
    
2021-12-27 15:36:13.604401 (Thread-1): finished collecting timing info
2021-12-27 15:36:13.605354 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a191896-ac6f-4435-abea-c14644ab8eb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe02e07f0>]}
2021-12-27 15:36:13.606135 (Thread-1): 22:36:13 | 4 of 6 OK created table model dataset_dbt.dqa_uniqueness............. [CREATE TABLE (1.0 rows, 1018.8 KB processed) in 2.77s]
2021-12-27 15:36:13.606563 (Thread-1): Finished running node model.my_new_project.dqa_uniqueness
2021-12-27 15:36:13.606977 (Thread-1): Began running node model.my_new_project.dqa_validity
2021-12-27 15:36:13.607953 (Thread-1): 22:36:13 | 5 of 6 START table model dataset_dbt.dqa_validity.................... [RUN]
2021-12-27 15:36:13.608718 (Thread-1): Acquiring new bigquery connection "model.my_new_project.dqa_validity".
2021-12-27 15:36:13.609269 (Thread-1): Compiling model.my_new_project.dqa_validity
2021-12-27 15:36:13.617166 (Thread-1): Writing injected SQL for node "model.my_new_project.dqa_validity"
2021-12-27 15:36:13.618125 (Thread-1): finished collecting timing info
2021-12-27 15:36:13.621488 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 15:36:13.627322 (Thread-1): Client.dataset is deprecated and will be removed in a future version. Use a string like 'my_project.my_dataset' or a cloud.google.bigquery.DatasetReference object, instead.
2021-12-27 15:36:14.004915 (Thread-1): Writing runtime SQL for node "model.my_new_project.dqa_validity"
2021-12-27 15:36:14.005874 (Thread-1): On model.my_new_project.dqa_validity: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.dqa_validity"} */


  create or replace table `playground-325606`.`dataset_dbt`.`dqa_validity`
  
  
  OPTIONS()
  as (
    

with x as (
  select 
    user_id,
    ga_session_id,
    video_inspirasi_id  
  FROM 
    `playground-325606.dataset_dbt.event_tracker_belajar`
)
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result1.criteria, 
  result1.metrics, 
  result1.total_data, 
  result1.good_data, 
  result1.bad_data, 
  cast(
    cast(result1.good_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result1.bad_data * 100 as decimal)/ result1.total_data as numeric
  ) as percentage_bad_data, 
  "user_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length user_id' metrics, 
      count(x.user_id) total_data, 
      count(
        case when length(x.user_id) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.user_id) < 10 
        or length(x.user_id) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result1 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result2.criteria, 
  result2.metrics, 
  result2.total_data, 
  result2.good_data, 
  result2.bad_data, 
  cast(
    cast(result2.good_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result2.bad_data * 100 as decimal)/ result2.total_data as numeric
  ) as percentage_bad_data, 
  "ga_session_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length ga_session_id' metrics, 
      count(x.ga_session_id) total_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        ) = 10 then 'pass' end
      ) as good_data, 
      count(
        case when length(
          cast(x.ga_session_id as string)
        )< 10 
        or length(
          cast(x.ga_session_id as string)
        ) > 10 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result2 
union all 
select 
  datetime(
    current_timestamp(), 
    "Asia/Jakarta"
  ) as time_execution, 
  result3.criteria, 
  result3.metrics, 
  result3.total_data, 
  result3.good_data, 
  result3.bad_data, 
  cast(
    cast(result3.good_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_good_data, 
  cast(
    cast(result3.bad_data * 100 as decimal)/ result3.total_data as numeric
  ) as percentage_bad_data, 
  "video_inspirasi_id" field_name_checking 
from 
  (
    select 
      'Validity' criteria, 
      'not valid format, syntax or length video_inspirasi_id' metrics, 
      count(x.video_inspirasi_id) total_data, 
      count(
        case when length(x.video_inspirasi_id) = 11 then 'pass' end
      ) as good_data, 
      count(
        case when length(x.video_inspirasi_id) < 11 
        or length(x.video_inspirasi_id) > 11 then 'fail' end
      ) as bad_data, 
    from 
      x
  ) result3
  );
    
2021-12-27 15:36:17.324934 (Thread-1): finished collecting timing info
2021-12-27 15:36:17.325869 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a191896-ac6f-4435-abea-c14644ab8eb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe02e0430>]}
2021-12-27 15:36:17.326637 (Thread-1): 22:36:17 | 5 of 6 OK created table model dataset_dbt.dqa_validity............... [CREATE TABLE (3.0 rows, 113.1 KB processed) in 3.72s]
2021-12-27 15:36:17.327052 (Thread-1): Finished running node model.my_new_project.dqa_validity
2021-12-27 15:36:17.327720 (Thread-1): Began running node model.my_new_project.view_models_dqa
2021-12-27 15:36:17.327940 (Thread-1): 22:36:17 | 6 of 6 START view model dataset_dbt.view_models_dqa.................. [RUN]
2021-12-27 15:36:17.328307 (Thread-1): Acquiring new bigquery connection "model.my_new_project.view_models_dqa".
2021-12-27 15:36:17.328448 (Thread-1): Compiling model.my_new_project.view_models_dqa
2021-12-27 15:36:17.334317 (Thread-1): Writing injected SQL for node "model.my_new_project.view_models_dqa"
2021-12-27 15:36:17.335116 (Thread-1): finished collecting timing info
2021-12-27 15:36:17.348974 (Thread-1): Writing runtime SQL for node "model.my_new_project.view_models_dqa"
2021-12-27 15:36:17.349395 (Thread-1): Opening a new connection, currently in state closed
2021-12-27 15:36:17.352958 (Thread-1): On model.my_new_project.view_models_dqa: /* {"app": "dbt", "dbt_version": "0.21.0", "profile_name": "default", "target_name": "prod", "node_id": "model.my_new_project.view_models_dqa"} */


  create or replace view `playground-325606`.`dataset_dbt`.`view_models_dqa`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `playground-325606`.`dataset_dbt`.`dqa_uniqueness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_completeness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_timeliness`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_validity`
;
select *
from `playground-325606`.`dataset_dbt`.`dqa_all_criteria`;


2021-12-27 15:36:24.463240 (Thread-1): finished collecting timing info
2021-12-27 15:36:24.464186 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a191896-ac6f-4435-abea-c14644ab8eb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe0202610>]}
2021-12-27 15:36:24.464977 (Thread-1): 22:36:24 | 6 of 6 OK created view model dataset_dbt.view_models_dqa............. [SCRIPT (9.2 KB processed) in 7.14s]
2021-12-27 15:36:24.465426 (Thread-1): Finished running node model.my_new_project.view_models_dqa
2021-12-27 15:36:24.467588 (MainThread): Acquiring new bigquery connection "master".
2021-12-27 15:36:24.468503 (MainThread): 22:36:24 | 
2021-12-27 15:36:24.468958 (MainThread): 22:36:24 | Finished running 5 table models, 1 view model in 25.86s.
2021-12-27 15:36:24.469439 (MainThread): Connection 'master' was properly closed.
2021-12-27 15:36:24.469798 (MainThread): Connection 'model.my_new_project.view_models_dqa' was properly closed.
2021-12-27 15:36:24.473362 (MainThread): unclosed <ssl.SSLSocket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 60618), raddr=('74.125.130.95', 443)>
2021-12-27 15:36:24.473575 (MainThread): unclosed <ssl.SSLSocket fd=6, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 45390), raddr=('34.101.5.74', 443)>
2021-12-27 15:36:24.473724 (MainThread): unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 60624), raddr=('74.125.130.95', 443)>
2021-12-27 15:36:24.473830 (MainThread): unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 45396), raddr=('34.101.5.74', 443)>
2021-12-27 15:36:24.473947 (MainThread): unclosed <ssl.SSLSocket fd=11, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 60628), raddr=('74.125.130.95', 443)>
2021-12-27 15:36:24.474040 (MainThread): unclosed <ssl.SSLSocket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.38', 45400), raddr=('34.101.5.74', 443)>
2021-12-27 15:36:24.477128 (MainThread): 
2021-12-27 15:36:24.477307 (MainThread): Completed successfully
2021-12-27 15:36:24.477479 (MainThread): 
Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
2021-12-27 15:36:24.477689 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe01f4cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe1c67400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2fe0213e80>]}
2021-12-27 15:36:24.477856 (MainThread): Flushing usage events
